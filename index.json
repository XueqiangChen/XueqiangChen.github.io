[{"contents":"kafka不仅仅是消息系统，更可以用在流式处理的场景中。本文主要介绍消息系统概述，kafka系统架构和原理。\n消息系统概述  为什么需要消息系统\n 解耦、冗余、扩展性、灵活性\u0026amp;峰值处理、可恢复性、顺序保证、缓冲、异步处理、消息通信\nkafka是个消息队列，但是又不是简单的消息队列。首先有主题，针对不同的主题有不同的消费。特性：主题、不同订阅端消费不同的主题、接受不同的生产者，数据源\n 消息系统\n ActiveMQ、Kafka、RabbitMQ、Redis、Jafka、ZeroMQ\n 生产者和消费者模式\n 在生产者／消费者模型中，生产者Producer负责生产数据，而消费者Consumer负责\n使用数据。多个生产者会在同一时间生产数据，并放到内存中一个共享的区域。\nKafka的发展历程和现状  KSQL\n 流式计算，针对流进行增强\nKafka系统架构和原理  Topics 和 Append Log\n 一个topic是对一组消息的归纳，一个topic是对一组消息的归纳，每个partition会生成一个append log文件。每条消息在文件中的位置称为offset（偏移量），唯一标记一条消息。\n Producer和Consumer\n Producer即生产者，向Kafka集群发送消息，消息会按照Topic进行分类。\nConsumer即消费者，消费者通过与Kafka集群建立长连接的方式，不断地从集群中\n拉取消息，然后可以对这些消息进行处理。它需要保存消费消息的offset：当\nconsumer正常消费消息时，offset将会“线性”的增加，即消息将依次顺序被消费。\n通过设置offset，consumer可以从任意位置消费消息。\nConsumer可以在本地保存最后消息的offset，并向ZooKeeper注册offset。\n Consumer Group\n 这是Kafka用来实现一个topic消息广播的手段。即多个不同的group来同时消费同一\n个topic下的消息。他们消费的offset各不相同，各不干扰。\n一个group中，Consumer的数量不应该多于Partition的数量，即一个消费者可以消\n费多个分区，但一个分区只能给一个消费者消费。若一个group中的消费者数量大于\nPartition数量的话，多余的消费者将不会收到任何消息。\n Broker\n 消息保存在一组服务器中，它们被称为代理（Broker）或Kafka集群。\n每个Broker有一个Broker id。\n每个Broker存储Topic的部分数据。\nKafka的存储文件按照offset.kafka来命名，用offset做名字的好处是方便查找。例如\n你想找位于2049的位置，只要找到2048.kafka的文件即可。第一个个存储文件是\n00000000000.kafka。\n 整体架构图\n  消息传送机制\n 对于传统消息系统，消息传输担保非常直接：有且只有一次(exactly once)。在Kafka\n中稍有不同：\n• at most once：消息最多发生一次，无论消费者是否接受到，都不会重发。\n• at least once：消息至少发送一次，如果消息未能接受成功，会重发直到接收成功。\n• exactly once：消息只会发送一次\n新版本才有。\n Kafka中各语义如何实现\n at most once：消费者fetch消息，先保存消息的offset，然后处理消息。当消费者保\n存offset之后，但是在消息处理过程中出现了异常，导致部分消息未能继续处理。那\n么此后“未处理”的消息将不能被fetch到，这就是“at most once”。\nat least once：消费者fetch消息，先处理消息，然后保存offset。如果消息处理成功\n之后，但是在保存offset阶段，ZooKeeper异常导致保存操作未能执行成功，这就导\n致接下来再次fetch时可能获得上次已经处理过的消息，这就是“at least once”。\nexactly once：Kafka中并没有严格的去实现（基于2阶段提交、事务），我们认为这\n种策略在Kafka中是没有必要的\n如何做到可靠数据传递  Replica\n Kafka将每个partition数据复制到多个server上，任何一个partition有一个leader和\n零个或多个follower（ISR），称为partition的副本（replica）。\n• Leader处理所有的读写请求，follower需要和leader保持同步。Follower和\nconsumer一样，消费消息并保存在本地日志中。\n• 当所有的follower都将一条消息保存成功，此消息才被认为是“committed”。只有\ncommitted的消息，consumer才能消费它。\n• 当leader失效时，需在followers中选取出新的leader。可能此时follower落后于\nleader，因此需要选择一个尽可能“新”的follower。\n 容灾\n  Producer ACK\n  ACK=0，即没有ack，可能会造成数据丢失（at most once） ACK=1，producer会等接收到Leader的ack再发生下一条消息（at most once） ACK=all，producer会等接收到Leader+Follower的ack再发生下一条消息（at least\nonce / exactly once）   Producer Key\n  Producer可以为发送的消息设置Key来进行发生模式选择。 当key=null，数据才用轮训（Round Robin）方式进行发送。 当key!=null，数据可以根据散列键（Hash）方式进行发送，相同Key的数据能发送\n到同一个Partition。   Consumer Rebalance\n  Consumer记录每个partition所消费的maximum offset，并定期commit到offset\nmanager。 属于同一个group的consumer（group id一样）平均分配partition，每个partition\n只会被一个consumer消费。   ZooKeeper会管理broker与consumer的动态关系。 当broker或consumer加入或离开时会触发负载均衡算法，使得一个consumer group内的多个consumer的订阅负载平衡。 ZooKeeper还维护每个partition的消费offset。 算法：\n假如topic1,具有如下partitions: P0,P1,P2,P3\n假如group中,有如下consumer: C1,C2\n首先根据partition索引号对partitions排序: P0,P1,P2,P3\n根据consumer.id排序: C0,C1\n计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整)\n然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3]\nCi = [P(i * M),P((i + 1) * M -1)]  Page Cache和零拷贝  文件存储\n  在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，\npartiton目录的命名规则为topic名称+有序序号，第一个partiton序号从0开始。 每个partition目录存储着多个segment数据文件。Segment数据文件size固定，但消息数量不一定相等。默认保留7天。 每个segment由index file（.index后缀）和data file（.log后缀）”组成。 Partition全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。   Segment 存储\n   疑问一：为什么要分区呢？\n为了性能考虑，如果不分区每个topic的消息只存在一个broker上，那么所有的消费者都是从这个broker上消费消息，那么单节点的broker成为性能的瓶颈，如果有分区的话生产者发过来的消息分别存储在各个broker不同的partition上，这样消费者可以并行的从不同的broker不同的partition上读消息，实现了水平扩展。\n  疑问二：为什么有了partition还需要segment ？\n如果不引入segment，那么一个partition只对应一个文件（log），随着消息的不断发送这个文件不断增大，由于Kafka的消息不会做更新操作都是顺序写入的，如果做消息清理的时候只能删除文件的前面部分删除，不符合Kafka顺序写入的设计，如果多个segment的话那就比较方便了，直接删除整个文件即可保证了每个segment的顺序写入。\n  疑问三：分区文件下到底存了那些东西？\n其实每个分区下保存了很多文件，而概念上我们把他叫segment，即每个分区都是又多个\nsegment构成的，其中index（索引文件）、log（数据文件）、time index（时间索引文件）统称为一个segment。\n   消费者如何通过offset查找message\n 假如我们想要读取offset=1066的message，需要通过下面2个步骤查找：\n  查找segment file\n0000.index表示最开始的文件，起始偏移量(offset)为0。第二个文件1018.index的消息量起始偏移量为1019(1018 +1)。同样，第三个文件2042.index的起始偏移量为2043(2042 + 1)。只要根据offset进行二分查找文件列表，就可以快速定位到具体文件。当offset=1066时定位到1018.index|log。\n  通过segment file 查找message\n通过第一步定位到segment file，当offset=1066时，依次定位到1018.index的元数据物理位置和1018.log的物理偏移地址，此时我们只能拿到1065的物理偏移地址，然后再通过368769.log顺序查找直到offset=1066为止。\n   高性能\n Kafka官方测试其数据读写速率能达到600M/s，那么为什么性能会这么高呢？\n sequence IO PageCache Zero Copy（SendFile）   Sequence IO\n Kafka在将数据持久化到磁盘时，采用只追加的顺序写，有效降低了寻址时间，提高效率。\n PageCache\n PageCache是Linux系统级别的缓存，它把尽可能多的空闲内存当作磁盘缓存使用来进一步提高IO效率。\n当写操作发生时，操作系统只是将数据写入PageCache，同时标记Page属性为Dirty。当读操作发生时，先从PageCache中查找，如果发生缺页才进行磁盘调度，最终返回需要的数据。\n Producer把消息发到broker后，数据并不是直接落入磁盘的，而是先进入PageCache。PageCache中的数据会被内核中的处理线程采用同步或异步的方式写回到磁盘。 Consumer消费消息时，会先从PageCache获取消息，获取不到才回去磁盘读取，并且会预读出一些相邻的块放入PageCache，以方便下一次读取。   Zero Copy\n 传统的网络I/O过程：\n 操作系统从磁盘把数据读到内核区 用户进程把数据从内核区copy到用户区 然后用户进程再把数据写入到内核区的socket buffer上 最后把数据从socket buffer中发送到网卡  通过SendFile（又称Zero Copy）优化后，数据从内核区copy到socket，然后发送到网卡，避免了数据在内核态和用户态的来回拷贝。\n从Linux内核2.4版本开始，对于网卡支持SG-DMA的，SendFile进一步减少通过CPU把内核缓冲区里的数据拷贝到socket缓冲区的过程。\nKSQL","date":"2021年10月11日","permalink":"https://ahamoment.cn/posts/bigdata/kafka/","summary":"\u003cp\u003ekafka不仅仅是消息系统，更可以用在流式处理的场景中。本文主要介绍消息系统概述，kafka系统架构和原理。\u003c/p\u003e","title":"流式处理和实时计算：Kafka"},{"contents":"大数据 OLAP 引擎：Presto 概述  SQL on Anything\n  presto特点\n  presto架构\n Presto查询引擎是一个Master-Slave的架构，由一个Coordinator节点，一个Discovery Server节点，多个Worker节点组成，Discovery Server通常内嵌于Coordinator节点中。 Coordinator负责解析SQL语句，生成执行计划，分发执行任务给Worker节点执行。 Worker节点负责实际执行查询任务。Worker节点启动后向Discovery Server服务注册， Coordinator从Discovery Server获得可以正常工作的Worker节点。如果配置了Hive Connector，需要配置一个Hive MetaStore服务为Presto提供Hive元信息，Worker节点 与HDFS交互读取数据。\n presto比hive快的原因\n dag结构，memory to memory\n 应用场景\n etl，adhoc查询\n Docker 环境部署\n docker pull alluxio/alluxio-presto-sandbox\ndocker run -d \\ \u0026ndash;shm-size 1G \\ -p 19999:19999 \\ -p 8080:8080 \\ \u0026ndash;name alluxio-presto-sandbox \\ alluxio/alluxio-presto-sandbox\ndocker exec -it alluxio-presto-sandbox bash\npresto \u0026ndash;catalog hive \u0026ndash;debug\n或者通过下面的方式部署\nhttps://hub.docker.com/r/ahanaio/prestodb-sandbox\n参考资料：\n Presto官网：https://prestodb.io/ Presto下载安装包：https://prestodb.io/download.html Presto docker sandbox: https://hub.docker.com/r/ahanaio/prestodb-sandbox Presto与Alluxio集成Docker环境上手指南：https://www.alluxio.io/alluxiopresto-sandbox-docker/ Presto安装配置文档： https://prestodb.io/docs/current/installation/deployment.html Presto与Hive集成文档：https://prestodb.io/docs/current/connector/hive.html Presto SQL语法文档：https://prestodb.io/docs/current/sql.html  ","date":"2021年10月09日","permalink":"https://ahamoment.cn/posts/bigdata/olap-presto/","summary":"大数据 OLAP 引擎：Presto 概述 SQL on Anything presto特点 presto架构 Presto查询引擎是一个Master-Slave的架构，由一个Coor","title":"大数据 OLAP 引擎：Presto 概述"},{"contents":"OLAP OLAP vs OLTP 的差异\nOLAP定义：联机分析处理，数据访问和分析，多维数据，交互性，深入观察，决策。\nOLAP 的目标：查询和报表\n相关概念：\n 维度 维的层次 维的成员 多维数组 度量  基本特征：\n 快速性：秒级 可分析性：统计 多维性 信息性  多维数据结构：\n 数据超立方体：三维或更多的维数，彼此垂直  分类：\n按存储类型：\n MOLAP：多维 ROLAP：关系型，星形模型和雪花模型 HOLAP：混合  ROLAP的优势：\n 没有大小限制（不需要额外存储空间，键关联） 关系数据库的技术沿用 SQL查询 优化多，提高查询性能  缺点：\n 响应速度差 不支持预计算 SQL无法完成部分计算 内存需求大  molap的优势：\n 性能好，响应快，不需要关联，主外键连接 专为olap设计，可以考虑olap的一些问题  缺点：\n 增加系统复杂度，增加系统培训与维护成本 需要进行预计算，导致数据急剧膨胀 支持维的动态变化比较困难  ROLAP架构\nMOLAP架构\nHOLAP架构\n OLAP引擎有哪些？是属于那种类型的OLAP？\n ROLAP: hive, presto\nMOLAP: kylin，\n OLAP多维技术分析\n 多维数据模型\n实现：\n 基于关系数据库  星型模型 雪花模型 事实群模型   基于多维数组  多维数组存储\n 关系存储：扩展性好，不存在稀疏问题，访问速度不够快 多维数组存储：访问速度快，有大量无效的值，存储效率下降   解决数据稀疏造成空间浪费的问题\n  采用数据压缩技术，如头文件压缩，lzw压缩方法(https://segmentfault.com/a/1190000011425787)等   解决不同维的访问效率差别大的问题\n  将一个n维数组分成多个小的n为数据块（chunk）的方法  多维分析操作\n 建立在关系聚合操作上的一些复合操作 基本的分析是求聚集函数  聚集Aggregation\n 分布型：sum，count，max，min 代数型：avg 整体型：median，rank  OLAP多维分析方法：\nOLAP的四种基本分析手段：多维视图\n 切片和切块(Slice and Dice) ：在确定某些维数据的确定情况下对其他维进行观察， 在多维数据结构中,按二维进行切片,按三维进行切块,可得到所需要的数据。如在“城 市、时间、漫游”三维立方体中进行切块和切片,可得到各城市、各漫游类型的费用情 况 钻取(Drill)：在一个维内沿着从高到低或者从低到高的方向考察数据，钻取包含向下 钻取（Drill-down）和向上钻取（Drill-up） / 上卷（Roll-up）操作， 钻取的深度 与维所划分的层次相对应。 旋转(Rotate) / 转轴(Pivot):通过旋转可以得到不同视角的数据，按不同的顺序组织维， 对结果进行考察 穿透(Drill-through)：是指从多维数据库向关系型数据库读取明细数据  目前主流的大数据OLAP引擎\nImpala, kylin,druid,presto,clickhouse\nOLAP引擎选型考量：\n SQL查询度和OLAP操作支持度 时效性 生态和工具完备性 查询性能和写入性能  ","date":"2021年09月15日","permalink":"https://ahamoment.cn/posts/bigdata/olap/","summary":"OLAP OLAP vs OLTP 的差异 OLAP定义：联机分析处理，数据访问和分析，多维数据，交互性，深入观察，决策。 OLAP 的目标：查询和报表 相关概念： 维度 维的层次 维的成","title":"OLAP概述"},{"contents":"简单邮件传输协议 (SMTP) 是一种以电子方式传输邮件的标准通信协议。 SMTP 使从应用程序内部发送邮件消息成为可能。 在本教程中，我们将使用 SMTP 和 Spring Boot 从我们的应用程序发送邮件消息。\n1. Maven 依赖 \u0026lt;!--mail--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-mail\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;   如果需要指定版本，也可以从mvn repo中找到你需要的版本号。\n 2. Spring Mail 服务简介 Spring mail 是Spring 框架提供的一个程序库，用于发送电子邮件，使我们不受底层邮件系统的限制，只关注客户端进行资源处理。Spring mail 包的内容如下：\n MailSender 接口：核心接口，提供用于发送简单电子邮件的基本功能。 JavaMailSender 接口：上述MailSender的子接口。它支持MIME消息，并且通常与MimeMessageHelper类结合使用以创建MimeMessage。 JavaMailSenderImpl类：提供JavaMailSender接口的实现。它支持MimeMessage和SimpleMailMessage。 SimpleMailMessage类：用于创建简单的邮件，包括from(发送者)，to(接收者)，cc(抄送)，subject(主题)和text(文本)等字段。 MimeMessagePreparator接口：提供用于接收MIME消息的回调接口。 MimeMessageHelper类：用于创建MIME消息的帮助器类。它提供在HTML布局中对图像，典型邮件附件和文本内容的支持。  3. 邮件服务配置 引入maven依赖之后，下一步就是使用spring.mail.*namespace 在 application.properties 文件中配置邮件服务。\nspring: mail: #字符集编码 默认 UTF-8 default-encoding: UTF-8 #邮件服务器的地址：例如smtp.qq.com,smtp.gmail.com,端口：25/465/587 host: localhost #登陆服务器的用户名和密码 username: username password: password port: 25 properties: mail: debug: false smtp: debug: false auth: true #启用tls连接 starttls: true # smtp 服务器需要身份验证，所以要配置用户密码 protocol: smtp test-connection: false  当我们的邮件服务是SSL安全连接的时候，我们需要添加ssl相关的信息：\nspring: mail: host: localhost port: 465 username: username password: password properties: mail: smtp: auth: true ssl: enable: true trust: smtp.gmail.com  trust: smtp.gmail.com 这个配置可以让我们避免一些关于PKIX路径错误的报错，不需要添加证书到java的security文件中，否则会报下面的这种错误：\nCaused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target at sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:196) at java.security.cert.CertPathBuilder.build(CertPathBuilder.java:268) at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:380) ... 29 more  当然不同的邮件服务器的配置有一些细微的差异，具体的可以参考Spring Boot 发送邮件全解析这篇文章。\n4. 发送邮件 4.1 发送简单邮件 @Service public class MailServiceImpl implements MailService { @Autowired private JavaMailSender javaMailSender; @Override public void sendSimpleMessage() { SimpleMailMessage message = new SimpleMailMessage(); message.setFrom(\u0026quot;abc@qq.com\u0026quot;); message.setTo(\u0026quot;efd@qq.com\u0026quot;); message.setSubject(\u0026quot;Test send simple mail message\u0026quot;); message.setText(\u0026quot;Hello world!\u0026quot;); javaMailSender.send(message); } }  4.2 发送附件邮件 @Override public void sendMessageWithAttachment() { MimeMessage message = javaMailSender.createMimeMessage(); try { MimeMessageHelper helper = new MimeMessageHelper(message, true); helper.setTo(\u0026quot;to@qq.com\u0026quot;); helper.setFrom(\u0026quot;from@qq.com\u0026quot;); helper.setSubject(\u0026quot;Send attachment file to email\u0026quot;); helper.setText(\u0026quot;attachment file...\u0026quot;); FileSystemResource file = new FileSystemResource(new File(\u0026quot;Absolute path\u0026quot;)); helper.addAttachment(\u0026quot;Invoice\u0026quot;, file); javaMailSender.send(message); } catch (MessagingException ex) { logger.error(\u0026quot;Failed to send email to. error={}\u0026quot;, ex.getMessage()); } }  4.3 发送模板邮件 Spring 中可以作为邮件模板的有几个选择：Velocity，Freemarker，Thymeleaf。 SpringBoot 1.4.0以后 Velocity 废弃了，官方建议用Freemarker。而Thymeleaf的效率没有freemaker高(评测见参考文章【4】）。\n Freemarker 的语法可以参考官网的手册:https://freemarker.apache.org/docs/index.html 中文手册：https://sourceforge.net/projects/freemarker/files/chinese-manual/\n 同样，我们先引入 Freemarker 的 Maven 依赖。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-freemarker\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  然后在项目的 /resoource/templates 目录下添加一个 Freemarker 模板文件 notification.flt。\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Hello world!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;My name is ${name}\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  在 SpringBoot 的配置文件中加上 Freemarker 的相关配置：\nspring: freemarker: template-loader-path: classpath:/templates/ enabled: true cache: false charset: UTF-8 content-type: text/html check-template-location: true  在 EmailServiceImpl 中使用 Freemarker 模板发送邮件：\n@Service public class MailServiceImpl implements MailService { private static final Logger logger = LoggerFactory.getLogger(MailServiceImpl.class); @Autowired private JavaMailSender javaMailSender; @Autowired private FreeMarkerConfigurer freeMarkerConfigurer; @Override public void sendMessageWithFreemarkerTemplate() { MimeMessage message = javaMailSender.createMimeMessage(); try { MimeMessageHelper helper = new MimeMessageHelper(message, true); helper.setTo(\u0026quot;to@qq.com\u0026quot;); helper.setFrom(\u0026quot;from@qq.com\u0026quot;); helper.setSubject(\u0026quot;Send freemarker template to email\u0026quot;); HashMap\u0026lt;String, Object\u0026gt; models = new HashMap\u0026lt;\u0026gt;(); models.put(\u0026quot;name\u0026quot;, \u0026quot;freemarker\u0026quot;); Template template = freeMarkerConfigurer.getConfiguration().getTemplate(\u0026quot;notification.flt\u0026quot;); String text = FreeMarkerTemplateUtils.processTemplateIntoString(template, models); helper.setText(text); javaMailSender.send(message); } catch (Exception ex) { logger.error(\u0026quot;Failed to send email to. error={}\u0026quot;, ex.getMessage()); } } }  至此实现了三中方式：纯文本，富文本(图片/附件)，Freemarker模版的邮件发送功能，接下来就来测试一下我们的邮件是否能发送出去吧。\n5. 测试邮件发送服务 我们为了测试邮件服务的发送功能，暂时可以先不用使用真正的邮件服务器，而是换成GreenMail。 GreenMail是用于测试目的的电子邮件服务器，可以用于邮件集成测试或用于开发的轻量级沙盒邮件服务器。具体的使用方法可以参考官网上的用例。\n这里先引入GreenMail 的Maven依赖：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.icegreen\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;greenmail\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.5.11\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt;  在Spring boot 的测试文件(application-test.yaml)中配置用于邮件服务的相关属性。\nspring: mail: default-encoding: UTF-8 host: localhost username: abc@qq.com password: password port: 3025 properties: mail: debug: false smtp: debug: false auth: true starttls: true protocol: smtp test-connection: false  我们创建一个自定义的 JUnit Rule 来初始化和停止GreenMail邮件服务器。\npublic class SmtpServerRule extends ExternalResource { # 设置发送邮件服务的用户的用户名 private static final String USER_PASSWORD = \u0026quot;password\u0026quot;; # 设置发送邮件服务用户的密码 private static final String USER_NAME = \u0026quot;abc@qq.com\u0026quot;; private GreenMail smtpServer; @Override protected void before() throws Throwable { super.before(); smtpServer = new GreenMail(ServerSetupTest.SMTP); smtpServer.start(); // setup user on the mail server smtpServer.setUser(USER_NAME, USER_PASSWORD); } public MimeMessage[] getMessage() { return smtpServer.getReceivedMessages(); } @Override protected void after() { super.after(); smtpServer.stop(); } }  然后就可以开始写我们的测试用例了。 我们使用JUnit @Rule注解配置SmtpServerRule。这标记了在每个集成测试之前和之后要调用的自定义规则。并允许我们拦截传入的电子邮件。最后，我们做出一些断言并验证发送的电子邮件是否等于接收的电子邮件。\n@ActiveProfiles(\u0026quot;test\u0026quot;) @SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT) @RunWith(SpringRunner.class) public class MailServiceImplTest { @Rule public SmtpServerRule smtpServerRule = new SmtpServerRule(); @Autowired private MailService mailService; @Test public void sendSimpleMessage() throws MessagingException { mailService.sendSimpleMessage(); MimeMessage[] messages = smtpServerRule.getMessage(); assertEquals(\u0026quot;Test send simple mail message\u0026quot;, messages[0].getSubject()); assertEquals(\u0026quot;abc@qq.com\u0026quot;, messages[0].getFrom()[0].toString()); assertEquals(\u0026quot;efd@qq.com\u0026quot;, messages[0].getAllRecipients()[0].toString()); } }  另外的两种邮件参考上面的代码实现。\n6. 总结 在这篇文章中，我们展示了如何通过Spring Boot应用程序设置和发送电子邮件。所有这些示例和代码片段的实现都可以在MyGitHub项目中找到。\n7. 参考文章 【1】Testing mail code in Spring Boot application\n【2】Spring Mail Integration Testing with JUnit and GreenMail Example\n【3】Guide to Spring Email\n【4】SpringBoot开发案例之整合mail发送服务\n","date":"2021年09月08日","permalink":"https://ahamoment.cn/posts/java/springboot-send-email/","summary":"\u003cp\u003e简单邮件传输协议 (SMTP) 是一种以电子方式传输邮件的标准通信协议。 SMTP 使从应用程序内部发送邮件消息成为可能。\n在本教程中，我们将使用 SMTP 和 Spring Boot 从我们的应用程序发送邮件消息。\u003c/p\u003e","title":"Spring Boot 发送邮件"},{"contents":"设置 JAVA_HOME\nexport JAVA_HOME=\u0026quot;$(dirname $(dirname $(realpath $(which javac))))\u0026quot;  tee Linux tee命令用于读取标准输入的数据，并将其内容输出成文件。\n语法 tee [OPTION]... [FILE]...  说明 -a, --append 添加内容到指定文件，不会覆盖源文件内容 -i, --ignore-interrupts 忽略输入  awk 用法示例\n   命令 说明     awk '{print}' c.txt 打印整个文件的内容   awk '{print $1}' c.txt 打印文本的第一列的内容   awk '{print $1, $2, $3}' c.txt 打印文本的第一列到第三列的内容   awk '{print $1 \u0026quot;\\t\u0026quot; $2 \u0026quot;\\t\u0026quot; $3}' c.txt 打印文本的第一列到第三列的内容，并按照制表符间隔   awk '{print NR \u0026quot;\\t\u0026quot; $1 \u0026quot;\\t\u0026quot; $2 \u0026quot;\\t\u0026quot; $3}' c.txt 打印文本的第一列到第三列的内容和行号，并按照制表符间隔。（NR：number of record）                文件 统计某文件夹下文件的个数\nls -l |grep \u0026quot;^-\u0026quot;|wc -l  统计某文件夹下目录的个数\nls -l |grep \u0026quot;^ｄ\u0026quot;|wc -l  统计文件夹下文件的个数，包括子文件夹里的\nls -lR|grep \u0026quot;^-\u0026quot;|wc -l  注意，如果文件中有隐藏的文件，需要在ls命令后面加上-a选项。\nls -laR | grep \u0026quot;^-\u0026quot; | wc -l  ","date":"2021年08月13日","permalink":"https://ahamoment.cn/posts/linux/linux-commands/","summary":"设置 JAVA_HOME export JAVA_HOME=\u0026quot;$(dirname $(dirname $(realpath $(which javac))))\u0026quot; tee Linux tee命令用于读取标准输入的数据，并将其内容输出成文件。 语法 tee [OPTION]... [FILE]... 说明 -a, --append 添加内容到指定文件，不会覆盖源文件内容 -i, --ignore-interrupts 忽","title":"Linux 常用命令"},{"contents":"前言 原先的博客主题 even 用了一段时间，由于想把博客内的一些系列文章整理成一个独立的笔记链接，索性就一起更换了博客的主题，现在使用的这款主题是eureka，这款主题配色简洁，文档丰富，基本的功能都有，在这款主题的基础上，作者参考了怡红院落的改造，增加了归档、搜索的功能。 另外之前一直想用 github action 的自动化来部署博客，一直都没有时间，这次一起做了。\n1. Hugo 博客是基于Hugo来构建的。Hugo 的安装十分简单，只要下载 hugo 的二进制文件，解压后放到/usr/local/bin/目录下即可。\n Hugo Release: https://github.com/gohugoio/hugo/releases 完成 Hugo 的安装之后就可以创建博客了，通过\n hugo new site myblog  来创建一个博客文件夹。创建后的目录如下：\nchenxq@ubuntu   /mnt/hgfs/GitRepos/exampleblog  ll    1049  11:27:58  总用量 512 drwxrwxrwx 1 root root 0 8月 10 11:27 archetypes -rwxrwxrwx 1 root root 82 8月 10 11:27 config.toml drwxrwxrwx 1 root root 0 8月 10 11:27 content drwxrwxrwx 1 root root 0 8月 10 11:27 data drwxrwxrwx 1 root root 0 8月 10 11:27 layouts drwxrwxrwx 1 root root 0 8月 10 11:27 static drwxrwxrwx 1 root root 0 8月 10 11:27 themes  Hugo 的官方文档有介绍具体的目录结构用途，这里不再赘述。 接下来，我们需要添加一个主题，使用 git submodule 命令添加：\ngit submodule add https://github.com/wangchucheng/hugo-eureka.git themes/eureka  添加之后删除掉config.toml文件并将themes/eureka/exampleSite/目录下的config和content复制到我们的博客根目录下。 eureka 主题使用的配置是放到config文件夹中的，并且按照不同的功能分开，我们修改config/_default/config.yaml文件的baseURL为我们的域名https://ahamoment.cn，其他的配置可根据需要进行修改。 eureka 主题可以自定义主页，不过作者偏向于传统的左右分栏的博客页面，就按照Hugo 主题 Eureka 自定义这篇博客的内容，做了定制。代码沿用了怡红公子的源代码，文件上做了一点调整，例如搜索功能，需要在content/文件夹下创建目录，在目录里面添加_index.md文件，在文件的meta中声明layout为search。\n--- title: 搜索 layout: search ---  在layouts/_default/中增加搜索的页面search.html。 最后我们添加一个文章\nhugo new posts/hello-world.md  这样，就会在content/posts/文件夹下增加一篇文章。 完成了博客的主题和内容之后，我们需要把githug pages上，这个过程是用hugo命令生成静态文件public，然后将public文件中的内容push到github的yourusername.github.io这个仓库。这里就是我们在前言说的，可以通过 github action 来完成自动化部署的工作。\n2. github action github action 是Github官方提供的CI/CD工具。可以直接从 GitHub 构建、测试和部署我们的代码。要使用github action，我们需要在博客的根目录下创建 .github/workflows/gh-pages.yml 文件。\nname: github pages on: push: branches: - master # Set a branch to deploy pull_request: jobs: deploy: runs-on: ubuntu-18.04 steps: - uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: 'latest' extended: false - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} external_repository: XueqiangChen/XueqiangChen.github.io publish_branch: master # default: gh-pages publish_dir: ./public user_name: 'XueqiangChen' user_email: '569503960@qq.com'  该文件的内容可以参考hugo的文档Build Hugo With GitHub Action。 这里需要注意的是 ACTIONS_DEPLOY_KEY, 我们需要在两个地方添加这个key，首先生成ssh key：\nsh-keygen -t rsa -b 4096 -C \u0026quot;$(git config user.email)\u0026quot; -f gh-pages -N \u0026quot;\u0026quot;  将生成的私钥放到当前的博客仓库的settings-secrets中，将公钥放到xueqiangchen.github.io仓库的settings-deploy keys中。 至此，我们完成了自动化部署的配置，后续提交代码到exampleblog中就会触发github action构建和部署我们的代码。\n3. 后台管理 后台管理采用的是 forestry.io 提供的 CMS 博客管理功能，可以在线编辑。 引入后台管理的目的是为了偶尔不在自己的电脑上写文章，可以快速完成，不需要搭建typora+picgo的环境。\n总结 作者的博客样式调整了多次，可以说是生命不息，折腾不止。当初选择脱离CSDN也是因为在CSDN发文章要经过审核乱七八糟的流程，然后页面定制还要付费，干脆就自己动手搞一个。这些年也明白了一个博客的核心还是内容，这一次折腾之后，回归到内容，多输出一些好文章。\n","date":"2021年06月17日","permalink":"https://ahamoment.cn/posts/tool/tool-hugo-blog/","summary":"前言 原先的博客主题 even 用了一段时间，由于想把博客内的一些系列文章整理成一个独立的笔记链接，索性就一起更换了博客的主题，现在使用的这款主题是eu","title":"Hugo 博客搭建"},{"contents":" 本文来自张磊老师的《深入剖析Kuberntes》课程笔记，请勿转载。\n 在上一篇文章中，我主要为你介绍了 Kubernetes 里关于资源模型和资源管理的设计方法。而在今天这篇文章中，我就来为你介绍一下 Kubernetes 的默认调度器（default scheduler）。\n在 Kubernetes 项目中，默认调度器的主要职责，就是为一个新创建出来的 Pod，寻找一个最合适的节点（Node）。\n而这里“最合适”的含义，包括两层：\n 从集群所有的节点中，根据调度算法挑选出所有可以运行该 Pod 的节点； 从第一步的结果中，再根据调度算法挑选一个最符合条件的节点作为最终结果。  所以在具体的调度流程中，默认调度器会首先调用一组叫作 Predicate 的调度算法，来检查每个 Node。然后，再调用一组叫作 Priority 的调度算法，来给上一步得到的结果里的每个 Node 打分。最终的调度结果，就是得分最高的那个 Node。\n而我在前面的文章中曾经介绍过，调度器对一个 Pod 调度成功，实际上就是将它的 spec.nodeName 字段填上调度结果的节点名字。\n在 Kubernetes 中，上述调度机制的工作原理，可以用如下所示的一幅示意图来表示。\n可以看到，Kubernetes 的调度器的核心，实际上就是两个相互独立的控制循环。\n其中，第一个控制循环，我们可以称之为 Informer Path。它的主要目的，是启动一系列 Informer，用来监听（Watch）Etcd 中 Pod、Node、Service 等与调度相关的 API 对象的变化。比如，当一个待调度 Pod（即：它的 nodeName 字段是空的）被创建出来之后，调度器就会通过 Pod Informer 的 Handler，将这个待调度 Pod 添加进调度队列。\n在默认情况下，Kubernetes 的调度队列是一个 PriorityQueue（优先级队列），并且当某些集群信息发生变化的时候，调度器还会对调度队列里的内容进行一些特殊操作。这里的设计，主要是出于调度优先级和抢占的考虑，我会在后面的文章中再详细介绍这部分内容。\n此外，Kubernetes 的默认调度器还要负责对调度器缓存（即：scheduler cache）进行更新。事实上，Kubernetes 调度部分进行性能优化的一个最根本原则，就是尽最大可能将集群信息 Cache 化，以便从根本上提高 Predicate 和 Priority 调度算法的执行效率。\n而第二个控制循环，是调度器负责 Pod 调度的主循环，我们可以称之为 Scheduling Path。\nScheduling Path 的主要逻辑，就是不断地从调度队列里出队一个 Pod。然后，调用 Predicates 算法进行“过滤”。这一步“过滤”得到的一组 Node，就是所有可以运行这个 Pod 的宿主机列表。当然，Predicates 算法需要的 Node 信息，都是从 Scheduler Cache 里直接拿到的，这是调度器保证算法执行效率的主要手段之一。\n接下来，调度器就会再调用 Priorities 算法为上述列表里的 Node 打分，分数从 0 到 10。得分最高的 Node，就会作为这次调度的结果。\n调度算法执行完成后，调度器就需要将 Pod 对象的 nodeName 字段的值，修改为上述 Node 的名字。这个步骤在 Kubernetes 里面被称作 Bind。\n但是，为了不在关键调度路径里远程访问 APIServer，Kubernetes 的默认调度器在 Bind 阶段，只会更新 Scheduler Cache 里的 Pod 和 Node 的信息。这种基于“乐观”假设的 API 对象更新方式，在 Kubernetes 里被称作 Assume。\nAssume 之后，调度器才会创建一个 Goroutine 来异步地向 APIServer 发起更新 Pod 的请求，来真正完成 Bind 操作。如果这次异步的 Bind 过程失败了，其实也没有太大关系，等 Scheduler Cache 同步之后一切就会恢复正常。\n当然，正是由于上述 Kubernetes 调度器的“乐观”绑定的设计，当一个新的 Pod 完成调度需要在某个节点上运行起来之前，该节点上的 kubelet 还会通过一个叫作 Admit 的操作来再次验证该 Pod 是否确实能够运行在该节点上。这一步 Admit 操作，实际上就是把一组叫作 GeneralPredicates 的、最基本的调度算法，比如：“资源是否可用”“端口是否冲突”等再执行一遍，作为 kubelet 端的二次确认。\n 备注：关于 Kubernetes 默认调度器的调度算法，我会在下一篇文章里为你讲解。\n 除了上述的“Cache 化”和“乐观绑定”，Kubernetes 默认调度器还有一个重要的设计，那就是“无锁化”。\n在 Scheduling Path 上，调度器会启动多个 Goroutine 以节点为粒度并发执行 Predicates 算法，从而提高这一阶段的执行效率。而与之类似的，Priorities 算法也会以 MapReduce 的方式并行计算然后再进行汇总。而在这些所有需要并发的路径上，调度器会避免设置任何全局的竞争资源，从而免去了使用锁进行同步带来的巨大的性能损耗。\n所以，在这种思想的指导下，如果你再去查看一下前面的调度器原理图，你就会发现，Kubernetes 调度器只有对调度队列和 Scheduler Cache 进行操作时，才需要加锁。而这两部分操作，都不在 Scheduling Path 的算法执行路径上。\n当然，Kubernetes 调度器的上述设计思想，也是在集群规模不断增长的演进过程中逐步实现的。尤其是 “Cache 化”，这个变化其实是最近几年 Kubernetes 调度器性能得以提升的一个关键演化。\n不过，随着 Kubernetes 项目发展到今天，它的默认调度器也已经来到了一个关键的十字路口。事实上，Kubernetes 现今发展的主旋律，是整个开源项目的“民主化”。也就是说，Kubernetes 下一步发展的方向，是组件的轻量化、接口化和插件化。所以，我们才有了 CRI、CNI、CSI、CRD、Aggregated APIServer、Initializer、Device Plugin 等各个层级的可扩展能力。可是，默认调度器，却成了 Kubernetes 项目里最后一个没有对外暴露出良好定义过的、可扩展接口的组件。\n当然，这是有一定的历史原因的。在过去几年，Kubernetes 发展的重点，都是以功能性需求的实现和完善为核心。在这个过程中，它的很多决策，还是以优先服务公有云的需求为主，而性能和规模则居于相对次要的位置。\n而现在，随着 Kubernetes 项目逐步趋于稳定，越来越多的用户开始把 Kubernetes 用在规模更大、业务更加复杂的私有集群当中。很多以前的 Mesos 用户，也开始尝试使用 Kubernetes 来替代其原有架构。在这些场景下，对默认调度器进行扩展和重新实现，就成了社区对 Kubernetes 项目最主要的一个诉求。\n所以，Kubernetes 的默认调度器，是目前这个项目里为数不多的、正在经历大量重构的核心组件之一。这些正在进行的重构的目的，一方面是将默认调度器里大量的“技术债”清理干净；另一方面，就是为默认调度器的可扩展性设计进行铺垫。\n而 Kubernetes 默认调度器的可扩展性设计，可以用如下所示的一幅示意图来描述：\n可以看到，默认调度器的可扩展机制，在 Kubernetes 里面叫作 Scheduler Framework。顾名思义，这个设计的主要目的，就是在调度器生命周期的各个关键点上，为用户暴露出可以进行扩展和实现的接口，从而实现由用户自定义调度器的能力。\n上图中，每一个绿色的箭头都是一个可以插入自定义逻辑的接口。比如，上面的 Queue 部分，就意味着你可以在这一部分提供一个自己的调度队列的实现，从而控制每个 Pod 开始被调度（出队）的时机。\n而 Predicates 部分，则意味着你可以提供自己的过滤算法实现，根据自己的需求，来决定选择哪些机器。\n需要注意的是，上述这些可插拔式逻辑，都是标准的 Go 语言插件机制（Go plugin 机制），也就是说，你需要在编译的时候选择把哪些插件编译进去。\n有了上述设计之后，扩展和自定义 Kubernetes 的默认调度器就变成了一件非常容易实现的事情。这也意味着默认调度器在后面的发展过程中，必然不会在现在的实现上再添加太多的功能，反而还会对现在的实现进行精简，最终成为 Scheduler Framework 的一个最小实现。而调度领域更多的创新和工程工作，就可以交给整个社区来完成了。这个思路，是完全符合我在前面提到的 Kubernetes 的“民主化”设计的。\n不过，这样的 Scheduler Framework 也有一个不小的问题，那就是一旦这些插入点的接口设计不合理，就会导致整个生态没办法很好地把这个插件机制使用起来。而与此同时，这些接口本身的变更又是一个费时费力的过程，一旦把控不好，就很可能会把社区推向另一个极端，即：Scheduler Framework 没法实际落地，大家只好都再次 fork kube-scheduler。\n总结\n在本篇文章中，我为你详细讲解了 Kubernetes 里默认调度器的设计与实现，分析了它现在正在经历的重构，以及未来的走向。不难看到，在 Kubernetes 的整体架构中，kube-scheduler 的责任虽然重大，但其实它却是在社区里最少受到关注的组件之一。这里的原因也很简单，调度这个事情，在不同的公司和团队里的实际需求一定是大相径庭的，上游社区不可能提供一个大而全的方案出来。所以，将默认调度器进一步做轻做薄，并且插件化，才是 kube-scheduler 正确的演进方向。\n思考题\n请问，Kubernetes 默认调度器与 Mesos 的“两级”调度器，有什么异同呢？\n问题回答：messos二级调度是资源调度和业务调度分开；优点：插件化调度框架（用户可以自定义自己调度器然后注册到messos资源调度框架即可），灵活可扩展性高.缺点：资源和业务调度分开无法获取资源使用情况，进而无法做更细粒度的调度.k8s调度是统一调度也就是业务和资源调度进行统一调度，可以进行更细粒度的调度；缺点其调度器扩展性差。\n","date":"2021年04月21日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter6/default-scheduler/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文来自张磊老师的\u003ca href=\"https://time.geekbang.org/column/article/69678\"\u003e《深入剖析Kuberntes》\u003c/a\u003e课程笔记，请勿转载。\u003c/p\u003e\n\u003c/blockquote\u003e","title":"十字路口上的Kubernetes默认调度器"},{"contents":"QOS 的作用请参考上面的几篇文章Kubernetes的资源模型和资源管理\nqos\n// pkg/apis/core/v1/helper/qos/qos.go func GetPodQOS(pod *v1.Pod) v1.PodQOSClass { requests := v1.ResourceList{} limits := v1.ResourceList{} zeroQuantity := resource.MustParse(\u0026quot;0\u0026quot;) isGuaranteed := true allContainers := []v1.Container{} allContainers = append(allContainers, pod.Spec.Containers...) allContainers = append(allContainers, pod.Spec.InitContainers...) for _, container := range allContainers { // process requests for name, quantity := range container.Resources.Requests { if !isSupportedQoSComputeResource(name) { continue } if quantity.Cmp(zeroQuantity) == 1 { delta := quantity.DeepCopy() if _, exists := requests[name]; !exists { requests[name] = delta } else { delta.Add(requests[name]) requests[name] = delta } } } // process limits qosLimitsFound := sets.NewString() for name, quantity := range container.Resources.Limits { if !isSupportedQoSComputeResource(name) { continue } if quantity.Cmp(zeroQuantity) == 1 { qosLimitsFound.Insert(string(name)) delta := quantity.DeepCopy() if _, exists := limits[name]; !exists { limits[name] = delta } else { delta.Add(limits[name]) limits[name] = delta } } } if !qosLimitsFound.HasAll(string(v1.ResourceMemory), string(v1.ResourceCPU)) { isGuaranteed = false } } if len(requests) == 0 \u0026amp;\u0026amp; len(limits) == 0 { return v1.PodQOSBestEffort } // Check is requests match limits for all resources. if isGuaranteed { for name, req := range requests { if lim, exists := limits[name]; !exists || lim.Cmp(req) != 0 { isGuaranteed = false break } } } if isGuaranteed \u0026amp;\u0026amp; len(requests) == len(limits) { return v1.PodQOSGuaranteed } return v1.PodQOSBurstable }  上面有注释我就不过多介绍，非常的简单。\n下面这里是QOS OOM打分机制，通过给不同的pod打分来判断，哪些pod可以被优先kill掉，分数越高的越容易被kill。\npolicy\n//\\pkg\\kubelet\\qos\\policy.go // 分值越高越容易被kill const ( // KubeletOOMScoreAdj is the OOM score adjustment for Kubelet KubeletOOMScoreAdj int = -999 // KubeProxyOOMScoreAdj is the OOM score adjustment for kube-proxy KubeProxyOOMScoreAdj int = -999 guaranteedOOMScoreAdj int = -998 besteffortOOMScoreAdj int = 1000 )  policy#GetContainerOOMScoreAdjust\n// pkg/kubelet/qos/policy.go // GetContainerOOMScoreAdjust returns the amount by which the OOM score of all processes in the // container should be adjusted. // The OOM score of a process is the percentage of memory it consumes // multiplied by 10 (barring exceptional cases) + a configurable quantity which is between -1000 // and 1000. Containers with higher OOM scores are killed if the system runs out of memory. // See https://lwn.net/Articles/391222/ for more information. func GetContainerOOMScoreAdjust(pod *v1.Pod, container *v1.Container, memoryCapacity int64) int { if types.IsNodeCriticalPod(pod) { // Only node critical pod should be the last to get killed. return guaranteedOOMScoreAdj } switch v1qos.GetPodQOS(pod) { case v1.PodQOSGuaranteed: // Guaranteed containers should be the last to get killed. return guaranteedOOMScoreAdj case v1.PodQOSBestEffort: return besteffortOOMScoreAdj } // Burstable containers are a middle tier, between Guaranteed and Best-Effort. Ideally, // we want to protect Burstable containers that consume less memory than requested. // The formula below is a heuristic. A container requesting for 10% of a system's // memory will have an OOM score adjust of 900. If a process in container Y // uses over 10% of memory, its OOM score will be 1000. The idea is that containers // which use more than their request will have an OOM score of 1000 and will be prime // targets for OOM kills. // Note that this is a heuristic, it won't work if a container has many small processes. memoryRequest := container.Resources.Requests.Memory().Value() oomScoreAdjust := 1000 - (1000*memoryRequest)/memoryCapacity // A guaranteed pod using 100% of memory can have an OOM score of 10. Ensure // that burstable pods have a higher OOM score adjustment. if int(oomScoreAdjust) \u0026lt; (1000 + guaranteedOOMScoreAdj) { return (1000 + guaranteedOOMScoreAdj) } // Give burstable pods a higher chance of survival over besteffort pods. if int(oomScoreAdjust) == besteffortOOMScoreAdj { return int(oomScoreAdjust - 1) } return int(oomScoreAdjust) }  这个函数返回容器中所有进程的OOM分数。进程的OOM分数是其消耗的内存百分比乘以10(除非有特殊情况)再加上一个可配置的数，这个数在 -1000 到 1000 之间。当系统内存不足时，OOM 分数越高的容器越容易被kill掉。\n对于静态Pod、镜像Pod和高优先级Pod，QoS直接被设置成为Guaranteed，而 Guaranteed 等级的容器最后被 kill。Besteffort 等级的容器最容易被删掉，而处于中间位置的 Burstable 等级的 Pod。\n理想情况下，我们要保护消耗少于请求的内存的Burstable容器。这里采用了一种启发式计算：\noomScoreAdjust = 1000 - (1000*memoryRequest)/memoryCapacity   问题：这里的memoryRequest 单位是什么？为什么要乘以1000\n 请求系统内存的10％的容器的OOM得分调整为900。如果容器Y中的进程使用了超过10％的内存，则其OOM得分将为1000。容器使用的内存比其请求多，那么OOM得分将为1000，并且将是OOM kill的主要目标。如果分数小于1000 + guaranteedOOMScoreAdj，也就是2分，那么被直接设置成2分，避免分数过低。\n参考文献\n https://lwn.net/Articles/391222/ ","date":"2021年04月20日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter6/qos-sc/","summary":"\u003cp\u003eQOS 的作用请参考上面的几篇文章\u003ca href=\"https://chenxq.xyz/post/cloud-k8s-resource-model-and-resource-manager/\"\u003eKubernetes的资源模型和资源管理\u003c/a\u003e\u003c/p\u003e","title":"QoS 源代码分析"},{"contents":"二叉树的遍历方法分为先序遍历，中序遍历，后序遍历以及层序遍历这四种，其中先序，中序以及后序又可以用递归和非递归的方式来实现，层序遍历一般则是用一个队列来实现。关于这几种遍历方式和代码可以参考本博客的之前的一篇文章\u0026mdash;\u0026gt;传送门\n这里我们通过一道 leetcode 题目来对二叉树的中序遍历法展开讨论。这道题的描述如下：\nleetcode 94 [https://leetcode-cn.com/problems/binary-tree-inorder-traversal/] 给定一个二叉树的根节点 root ，返回它的 中序 遍历。 示例1： 1 \\ 2 / 3 输入：root = [1,null,2,3] 输出：[1,3,2]  1. 递归 首先我们需要了解什么是二叉树的中序遍历：按照访问左子树——根节点——右子树的方式遍历这棵树，而在访问左子树或者右子树的时候我们按照同样的方式遍历，直到遍历完整棵树。因此整个遍历过程天然具有递归的性质，我们可以直接用递归函数来模拟这一过程。\n定义 inorder(root) 表示当前遍历到root节点的答案，那么按照定义，我们只要递归调用 inorder(root.left) 来遍历 root 节点的左子树，然后将 root 节点的值加入答案，再递归调用 inorder(root.right) 来遍历 root 节点的右子树即可，递归终止的条件为碰到空节点。\nclass Solution { public List\u0026lt;Integer\u0026gt; inorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;Integer\u0026gt;(); inorder(root, res); return res; } public void inorder(TreeNode root, List\u0026lt;Integer\u0026gt; res) { if (root == null) { return; } inorder(root.left, res); res.add(root.val); inorder(root.right, res); } }  2. 迭代 方法一的递归函数我们也可以用迭代的方式实现，两种方式是等价的，区别在于递归的时候隐式地维护了一个栈，而我们在迭代的时候需要显式地将这个栈模拟出来，其他都相同，具体实现可以看下面的代码。\nclass Solution { public List\u0026lt;Integer\u0026gt; inorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;Integer\u0026gt;(); // 定义一个栈 Deque\u0026lt;TreeNode\u0026gt; stk = new LinkedList\u0026lt;TreeNode\u0026gt;(); // 节点不为空，并且栈不为空 while (root != null || !stk.isEmpty()) { // 一直走到最左的节点，边走边压栈 while (root != null) { stk.push(root); root = root.left; } // 从最左的节点开始弹栈,并且判断右节点是否为空 root = stk.pop(); res.add(root.val); root = root.right; } return res; } }  3. Morris 中序遍历 Morris 遍历算法是另一种遍历二叉树的方法，它能将非递归的中序遍历空间复杂度降为 O(1)。\nMorris 遍历算法整体步骤如下（假设当前遍历到的节点为 x）：\n 如果 x 无左孩子，先将 x 的值加入答案数组，再访问 x 的右孩子，即 x=x.right。 如果 x 有左孩子，则找到 x 左子树上最右的节点（即左子树中序遍历的最后一个节点，x 在中序遍历中的前驱节点），我们记为 predecessor。根据 predecessor 的右孩子是否为空，进行如下操作。 如果 predecessor 的右孩子为空，则将其右孩子指向 x，然后访问 x 的左孩子，即 x=x.left。 如果 predecessor 的右孩子不为空，则此时其右孩子指向 x，说明我们已经遍历完 x 的左子树，我们将 predecessor 的右孩子置空，将 x 的值加入答案数组，然后访问 x 的右孩子，即 x=x.right。 重复上述操作，直至访问完整棵树。  其实整个过程我们就多做一步：假设当前遍历到的节点为 x，将 x 的左子树中最右边的节点的右孩子指向 x，这样在左子树遍历完成后我们通过这个指向走回了 x，且能通过这个指向知晓我们已经遍历完成了左子树，而不用再通过栈来维护，省去了栈的空间复杂度。\nclass Solution { public List\u0026lt;Integer\u0026gt; inorderTraversal(TreeNode root) { List\u0026lt;Integer\u0026gt; res = new ArrayList\u0026lt;Integer\u0026gt;(); TreeNode predecessor = null; while (root != null) { if (root.left != null) { // predecessor 节点就是当前 root 节点向左走一步，然后一直向右走至无法走为止 predecessor = root.left; while (predecessor.right != null \u0026amp;\u0026amp; predecessor.right != root) { predecessor = predecessor.right; } // 让 predecessor 的右指针指向 root，继续遍历左子树 if (predecessor.right == null) { predecessor.right = root; root = root.left; } // 说明左子树已经访问完了，我们需要断开链接 else { res.add(root.val); predecessor.right = null; root = root.right; } } // 如果没有左孩子，则直接访问右孩子 else { res.add(root.val); root = root.right; } } return res; } }  复杂度分析\n时间复杂度：O(n)，其中 n 为二叉搜索树的节点个数。Morris 遍历中每个节点会被访问两次，因此总时间复杂度为 O(2n)=O(n)。\n空间复杂度：O(1)。\n","date":"2021年04月15日","permalink":"https://ahamoment.cn/posts/algorithm/algorithm-tree-inorder/","summary":"二叉树的遍历方法分为先序遍历，中序遍历，后序遍历以及层序遍历这四种，其中先序，中序以及后序又可以用递归和非递归的方式来实现，层序遍历一般则是","title":"二叉树的中序遍历"},{"contents":" 本文来自张磊老师的《深入剖析Kuberntes》课程笔记，请勿转载。\n 作为一个容器集群编排与管理项目，Kubernetes 为用户提供的基础设施能力，不仅包括了我在前面为你讲述的应用定义和描述的部分，还包括了对应用的资源管理和调度的处理。那么，从今天这篇文章开始，我就来为你详细讲解一下后面这部分内容。\n1. 资源模型 而作为 Kubernetes 的资源管理与调度部分的基础，我们要从它的资源模型开始说起。\n我在前面的文章中已经提到过，在 Kubernetes 里，Pod 是最小的原子调度单位。这也就意味着，所有跟调度和资源管理相关的属性都应该是属于 Pod 对象的字段。而这其中最重要的部分，就是 Pod 的 CPU 和内存配置，如下所示：\napiVersion: v1 kind: Pod metadata: name: frontend spec: containers: - name: db image: mysql env: - name: MYSQL_ROOT_PASSWORD value: \u0026quot;password\u0026quot; resources: requests: memory: \u0026quot;64Mi\u0026quot; cpu: \u0026quot;250m\u0026quot; limits: memory: \u0026quot;128Mi\u0026quot; cpu: \u0026quot;500m\u0026quot; - name: wp image: wordpress resources: requests: memory: \u0026quot;64Mi\u0026quot; cpu: \u0026quot;250m\u0026quot; limits: memory: \u0026quot;128Mi\u0026quot; cpu: \u0026quot;500m\u0026quot;   备注：关于哪些属性属于 Pod 对象，而哪些属性属于 Container，你可以在回顾一下第 14 篇文章《深入解析 Pod 对象（一）：基本概念》中的相关内容。\n 在 Kubernetes 中，像 CPU 这样的资源被称作“可压缩资源”（compressible resources）。它的典型特点是，当可压缩资源不足时，Pod 只会“饥饿”，但不会退出。而像内存这样的资源，则被称作“不可压缩资源（incompressible resources）。当不可压缩资源不足时，Pod 就会因为 OOM（Out-Of-Memory）被内核杀掉。\n而由于 Pod 可以由多个 Container 组成，所以 CPU 和内存资源的限额，是要配置在每个 Container 的定义上的。这样，Pod 整体的资源配置，就由这些 Container 的配置值累加得到。\n其中，Kubernetes 里为 CPU 设置的单位是“CPU 的个数”。比如，cpu=1 指的就是，这个 Pod 的 CPU 限额是 1 个 CPU。当然，具体“1 个 CPU”在宿主机上如何解释，是 1 个 CPU 核心，还是 1 个 vCPU，还是 1 个 CPU 的超线程（Hyperthread），完全取决于宿主机的 CPU 实现方式。Kubernetes 只负责保证 Pod 能够使用到“1 个 CPU”的计算能力。\n 备注：关于CPU的几个概念，物理CPU，物理核，逻辑CPU，vCPU的区别\na. 物理CPU:物理CPU是相对于虚拟CPU而言的概念，指实际存在的处理器,就是我们可以看的见，摸得着的CPU，就是插在主板上面的。\nb. 物理核：CPU中包含的物理内核个数，比如我们通常说的双核CPU，单核CPU。这个呢有点看不见摸不着，已经集成在CPU内部了。在linux系统下面的/proc/cpuinfo文件的条目中：1.有多少个不同的physical id就有多少个物理CPU。2.cpu cores记录了对应的物理CPU（以该条目中的physical id标识）有多少个物理核，现在我们个人使用的单机PC大部分使用的都是双核CPU。\nc. 逻辑CPU（逻辑核）：用Intel的超线程技术(HT)将物理核虚拟而成的逻辑处理单元,现在大部分的主机的CPU都在使用HT技术，我们在windows系统下面看下图，我们看到有4个cpu记录，其实我们使用的双核CPU只是使用HT技术虚拟出来4个逻辑CPU.在linux系统下面的/proc/cpuinfo文件的条目中siblings记录了对应的物理CPU（以该条目中的physical id标识）有多少个逻辑核。\nd. vCPU:虚拟cpu是我们在做虚拟化时候，利用虚拟化技术，虚拟出来的CPU。讨论vCPU离不开VM，因此vCPU的讨论都是在虚拟化时候，划分cpu才会讨论的问题。通常一个物理CPU按照1:4——1：10的比例划分，假如我们有4个8物理核心的CPU按照1:5的比例划分，可以得到4X8X5=160vCPU.\n 此外，Kubernetes 允许你将 CPU 限额设置为分数，比如在我们的例子里，CPU limits 的值就是 500m。所谓 500m，指的就是 500 millicpu，也就是 0.5 个 CPU 的意思。这样，这个 Pod 就会被分配到 1 个 CPU 一半的计算能力。\n当然，你也可以直接把这个配置写成 cpu=0.5。但在实际使用时，我还是推荐你使用 500m 的写法，毕竟这才是 Kubernetes 内部通用的 CPU 表示方式。\n而对于内存资源来说，它的单位自然就是 bytes。Kubernetes 支持你使用 Ei、Pi、Ti、Gi、Mi、Ki（或者 E、P、T、G、M、K）的方式来作为 bytes 的值。比如，在我们的例子里，Memory requests 的值就是 64MiB (2 的 26 次方 bytes) 。这里要注意区分 MiB（mebibyte）和 MB（megabyte）的区别。\n 备注：1Mi=1024*1024；1M=1000*1000\n 此外，不难看到，Kubernetes 里 Pod 的 CPU 和内存资源，实际上还要分为 limits 和 requests 两种情况，如下所示：\nspec.containers[].resources.limits.cpu spec.containers[].resources.limits.memory spec.containers[].resources.requests.cpu spec.containers[].resources.requests.memory  这两者的区别其实非常简单：在调度的时候，kube-scheduler 只会按照 requests 的值进行计算。而在真正设置 Cgroups 限制的时候，kubelet 则会按照 limits 的值来进行设置。\n更确切地说，当你指定了 requests.cpu=250m 之后，相当于将 Cgroups 的 cpu.shares 的值设置为 (250/1000)*1024。而当你没有设置 requests.cpu 的时候，cpu.shares 默认则是 1024。这样，Kubernetes 就通过 cpu.shares 完成了对 CPU 时间的按比例分配。\n而如果你指定了 limits.cpu=500m 之后，则相当于将 Cgroups 的 cpu.cfs_quota_us 的值设置为 (500/1000)*100ms，而 cpu.cfs_period_us 的值始终是 100ms。这样，Kubernetes 就为你设置了这个容器只能用到 CPU 的 50%。\n而对于内存来说，当你指定了 limits.memory=128Mi 之后，相当于将 Cgroups 的 memory.limit_in_bytes 设置为 128 * 1024 * 1024。而需要注意的是，在调度的时候，调度器只会使用 requests.memory=64Mi 来进行判断。\n 关于cpu.share 和 cpu.quotas 的概念，可以参考这两篇文章，第一篇文章的那个例子很好，看懂了就基本上明白这两个概念之间的关系了：\n 从CFS的层面来分析docker是如何限制容器对CPU的使用的 Understanding Linux Container Scheduling  我们简单理解，shares 的大小决定了在一个CFS调度周期中，进程占用的比例，比如进程A的shares是1024，B的shares是512，假设调度周期为3秒，那么A将只用2秒，B将使用1秒。而 quotas 通常和 period 一起使用，指的是在一个重分配周期内，容器能够占用的最大时间。因为 shares 是不会限制最大的占用时间的，所以即使设置了shares的值，也可能会跑满整个cpu，这时候就需要 quotas 的值来限制了。\n Kubernetes 这种对 CPU 和内存资源限额的设计，实际上参考了 Borg 论文中对“动态资源边界”的定义，既：容器化作业在提交时所设置的资源边界，并不一定是调度系统所必须严格遵守的，这是因为在实际场景中，大多数作业使用到的资源其实远小于它所请求的资源限额。\n而 Kubernetes 的 requests+limits 的做法，其实就是上述思路的一个简化版：用户在提交 Pod 时，可以声明一个相对较小的 requests 值供调度器使用，而 Kubernetes 真正设置给容器 Cgroups 的，则是相对较大的 limits 值。不难看到，这跟 Borg 的思路相通的。\n2. QoS 模型 在理解了 Kubernetes 资源模型的设计之后，我再来和你谈谈 Kubernetes 里的 QoS 模型。在 Kubernetes 中，不同的 requests 和 limits 的设置方式，其实会将这个 Pod 划分到不同的 QoS 级别当中。\n   级别 条件     Guaranteed 1.Pod 中的每个容器，包含初始化容器，必须指定内存请求和内存限制，并且两者要相等。 2.Pod 中的每个容器，包含初始化容器，必须指定 CPU 请求和 CPU 限制，并且两者要相等。   Burstable 1.Pod 不符合 Guaranteed QoS 类的标准。2.Pod 中至少一个容器具有内存或 CPU 请求。   BestEffort 对于 QoS 类为 BestEffort 的 Pod，Pod 中的容器必须没有设置内存和 CPU 限制或请求。    当 Pod 里的每一个 Container 都同时设置了 requests 和 limits，并且 requests 和 limits 值相等的时候，这个 Pod 就属于 Guaranteed 类别，如下所示：\napiVersion: v1 kind: Pod metadata: name: qos-demo namespace: qos-example spec: containers: - name: qos-demo-ctr image: nginx resources: limits: memory: \u0026quot;200Mi\u0026quot; cpu: \u0026quot;700m\u0026quot; requests: memory: \u0026quot;200Mi\u0026quot; cpu: \u0026quot;700m\u0026quot;  当这个 Pod 创建之后，它的 qosClass 字段就会被 Kubernetes 自动设置为 Guaranteed。需要注意的是，当 Pod 仅设置了 limits 没有设置 requests 的时候，Kubernetes 会自动为它设置与 limits 相同的 requests 值，所以，这也属于 Guaranteed 情况。而当 Pod 不满足 Guaranteed 的条件，但至少有一个 Container 设置了 requests。那么这个 Pod 就会被划分到 Burstable 类别。比如下面这个例子：\napiVersion: v1 kind: Pod metadata: name: qos-demo-2 namespace: qos-example spec: containers: - name: qos-demo-2-ctr image: nginx resources: limits memory: \u0026quot;200Mi\u0026quot; requests: memory: \u0026quot;100Mi\u0026quot;  而如果一个 Pod 既没有设置 requests，也没有设置 limits，那么它的 QoS 类别就是 BestEffort。比如下面这个例子：\napiVersion: v1 kind: Pod metadata: name: qos-demo-3 namespace: qos-example spec: containers: - name: qos-demo-3-ctr image: nginx  那么，Kubernetes 为 Pod 设置这样三种 QoS 类别，具体有什么作用呢？\n实际上，QoS 划分的主要应用场景，是当宿主机资源紧张的时候，kubelet 对 Pod 进行 Eviction（即资源回收）时需要用到的。\n具体地说，当 Kubernetes 所管理的宿主机上不可压缩资源短缺时，就有可能触发 Eviction。比如，可用内存（memory.available）、可用的宿主机磁盘空间（nodefs.available），以及容器运行时镜像存储空间（imagefs.available）等等。\n目前，Kubernetes 为你设置的 Eviction 的默认阈值如下所示：\nmemory.available\u0026lt;100Mi nodefs.available\u0026lt;10% nodefs.inodesFree\u0026lt;5% imagefs.available\u0026lt;15%  当然，上述各个触发条件在 kubelet 里都是可配置的。\n阈值的定义方式为 [eviction-signal][operator][quantity]\neviction-signal\n按照官方文档分为以下几种：\n   Eviction Signal Description     memory.available memory.available := node.status.capacity[memory] – node.stats.memory.workingSet   nodefs.available nodefs.available := node.stats.fs.available   nodefs.inodesFree nodefs.inodesFree := node.stats.fs.inodesFree   imagefs.available imagefs.available := node.stats.runtime.imagefs.available   imagefs.inodesFree imagefs.inodesFree := node.stats.runtime.imagefs.inodesFree    nodefs和imagefs表示两种文件系统分区：\n  nodefs：文件系统，kubelet 将其用于卷和守护程序日志等。\n  imagefs：文件系统，容器运行时用于保存镜像和容器可写层。\n  operator\n关系运算符，例如“\u0026lt;”\nquantity\n是阈值的大小，可以容量大小，如：1Gi；也可以用百分比来表示：10%。\n如果kubelet在节点经历系统 OOM 之前无法回收内存，那么oom_killer将基于它在节点上 使用的内存百分比算出一个oom_score，然后结束得分最高的容器。这些值我们是可以设置的，比如下面这个例子：\nkubelet --eviction-hard=imagefs.available\u0026lt;10%,memory.available\u0026lt;500Mi,nodefs.available\u0026lt;5%,nodefs.inodesFree\u0026lt;5% --eviction-soft=imagefs.available\u0026lt;30%,nodefs.available\u0026lt;10% --eviction-soft-grace-period=imagefs.available=2m,nodefs.available=2m --eviction-max-pod-grace-period=600  在这个配置中，你可以看到 Eviction 在 Kubernetes 里其实分为 Soft 和 Hard 两种模式。\n其中，Soft Eviction 允许你为 Eviction 过程设置一段“优雅时间”，比如上面例子里的 imagefs.available=2m，就意味着当 imagefs 不足的阈值达到 2 分钟之后，kubelet 才会开始 Eviction 的过程。\n而 Hard Eviction 模式下，Eviction 过程就会在阈值达到之后立刻开始。\n Kubernetes 计算 Eviction 阈值的数据来源，主要依赖于从 Cgroups 读取到的值，以及使用 cAdvisor 监控到的数据。\n 当宿主机的 Eviction 阈值达到后，就会进入 MemoryPressure 或者 DiskPressure 状态，从而避免新的 Pod 被调度到这台宿主机上。\n而当 Eviction 发生的时候，kubelet 具体会挑选哪些 Pod 进行删除操作，就需要参考这些 Pod 的 QoS 类别了。\n 首当其冲的，自然是 BestEffort 类别的 Pod。\n其次，是属于 Burstable 类别、并且发生“饥饿”的资源使用量已经超出了 requests 的 Pod。\n最后，才是 Guaranteed 类别。并且，Kubernetes 会保证只有当 Guaranteed 类别的 Pod 的资源使用量超过了其 limits 的限制，或者宿主机本身正处于 Memory Pressure 状态时，Guaranteed 的 Pod 才可能被选中进行 Eviction 操作。\n 当然，对于同 QoS 类别的 Pod 来说，Kubernetes 还会根据 Pod 的优先级来进行进一步地排序和选择。在理解了 Kubernetes 里的 QoS 类别的设计之后，我再来为你讲解一下Kubernetes 里一个非常有用的特性：cpuset 的设置。\n3. cpuset 我们知道，在使用容器的时候，你可以通过设置 cpuset 把容器绑定到某个 CPU 的核上，而不是像 cpushare 那样共享 CPU 的计算能力。\n这种情况下，由于操作系统在 CPU 之间进行上下文切换的次数大大减少，容器里应用的性能会得到大幅提升。事实上，cpuset 方式，是生产环境里部署在线应用类型的 Pod 时，非常常用的一种方式。\n可是，这样的需求在 Kubernetes 里又该如何实现呢？其实非常简单。\n 首先，你的 Pod 必须是 Guaranteed 的 QoS 类型；\n然后，你只需要将 Pod 的 CPU 资源的 requests 和 limits 设置为同一个相等的整数值即可。\n 比如下面这个例子：\nspec: containers: - name: nginx image: nginx resources: limits: memory: \u0026quot;200Mi\u0026quot; cpu: \u0026quot;2\u0026quot; requests: memory: \u0026quot;200Mi\u0026quot; cpu: \u0026quot;2\u0026quot;  这时候，该 Pod 就会被绑定在 2 个独占的 CPU 核上。当然，具体是哪两个 CPU 核，是由 kubelet 为你分配的。以上，就是 Kubernetes 的资源模型和 QoS 类别相关的主要内容。\n4. 总结 在本篇文章中，我先为你详细讲解了 Kubernetes 里对资源的定义方式和资源模型的设计。然后，我为你讲述了 Kubernetes 里对 Pod 进行 Eviction 的具体策略和实践方式。\n正是基于上述讲述，在实际的使用中，我强烈建议你将 DaemonSet 的 Pod 都设置为 Guaranteed 的 QoS 类型。否则，一旦 DaemonSet 的 Pod 被回收，它又会立即在原宿主机上被重建出来，这就使得前面资源回收的动作，完全没有意义了。\n","date":"2021年04月14日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter6/resource-model/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文来自张磊老师的\u003ca href=\"https://time.geekbang.org/column/article/69678\"\u003e《深入剖析Kuberntes》\u003c/a\u003e课程笔记，请勿转载。\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Kubernetes的资源模型和资源管理"},{"contents":"在使用 Kubernetes REST API 编写应用程序时， 您并不需要自己实现 API 调用和 “请求/响应” 类型。 您可以根据自己的编程语言需要选择使用合适的客户端库。\n客户端库通常为您处理诸如身份验证之类的常见任务。 如果 API 客户端在 Kubernetes 集群中运行，大多数客户端库可以发现并使用 Kubernetes 服务帐户进行身份验证， 或者能够理解 kubeconfig 文件 格式来读取凭据和 API 服务器地址。\nkubernetes 官方支持的客户端有 go/python/java/dotnet/js 等，今天我们要讨论的是其中的 go 客户端。\n首先下载源代码，进入到examples目录：\n➜ git clone git@github.com:kubernetes/client-go.git ➜ ~ cd client-go/examples ➜ examples git:(master) ✗ ll total 32K -rwxr-xr-x 1 root root 2.0K Apr 6 17:33 README.md drwxr-xr-x 2 root root 4.0K Apr 6 17:46 create-update-delete-deployment drwxr-xr-x 2 root root 4.0K Apr 6 17:33 dynamic-create-update-delete-deployment drwxr-xr-x 2 root root 4.0K Apr 6 17:33 fake-client drwxr-xr-x 2 root root 4.0K Apr 6 17:33 in-cluster-client-configuration drwxr-xr-x 2 root root 4.0K Apr 6 17:33 leader-election drwxr-xr-x 2 root root 4.0K Apr 6 17:33 out-of-cluster-client-configuration drwxr-xr-x 2 root root 4.0K Apr 6 17:33 workqueue  go 客户端提供了很多的使用样例，我们重点来看一下create-update-delete-deployment 这个目录，这个目录中是关于操作deployment资源对象的样例，包括创建，查询，更新，删除。\n This example program demonstrates the fundamental operations for managing on Deployment resources, such as Create, List, Update and Delete.\nYou can adopt the source code from this example to write programs that manage other types of resources through the Kubernetes API.\n 我们从 main 函数开始看起，这里尝试使用 go-callvis 将整个调用关系可视化出来：\n总的来看，main 使用 kubeconfig 的配置来生成 rest client，通过 rest client 调用 k8s api 进行资源的操作。\n接下来，具体来看一下读取 kubeconfig 配置的代码：\nvar kubeconfig *string if home := homedir.HomeDir(); home != \u0026quot;\u0026quot; { kubeconfig = flag.String(\u0026quot;kubeconfig\u0026quot;, filepath.Join(home, \u0026quot;.kube\u0026quot;, \u0026quot;config\u0026quot;), \u0026quot;(optional) absolute path to the kubeconfig file\u0026quot;) } else { kubeconfig = flag.String(\u0026quot;kubeconfig\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;absolute path to the kubeconfig file\u0026quot;) } flag.Parse() config, err := clientcmd.BuildConfigFromFlags(\u0026quot;\u0026quot;, *kubeconfig)  命令行参数指定kubeconfig的绝对路径，调用clientcmd.BuildConfigFromFlags(\u0026quot;\u0026quot;, *kubeconfig)来解析 config 的配置信息。 BuildConfigFromFlags 函数是用来从master地址或者kubeconfig文件地址来构建config配置，这个函数做了两件事情：\n 调用func NewNonInteractiveDeferredLoadingClientConfig(loader ClientConfigLoader, overrides *ConfigOverrides) ClientConfig 调用 func (config *DeferredLoadingClientConfig) ClientConfig() (*restclient.Config, error) 函数  func BuildConfigFromFlags(masterUrl, kubeconfigPath string) (*restclient.Config, error) { ...... return NewNonInteractiveDeferredLoadingClientConfig( \u0026amp;ClientConfigLoadingRules{ExplicitPath: kubeconfigPath}, \u0026amp;ConfigOverrides{ClusterInfo: clientcmdapi.Cluster{Server: masterUrl}}).ClientConfig() }  我们来看看这两个链式函数做了什么事情：\nfunc NewNonInteractiveDeferredLoadingClientConfig(loader ClientConfigLoader, overrides *ConfigOverrides) ClientConfig { return \u0026amp;DeferredLoadingClientConfig{loader: loader, overrides: overrides, icc: \u0026amp;inClusterClientConfig{overrides: overrides}} }  NewNonInteractiveDeferredLoadingClientConfig 函数返回一个 ClientConfig 接口的实现: DeferredLoadingClientConfig。DeferredLoadingClientConfig 主要工作是确保装载的 Config 实例使用的是最新 kubeconfig 数据（对于配置了多个集群的，export KUBECONFIG=cluster1-config:cluster2-config，需要执行 merge）。在这个结构体的注释中写道：\nIt is used in cases where the loading rules may change after you've instantiated them and you want to be sure that the most recent rules are used. This is useful in cases where you bind flags to loading rule parameters before the parse happens and you want your calling code to be ignorant of how the values are being mutated to avoid passing extraneous information down a call stack 实例化加载规则后，如果要更改加载规则，并且要确保使用最新的规则，则可以使用它。这在以下情况下很有用： 在解析发生之前将标志绑定到加载规则参数，并且您希望您的调用代码不知道值的变化方式，以避免在调用堆栈中传递无关的信息  上一个函数返回了 ClientConfig 接口实例。然后调用 ClientConfig 接口定义的 ClientConfig() 方法。ClientConfig() 工作是解析、处理 kubeconfig 文件里的认证信息，并返回一个完整的 rest#Config 实例。\n// ClientConfig implements ClientConfig func (config *DeferredLoadingClientConfig) ClientConfig() (*restclient.Config, error) { mergedClientConfig, err := config.createClientConfig() ...... // load the configuration and return on non-empty errors and if the // content differs from the default config mergedConfig, err := mergedClientConfig.ClientConfig() ...... // check for in-cluster configuration and use it if config.icc.Possible() { klog.V(4).Infof(\u0026quot;Using in-cluster configuration\u0026quot;) return config.icc.ClientConfig() } // return the result of the merged client config return mergedConfig, err }  这个函数主要有两个重要部分：\n1.mergedClientConfig, err := config.createClientConfig()\n内部执行遍历 kubeconfig files （如果有多个）， 对每个 kubeconfig 执行 LoadFromFile 返回 tools/clientcmd/api#Config 实例。api#Config 顾名思义 api 包下的 Config，是把 kubeconfig （eg. $HOME/.kube/config） 序列化为一个 API 资源对象。\n现在,我们看到了几种结构体或接口命名相似，不要混淆了：\n api#Config：序列化 kubeconfig 文件后生成的对象 tools/clientcmd#ClientConfig：负责用 api#Config 真正创建 rest#Config。处理、解析 kubeconfig 中的认证信息，有了它才能创建 rest#Config，所以命名叫 ClientConfig rest#Config：用于创建 http 客户端  对于 merge 后的 api#Config，调用 NewNonInteractiveClientConfig 创建一个 ClientConfig 接口的实现。\n2.mergedConfig, err := mergedClientConfig.ClientConfig()\n真正创建 rest#Config 的地方。在这里解析、处理 kubeconfig 中的认证信息。\n完成 rest client 创建之后，就需要创建 clientset :\nclientset, err := kubernetes.NewForConfig(config)  这里的 func NewForConfig(c *rest.Config) (*Clientset, error) 函数会根据 api group 为不同的资源生成客户端。例如这里用到的 deployment 的客户端，定义了deployment的namespace：\ndeploymentsClient := clientset.AppsV1().Deployments(apiv1.NamespaceDefault)  接下来创建 deployment 的描述信息：\ndeployment := \u0026amp;appsv1.Deployment{ ObjectMeta: metav1.ObjectMeta{ Name: \u0026quot;demo-deployment\u0026quot;, }, Spec: appsv1.DeploymentSpec{ Replicas: int32Ptr(2), Selector: \u0026amp;metav1.LabelSelector{ MatchLabels: map[string]string{ \u0026quot;app\u0026quot;: \u0026quot;demo\u0026quot;, }, }, Template: apiv1.PodTemplateSpec{ ObjectMeta: metav1.ObjectMeta{ Labels: map[string]string{ \u0026quot;app\u0026quot;: \u0026quot;demo\u0026quot;, }, }, Spec: apiv1.PodSpec{ Containers: []apiv1.Container{ { Name: \u0026quot;web\u0026quot;, Image: \u0026quot;nginx:1.12\u0026quot;, Ports: []apiv1.ContainerPort{ { Name: \u0026quot;http\u0026quot;, Protocol: apiv1.ProtocolTCP, ContainerPort: 80, }, }, }, }, }, }, }, }  创建deployment：\n// Create Deployment fmt.Println(\u0026quot;Creating deployment...\u0026quot;) result, err := deploymentsClient.Create(context.TODO(), deployment, metav1.CreateOptions{}) if err != nil { panic(err) } fmt.Printf(\u0026quot;Created deployment %q.\\n\u0026quot;, result.GetObjectMeta().GetName())  这个创建的函数实际上就是调用 HTTP 的 POST 请求来跟K8s集群进行通信的。\nfunc (c *deployments) Create(ctx context.Context, deployment *v1.Deployment, opts metav1.CreateOptions) (result *v1.Deployment, err error) { result = \u0026amp;v1.Deployment{} err = c.client.Post(). Namespace(c.ns). Resource(\u0026quot;deployments\u0026quot;). VersionedParams(\u0026amp;opts, scheme.ParameterCodec). Body(deployment). Do(ctx). Into(result) return }  至于post的请求细节，这里不做详细的阐述。\n参考：\nk8s informer\nKubernetes: Controllers, Informers, Reflectors and Stores\n解读 kubernetes client-go 官方 examples](https://segmentfault.com/a/1190000018953168)\nclient-go学习\ngo-callvis：callvis 查看代码调用关系工具，一个不错的调用链可视化工具，可以方便我们分析代码\n","date":"2021年04月06日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter6/client-go/","summary":"在使用 Kubernetes REST API 编写应用程序时， 您并不需要自己实现 API 调用和 “请求/响应” 类型。 您可以根据自己的编程语言需要选择使用合适的客户端库。 客户端库通常为","title":"K8s Go 客户端浅析"},{"contents":"Go 语言分配内存的函数包括 new 和 make。new 用来获取类型对应的指针类型，即要获取指针类型的内存分配。make 只用来分配引用类型，即为channel，map，slice分配内存。\n对于值类型的变量，我们通过var 声明，系统会默认为他分配内存空间，并赋该类型的零值。如下，我们声明一个int类型变量i，输出为0。\npackage main import \u0026quot;fmt\u0026quot; func main() { var i int fmt.Println(i) }  而如果我们声明一个指针类型的变量，系统不会为他分配内存，改变量默认就是nil。此时如果你想直接使用，那么系统会抛异常。\nvar j *int fmt.Println(j) *j = 10 //panic: runtime error: invalid memory address or nil pointer dereference  也就是说，空指针还没有内存分配，是不能使用的。那么要想使用，此时就需要new出场啦。\nvar j *int j = new(int) fmt.Println(j) fmt.Println(*j) *j = 10 fmt.Println(*j)  0xc00000a0e0 0 10  声明指针类型变量后，通过new为他分配内存，有了内存空间，这个变量就可以自由的使用啦。\n我们来看一下new函数\n// The new built-in function allocates memory. The first argument is a type, // not a value, and the value returned is a pointer to a newly // allocated zero value of that type. func new(Type) *Type  它只接受一个参数，这个参数是一个类型，分配好内存后，返回一个指向该类型内存地址的指针。同时把分配的内存置为零，也就是类型的零值。\n接着，我们来看一下make函数。\n// The make built-in function allocates and initializes an object of type // slice, map, or chan (only). Like new, the first argument is a type, not a // value. Unlike new, make's return type is the same as the type of its // argument, not a pointer to it. The specification of the result depends on // the type: //\tSlice: The size specifies the length. The capacity of the slice is //\tequal to its length. A second integer argument may be provided to //\tspecify a different capacity; it must be no smaller than the //\tlength. For example, make([]int, 0, 10) allocates an underlying array //\tof size 10 and returns a slice of length 0 and capacity 10 that is //\tbacked by this underlying array. //\tMap: An empty map is allocated with enough space to hold the //\tspecified number of elements. The size may be omitted, in which case //\ta small starting size is allocated. //\tChannel: The channel's buffer is initialized with the specified //\tbuffer capacity. If zero, or the size is omitted, the channel is //\tunbuffered. func make(t Type, size ...IntegerType) Type   make 是分配内存并初始化，初始化并不是置为零值。 与new一样，它的第一个参数也是一个类型，但是不一样的是，make返回的是传入的类型，而不是指针！  var c chan int fmt.Printf(\u0026quot;%#v \\n\u0026quot;,c) //(chan int)(nil) c = make(chan int) fmt.Printf(\u0026quot;%#v\u0026quot;, c) //(chan int)(0xc000062060)  声明管道类型变量c，此时c还是nil，不可用；通过make来分配内存并初始化，c就获得了内存可以使用了。\n","date":"2021年04月01日","permalink":"https://ahamoment.cn/posts/go/go-new-vs-make/","summary":"Go 语言分配内存的函数包括 new 和 make。new 用来获取类型对应的指针类型，即要获取指针类型的内存分配。make 只用来分配引用类型，即为chan","title":"Go 语言中的 new 关键字和 make 关键字的区别"},{"contents":"常用快捷键 Overall Mac 键盘快捷键\n修饰键  Command（或 Cmd）⌘ Shift ⇧ Option（或 Alt）⌥ Control（或 Ctrl）⌃ Caps Lock ⇪ Fn  在 Windows PC 专用键盘上，请用 Alt 键代替 Option 键，用 Windows 标志键代替 Command 键。\n截图   全屏\nshift + command + 3\n  截取部分\nshift + command + 4\n  打开截屏工具\nshift + command + 5\n  文稿编辑  Fn-上箭头：Page Up：向上滚动一页。 Fn–下箭头：Page Down：向下滚动一页。 **Fn–左箭头：**Home：滚动到文稿开头。 Fn–右箭头：End：滚动到文稿末尾。 Command–上箭头：将插入点移至文稿开头。 Command–下箭头：将插入点移至文稿末尾。 Command–左箭头：将插入点移至当前行的行首。 Command–右箭头：将插入点移至当前行的行尾。 Option–左箭头：将插入点移至上一字词的词首。 Option–右箭头：将插入点移至下一字词的词尾。  ","date":"2021年03月25日","permalink":"https://ahamoment.cn/posts/tool/tool-mac-os-commands/","summary":"常用快捷键 Overall Mac 键盘快捷键 修饰键 Command（或 Cmd）⌘ Shift ⇧ Option（或 Alt）⌥ Control（或 Ctrl）⌃ Caps Lock ⇪ Fn 在 Windows PC 专用键盘","title":"Mac 命令汇总"},{"contents":"安装 Anaconda https://www.anaconda.com/products/individual\n根据系统选择下载不同的 anaconda 安装：\n这里安装的是 MacOS 的软件包，安装完成后使用 conda 命令查看是否安装成功：\n$ conda --version conda 4.9.2  更换 conda 镜像源 查看用户目录下的 .condarc 文件：\n更换成清华大学镜像源\nchannels: - defaults show_channel_urls: true default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud  运行 conda clean -i 清除索引缓存，保证用的是镜像站提供的索引。\n运行 conda create -n myenv numpy 测试一下吧。\n更换 pip 的镜像源 默认的 pip 下载地址速度较慢，我们将其更换成清华大学的镜像地址：\n$ pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple Writing to /Users/yaoxiaojiao/.config/pip/pip.conf  详情查看：pypi 镜像使用帮助\n安装 Tensorflow 2.2 创建一个 TF2.2 的 conda 实验环境并安装相关软件包：\nconda create -n TF2.2 python=3.8  进入 TF2.2 环境中：\nconda activate TF2.2  安装英伟达显卡的 SDK Tensorflow2.1 匹配的 cuda toolkit 是 10.1，由于我的电脑上没有英伟达的显卡，无法安装，先记录下安装命令，后续找台有英伟达显卡的机器实验：\nconda install cudatoolkit=10.1  tf2.1对应的英伟达显卡的深度学习软件包版本为 cudnn7.6\nconda install cudnn=7.6  mac pro 由于没有英伟达的显卡，我们可以尝试通过 pycharm 远程连接解释器的方式，在远程一台有显卡的服务器上进行解析。详情参考：Remote Debugging with PyCharm\n安装 tensorflow 2.2\npip install tensorflow==2.2.0  验证 tensorflow2.0 安装成功：\nimport tensorflow as tf tf.__version__  参考 https://zhuanlan.zhihu.com/p/87123943\nhttps://www.jetbrains.com/help/pycharm/remote-debugging-with-product.html#remote-debug-config\n","date":"2021年03月25日","permalink":"https://ahamoment.cn/posts/machinelearning/ml-tensorflow2.0-install/","summary":"安装 Anaconda https://www.anaconda.com/products/individual 根据系统选择下载不同的 anaconda 安装： 这里安装的是 MacOS 的软件包，安装完成后使用 conda 命令查看是否安装成功： $ conda --version conda 4.9.2 更换 conda 镜像源 查看用户目录下的 .condarc","title":"安装 Tensorflow2"},{"contents":"折腾了两天的vim，想要把它变成我的机器上默认的IDE，方便在没有环境的时候，快速查阅代码，做一些基本的开发任务，折腾过程记录在这篇博客上\n环境 # cat /etc/*release CentOS Linux release 7.9.2009 (Core)  系统自带的vim是7.x，对于很多的插件来说(例如 ycm)都已经不兼容了，所以第一步就是要将 vim 的版本更新到 8.0 以上\n安装软件 编译vim之前先卸载掉旧版本的vim\nyum remove vim-enhanced vim-common vim-filesystem vim-minimal  为了编译能够正常，需要安装上一些依赖和常用的工具包：\n# install devtoolset-8 yum install centos-release-scl yum-config-manager --enable rhel-server-rhscl-7-rpms yum install devtoolset-8 scl enable devtoolset-8 bash # install cmake, python3-devel, etc. yum -y install git ncurses-devel ruby ruby-devel lua lua-devel perl perl-devel python3 python3-devel python2-devel perl-ExtUtils-Embed lrzsz cmake wget gcc gcc-c++ unzi  其中有部分工具是给后面安装插件使用的。\n下载vim的源代码：\ngit clone https://github.com/vim/vim.git cd vim # if you build vim before make distclean # config # --enable-fail-if-missing 表示问题会提示报错，并停止 # --enable-***interp=yes 表示加入***支持 # --with-***-config-dir=*** 表示指定配置文件路径 ./configure --with-features=huge \\ --enable-rubyinterp=yes \\ --enable-luainterp=yes \\ --enable-perlinterp=yes \\ --enable-python3interp=yes \\ --enable-pythoninterp=yes \\ --with-python-config-dir=/usr/lib64/python2.7/config \\ --with-python3-config-dir=/usr/lib64/python3.6/config-3.6m-x86_64-linux-gnu \\ --enable-fontset=yes \\ --enable-cscope=yes \\ --enable-multibyte \\ --disable-gui \\ --enable-fail-if-missing \\ --prefix=/usr/local \\ --with-compiledby='Professional operations' # 编译 # make VIMRUNTIMEDIR=*** 表示指定VIM可执行文件的位置 make VIMRUNTIMEDIR=/usr/local/share/vim/vim82 \u0026amp;\u0026amp; make install  编译完成之后，查看一下vim的版本：\n$ vim --version VIM - Vi IMproved 8.2 (2019 Dec 12, compiled Mar 16 2021 16:12:42)  配置与插件 为了支持go代码的开发，这里先以go的IDE环境为示例，\n首先，插件的管理工具，这里选择 vim-plug。在Linux环境下直接用curl下载即可：\ncurl -fLo ~/.vim/autoload/plug.vim --create-dirs \\ https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim  插件的配置也非常简单，只要将所有的插件配置在 call plug#begin('~/.vim/plugged') 和 call plug#end() 之间即可，常见的插件基本上都可以从 github 中找到，如果 github 找不到的话基本上 vim.org 的脚本都可以在 vim-script 中找到备份\n来看一下 ~/.vimrc 的最终配置：\n\u0026quot; 显示行数 set number \u0026quot; 显示光标所在的当前行的行号，其他行都为相对于该行的相对行号。 \u0026quot; set relativenumber \u0026quot; 光标所在当前行高亮 \u0026quot; set cursorline \u0026quot; 自动折行，即太长的行分成几行显示。 \u0026quot; 关闭自动折行 \u0026quot; set nowrap set wrap \u0026quot; 只有遇到指定的符号（比如空格、连词号和其他标点符号），才发生折行。也就是说，不会在单词内部折行。 set linebreak \u0026quot; 指定折行处与编辑窗口的右边缘之间空出的字符数。 set wrapmargin=2 \u0026quot; 避免 backspace 不能使用 \u0026quot; 0 same as “:set backspace=” (Vi compatible) \u0026quot; 1 same as “:set backspace=indent,eol” \u0026quot; 2 same as “:set backspace=indent,eol,start” set backspace=2 \u0026quot; 自动将tab键转为空格 set expandtab \u0026quot; tab 转为多少个空格 set softtabstop=4 \u0026quot; 设置语法高亮 syntax enable syntax on \u0026quot; 按下回车键后，下一行的缩进会自动跟上一行的缩进保持一致。 set autoindent \u0026quot; 光标遇到圆括号、方括号、大括号时，自动高亮对应的另一个圆括号、方括号和大括号 set showmatch \u0026quot; 按下tab键时，vim显示的空格数 set tabstop=4 \u0026quot; 统一缩进为4 set shiftwidth=4 \u0026quot; 在底部显示，当前处于命令模式还是插入模式 set showmode \u0026quot; 支持使用鼠标 set mouse-=a \u0026quot; 使用 utf-8 编码 set encoding=utf-8 \u0026quot; 开启文件类型检查，并且载入与该类型对应的缩进规则。比如，如果编辑的是.py文件，Vim 就是会找 Python 的缩进规则~/.vim/indent/python.vim。 filetype indent on \u0026quot;-------------------------------------- \u0026quot; 搜索 \u0026quot;-------------------------------------- \u0026quot; 搜索时，高亮显示匹配结果 set hlsearch \u0026quot; 搜索时忽略大小写 set ignorecase \u0026quot;-------------------------------------- \u0026quot; 搜索(END) \u0026quot;-------------------------------------- \u0026quot;-------------------------------------- \u0026quot; 编辑 \u0026quot;-------------------------------------- \u0026quot; 打开英语单词的拼写检查 \u0026quot; set spell spelllang=en_us \u0026quot; 不创建交换文件。交换文件主要用于系统崩溃时恢复文件，文件名的开头是.、结尾是.swp。 set noswapfile \u0026quot; 启用自动补全 filetype plugin indent on \u0026quot; 退出插入模式指定类型的文件自动保存 au InsertLeave *.go,*.sh,*.php write \u0026quot;-------------------------------------- \u0026quot; 编辑(结束) \u0026quot;------------------------------------- set nocompatible \u0026quot; 去除VI一致性,必须 filetype off \u0026quot; 必须 \u0026quot;-------------------------------------- \u0026quot; vim-plug \u0026quot;-------------------------------------- \u0026quot;\u0026quot; 插件开始的位置 call plug#begin('~/.vim/plugged') \u0026quot; Shorthand notation; fetches https://github.com/junegunn/vim-easy-align \u0026quot; 可以快速对齐的插件 Plug 'junegunn/vim-easy-align' \u0026quot; 用来提供一个导航目录的侧边栏 Plug 'scrooloose/nerdtree' \u0026quot; 可以使 nerdtree 的 tab 更加友好些 Plug 'jistr/vim-nerdtree-tabs' \u0026quot; 可以在导航目录中看到 git 版本信息 Plug 'Xuyuanp/nerdtree-git-plugin' \u0026quot; 查看当前代码文件中的变量和函数列表的插件， \u0026quot; 可以切换和跳转到代码中对应的变量和函数的位置 \u0026quot; 大纲式导航, Go 需要 https://github.com/jstemmer/gotags 支持 Plug 'majutsushi/tagbar' \u0026quot; 自动补全括号的插件，包括小括号，中括号，以及花括号 Plug 'jiangmiao/auto-pairs' \u0026quot; Vim状态栏插件，包括显示行号，列号，文件类型，文件名，以及Git状态 Plug 'vim-airline/vim-airline' \u0026quot; 有道词典在线翻译 \u0026quot; Plug 'ianva/vim-youdao-translater' \u0026quot; 代码自动完成，安装完插件还需要额外配置才可以使用 Plug 'Valloric/YouCompleteMe' \u0026quot; 可以在文档中显示 git 信息 Plug 'airblade/vim-gitgutter' \u0026quot; 下面两个插件要配合使用，可以自动生成代码块 \u0026quot;Plug 'SirVer/ultisnips' \u0026quot;Plug 'honza/vim-snippets' \u0026quot; 可以在 vim 中使用 tab 补全 Plug 'vim-scripts/SuperTab' \u0026quot; 可以在 vim 中自动完成 \u0026quot;Plug 'Shougo/neocomplete.vim' \u0026quot; 配色方案 \u0026quot; colorscheme neodark Plug 'KeitaNakamura/neodark.vim' \u0026quot; colorscheme monokai Plug 'crusoexia/vim-monokai' \u0026quot; colorscheme github Plug 'acarapetis/vim-colors-github' \u0026quot; colorscheme one Plug 'rakr/vim-one' \u0026quot; go 主要插件 Plug 'fatih/vim-go', { 'tag': '*' } \u0026quot; go 中的代码追踪，输入 gd 就可以自动跳转 \u0026quot; Plug 'dgryski/vim-godef' \u0026quot; markdown 插件 Plug 'iamcco/mathjax-support-for-mkdp' Plug 'iamcco/markdown-preview.vim' \u0026quot; 插件结束的位置，插件全部放在此行上面 call plug#end() \u0026quot;------------------------------------- \u0026quot; VIM-PLUG(end) \u0026quot;------------------------------------- \u0026quot;============================================================================== \u0026quot; 主题配色 \u0026quot;============================================================================== \u0026quot; 使用256色 \u0026quot; set t_Co=256 \u0026quot; 开启24bit的颜色，开启这个颜色会更漂亮一些 set termguicolors \u0026quot; 配色方案, 可以从上面插件安装中的选择一个使用 colorscheme one \u0026quot; 主题 set background=dark \u0026quot; 主题背景 dark-深色; light-浅色 \u0026quot;============================================================================== \u0026quot; vim-go 插件 \u0026quot;============================================================================== let g:go_fmt_command = \u0026quot;goimports\u0026quot; \u0026quot; 格式化将默认的 gofmt 替换 let g:go_autodetect_gopath = 1 let g:go_list_type = \u0026quot;quickfix\u0026quot; let g:go_version_warning = 1 let g:go_highlight_types = 1 let g:go_highlight_fields = 1 let g:go_highlight_functions = 1 let g:go_highlight_function_calls = 1 let g:go_highlight_operators = 1 let g:go_highlight_extra_types = 1 let g:go_highlight_methods = 1 let g:go_highlight_generate_tags = 1 let g:godef_split=2 \u0026quot;============================================================================== \u0026quot; NERDTree 插件 \u0026quot;============================================================================== \u0026quot; 打开和关闭NERDTree快捷键 map \u0026lt;F3\u0026gt; :NERDTreeToggle\u0026lt;CR\u0026gt; \u0026quot; 显示行号 let NERDTreeShowLineNumbers=1 \u0026quot; 打开文件时是否显示目录 let NERDTreeAutoCenter=1 \u0026quot; 是否显示隐藏文件 let NERDTreeShowHidden=0 \u0026quot; 设置宽度 \u0026quot; let NERDTreeWinSize=31 \u0026quot; 忽略一下文件的显示 let NERDTreeIgnore=['\\.pyc','\\~$','\\.swp'] \u0026quot; 打开 vim 文件及显示书签列表 let NERDTreeShowBookmarks=2 \u0026quot; 在终端启动vim时，共享NERDTree let g:nerdtree_tabs_open_on_console_startup=1 \u0026quot;============================================================================== \u0026quot; majutsushi/tagbar 插件 \u0026quot;============================================================================== \u0026quot; majutsushi/tagbar 插件打开关闭快捷键 nmap \u0026lt;F9\u0026gt; :TagbarToggle\u0026lt;CR\u0026gt; let g:tagbar_type_go = { \\ 'ctagstype' : 'go', \\ 'kinds' : [ \\ 'p:package', \\ 'i:imports:1', \\ 'c:constants', \\ 'v:variables', \\ 't:types', \\ 'n:interfaces', \\ 'w:fields', \\ 'e:embedded', \\ 'm:methods', \\ 'r:constructor', \\ 'f:functions' \\ ], \\ 'sro' : '.', \\ 'kind2scope' : { \\ 't' : 'ctype', \\ 'n' : 'ntype' \\ }, \\ 'scope2kind' : { \\ 'ctype' : 't', \\ 'ntype' : 'n' \\ }, \\ 'ctagsbin' : 'gotags', \\ 'ctagsargs' : '-sort -silent' \\ } \u0026quot;============================================================================== \u0026quot; nerdtree-git-plugin 插件 \u0026quot;============================================================================== let g:NERDTreeGitStatusIndicatorMapCustom = { \\ \u0026quot;Modified\u0026quot; : \u0026quot;✹\u0026quot;, \\ \u0026quot;Staged\u0026quot; : \u0026quot;✚\u0026quot;, \\ \u0026quot;Untracked\u0026quot; : \u0026quot;✭\u0026quot;, \\ \u0026quot;Renamed\u0026quot; : \u0026quot;➜\u0026quot;, \\ \u0026quot;Unmerged\u0026quot; : \u0026quot;═\u0026quot;, \\ \u0026quot;Deleted\u0026quot; : \u0026quot;✖\u0026quot;, \\ \u0026quot;Dirty\u0026quot; : \u0026quot;✗\u0026quot;, \\ \u0026quot;Clean\u0026quot; : \u0026quot;✔︎\u0026quot;, \\ 'Ignored' : '☒', \\ \u0026quot;Unknown\u0026quot; : \u0026quot;?\u0026quot; \\ } let g:NERDTreeGitStatusShowIgnored = 1 \u0026quot;============================================================================== \u0026quot; Valloric/YouCompleteMe 插件 \u0026quot;============================================================================== \u0026quot; make YCM compatible with UltiSnips (using supertab) let g:ycm_key_list_select_completion = ['\u0026lt;C-n\u0026gt;', '\u0026lt;space\u0026gt;'] let g:ycm_key_list_previous_completion = ['\u0026lt;C-p\u0026gt;', '\u0026lt;Up\u0026gt;'] let g:SuperTabDefaultCompletionType = '\u0026lt;C-n\u0026gt;' \u0026quot; better key bindings for UltiSnipsExpandTrigger let g:UltiSnipsExpandTrigger = \u0026quot;\u0026lt;tab\u0026gt;\u0026quot; let g:UltiSnipsJumpForwardTrigger = \u0026quot;\u0026lt;tab\u0026gt;\u0026quot; let g:UltiSnipsJumpBackwardTrigger = \u0026quot;\u0026lt;s-tab\u0026gt;\u0026quot; \u0026quot;============================================================================== \u0026quot; 其他插件配置 \u0026quot;============================================================================== \u0026quot; markdwon 的快捷键 map \u0026lt;silent\u0026gt; \u0026lt;F5\u0026gt; \u0026lt;Plug\u0026gt;MarkdownPreview map \u0026lt;silent\u0026gt; \u0026lt;F6\u0026gt; \u0026lt;Plug\u0026gt;StopMarkdownPreview \u0026quot; tab 标签页切换快捷键 :nn \u0026lt;Leader\u0026gt;1 1gt :nn \u0026lt;Leader\u0026gt;2 2gt :nn \u0026lt;Leader\u0026gt;3 3gt :nn \u0026lt;Leader\u0026gt;4 4gt :nn \u0026lt;Leader\u0026gt;5 5gt :nn \u0026lt;Leader\u0026gt;6 6gt :nn \u0026lt;Leader\u0026gt;7 7gt :nn \u0026lt;Leader\u0026gt;8 8gt :nn \u0026lt;Leader\u0026gt;9 8gt :nn \u0026lt;Leader\u0026gt;0 :tablast\u0026lt;CR\u0026gt;  参考 将 VIM 打造成 go 语言的 ide\n","date":"2021年03月16日","permalink":"https://ahamoment.cn/posts/tool/tool-vim/","summary":"\u003cp\u003e折腾了两天的vim，想要把它变成我的机器上默认的IDE，方便在没有环境的时候，快速查阅代码，做一些基本的开发任务，折腾过程记录在这篇博客上\u003c/p\u003e","title":"Vim 折腾记"},{"contents":"GOPATH 是 Go语言中使用的一个环境变量，它使用绝对路径提供项目的工作目录。\n工作目录是一个工程开发的相对参考目录，好比当你要在公司编写一套服务器代码，你的工位所包含的桌面、计算机及椅子就是你的工作区。工作区的概念与工作目录的概念也是类似的。如果不使用工作目录的概念，在多人开发时，每个人有一套自己的目录结构，读取配置文件的位置不统一，输出的二进制运行文件也不统一，这样会导致开发的标准不统一，影响开发效率。\nGOPATH 适合处理大量 Go语言源码、多个包组合而成的复杂工程。\n在命令行中运行 go env 来查看当前 GOPATH 路径设置情况：\n# go env GO111MODULE=\u0026quot;\u0026quot; GOARCH=\u0026quot;amd64\u0026quot; GOBIN=\u0026quot;\u0026quot; GOCACHE=\u0026quot;/root/.cache/go-build\u0026quot; GOENV=\u0026quot;/root/.config/go/env\u0026quot; GOEXE=\u0026quot;\u0026quot; GOFLAGS=\u0026quot;\u0026quot; GOHOSTARCH=\u0026quot;amd64\u0026quot; GOHOSTOS=\u0026quot;linux\u0026quot; GOINSECURE=\u0026quot;\u0026quot; GOMODCACHE=\u0026quot;/root/go/pkg/mod\u0026quot; GONOPROXY=\u0026quot;\u0026quot; GONOSUMDB=\u0026quot;\u0026quot; GOOS=\u0026quot;linux\u0026quot; GOPATH=\u0026quot;/root/go\u0026quot; GOPRIVATE=\u0026quot;\u0026quot; GOPROXY=\u0026quot;https://proxy.golang.org,direct\u0026quot; GOROOT=\u0026quot;/usr/local/go\u0026quot; GOSUMDB=\u0026quot;sum.golang.org\u0026quot; GOTMPDIR=\u0026quot;\u0026quot; GOTOOLDIR=\u0026quot;/usr/local/go/pkg/tool/linux_amd64\u0026quot; GOVCS=\u0026quot;\u0026quot; GOVERSION=\u0026quot;go1.16\u0026quot; GCCGO=\u0026quot;gccgo\u0026quot; AR=\u0026quot;ar\u0026quot; CC=\u0026quot;gcc\u0026quot; CXX=\u0026quot;g++\u0026quot; CGO_ENABLED=\u0026quot;1\u0026quot; GOMOD=\u0026quot;/dev/null\u0026quot; CGO_CFLAGS=\u0026quot;-g -O2\u0026quot; CGO_CPPFLAGS=\u0026quot;\u0026quot; CGO_CXXFLAGS=\u0026quot;-g -O2\u0026quot; CGO_FFLAGS=\u0026quot;-g -O2\u0026quot; CGO_LDFLAGS=\u0026quot;-g -O2\u0026quot; PKG_CONFIG=\u0026quot;pkg-config\u0026quot; GOGCCFLAGS=\u0026quot;-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build439564896=/tmp/go-build -gno-record-gcc-switches\u0026quot;  命令行输出的说明如下：\n GOARCH 表示目标处理器的架构 GOBIN 表示编译器和链接器的安装位置 GOOS 表示目标操作系统 GOPATH 表示当前工作目录 GOROOT 表示GO开发包的安装目录  从命令行输出可以看出，当前GOPATH 的路径为： /root/go。\n在 GOPATH 指定的工作目录下，代码总是会保存在 $GOPATH/src 目录下。在工程经过 go build、go install 或 go get 等指令后，会将产生的二进制可执行文件放在 $GOPATH/bin 目录下，生成的中间缓存文件会被保存在 $GOPATH/pkg 下。\n如果需要将整个源码添加到版本管理工具（Version Control System，VCS）中时，只需要添加 $GOPATH/src 目录的源码即可。bin 和 pkg 目录的内容都可以由 src 目录生成。\n设置和使用 GOPATH 本节以 Linux 为演示平台，为大家演示使用 GOPATH 的方法。\n1. 设置当前目录为GOPATH 选择一个目录，在目录中的命令行中执行下面的指令：\nexport GOPATH=`pwd`  该指令中的 pwd 将输出当前的目录，使用反引号将 pwd 指令括起来表示命令行替换，也就是说，使用pwd将获得 pwd 返回的当前目录的值。例如，假设你的当前目录是“/home/davy/go”，那么使用pwd将获得返回值“/home/davy/go”。\n使用 export 指令可以将当前目录的值设置到环境变量 GOPATH中。\n2. 建立GOPATH中的源码目录 使用下面的指令创建 GOPATH 中的src目录，在 src 目录下还有一个 hello 目录，该目录用于保存源代码。\nmkdir -p src/hello  mkdir 指令的 -p 可以连续创建一个路径。\n3. 添加 main.go 源码文件 使用 Linux 编辑器将下面的源码保存为 main.go 并保存到 $GOPATH/src/hello 目录下。\npackage main import \u0026quot;fmt\u0026quot; func main(){ fmt.Println(\u0026quot;hello\u0026quot;) }  4. 编译源码运行 此时我们已经设置了 GOPATH， 因此在 GO 语言中可以通过 GOPATH 找到工程的位置。\n在命令行中执行如下指令编译源码：\ngo install hell  编译完成的可执行文件会保存在$GOPATH/bin 目录下。\n在 bin 目录中执行 ./hello，命令行输出如下：\nhello world\n在多项目工程中使用 GOPATH 在很多与 Go语言相关的书籍、文章中描述的 GOPATH 都是通过修改系统全局的环境变量来实现的。然而，根据笔者多年的 Go语言使用和实践经验及周边朋友、同事的反馈，这种设置全局 GOPATH 的方法可能会导致当前项目错误引用了其他目录的 Go 源码文件从而造成编译输出错误的版本或编译报出一些无法理解的错误提示。\n比如说，将某项目代码保存在 /home/davy/projectA 目录下，将该目录设置为 GOPATH。随着开发进行，需要再次获取一份工程项目的源码，此时源码保存在 /home/davy/projectB 目录下，如果此时需要编译 projectB 目录的项目，但开发者忘记设置 GOPATH 而直接使用命令行编译，则当前的 GOPATH 指向的是 /home/davy/projectA 目录，而不是开发者编译时期望的 projectB 目录。编译完成后，开发者就会将错误的工程版本发布到外网。\n因此，建议大家无论是使用命令行或者使用集成开发环境编译 Go 源码时，GOPATH 跟随项目设定。在 Jetbrains 公司的 GoLand 集成开发环境（IDE）中的 GOPATH 设置分为全局 GOPATH 和项目 GOPATH，如下图所示。\n图中的 Global GOPATH 代表全局 GOPATH，一般来源于系统环境变量中的 GOPATH；Project GOPATH 代表项目所使用的 GOPATH，该设置会被保存在工作目录的 .idea 目录下，不会被设置到环境变量的 GOPATH 中，但会在编译时使用到这个目录。建议在开发时只填写项目 GOPATH，每一个项目尽量只设置一个 GOPATH，不使用多个 GOPATH 和全局的 GOPATH。\n提示 Visual Studio 早期在设计时，允许 C++ 语言在全局拥有一个包含路径。当一个工程多个版本的编译，或者两个项目混杂有不同的共享全局包含时，会发生难以察觉的错误。在新版本 Visual Studio 中已经废除了这种全局包含的路径设计，并建议开发者将包含目录与项目关联。\nGo语言中的 GOPATH 也是一种类似全局包含的设计，因此鉴于 Visual Studio 在设计上的失误，建议开发者不要设置全局的 GOPATH，而是随项目设置 GOPATH\n","date":"2021年03月15日","permalink":"https://ahamoment.cn/posts/go/go-project-catalogs/","summary":"GOPATH 是 Go语言中使用的一个环境变量，它使用绝对路径提供项目的工作目录。 工作目录是一个工程开发的相对参考目录，好比当你要在公司编写一套服务器代码","title":"Go 语言工作目录"},{"contents":" 原文链接：Kubernetes Deep Dive: Code Generation for CustomResources\n CustomResourceDefinitions (CRDs)是kubernetes 1.7 中引入的，并在1.8中从alpha版本升级为beta版本了，最新版本的kubernetes 1.20 中，CRDs的版本已经是v1了。由于CRDs的易用性，在很多实现了控制器模式的例子中都可以看到它的身影。\n在Kubernetes 1.8中， CRDs 在基于golang的项目中的使用也变得更加自然：通过用户提供的 CustomResources，我们可以利用在Kubernetes提供的代码生成工具来生成代码。这篇文章展示了代码生成器的工作方式，以及如何以最少的代码行将其应用到自己的项目中，为您提供了生成的Deepcopy函数，内置的clients，listers和informers，所有这些都带有一个Shell脚本调用和几个代码注释。\n 注：deepcopy，意为”深拷贝“，深拷贝意味着会重新生成对象并拷贝对象中的所有字段、地址等数据；浅拷贝仅仅是对象的引用，并没有生成新的对象。\n 为什么要使用代码生成？ 那些在golang中原生使用ThirdPartyResources或CustomResourceDefinition的人可能会惊讶于突然在Kubernetes 1.8中需要生成client-go。更具体地说，client-go要求 runtime.Object 类型（golang中的CustomResources必须实现runtime.Object接口）必须具有DeepCopy方法。这里的代码生成通过deepcopy-gen生成器起作用，可以在k8s.io/code-generator存储库中找到。\n除了deepcopy-gen 生成器外，还有几个代码生成器是大多数CustomResources用户都想使用的：\n deepcopy-gen - 为每个T类型的方法创建func (t* T) DeepCopy() *T 函数 client-gen - 为 CustomResource APIGroups 创建内置的客户端集 informer-gen - 为CustomResources创建informer，该informer提供基于事件的界面以对服务器上CustomResources的更改做出反应 lister-gen - 为CustomResources创建listers，该listers为GET和LIST请求提供只读缓存层。  最后两个是构建控制器（或Operators）的基础。在后续博客中，我们将更详细地介绍控制器。这四个代码生成器使用与Kubernetes上游控制器所使用的相同的机制和软件包，构成了构建功能齐全，可用于生产环境的控制器的强大基础。\nk8s.io/code-generator 中还有其他用于其他上下文的生成器，例如，如果您构建自己的聚合API服务器，则除了版本化类型外，还将使用内部类型。Conversion-gen 将在这些内部和外部类型之间创建转换函数。Defaulter-gen将负责默认某些字段。\n在你的项目中调用代码生成器 所有的Kubernetes代码生成器都是在k8s.io/gengo之上实现的。它们共享许多公共命令行标志。基本上，所有生成器都获得输入包（--input-dirs）的列表，它们通过类型进行检查，并输出生成的代码。生成的代码：\n 要么进入与输入文件相同的目录，例如deepcopy-gen （ --output-file-base \u0026quot;zz_generated.deepcopy\u0026quot;定义文件名） 或者它们生成一个或多个输出包(--output-package)例如 client-, informer- 和 lister-gen 所做的（通常将生成的代码放在 pkg/client 目录下）  k8s.io/code-generator附带了一个shell脚本generator-group.sh，这个脚本会帮我们处理这些繁重的工作，通常只要调用hack/update-codegen.sh就行，例如：\n$ vendor/k8s.io/code-generator/generate-groups.sh all \\ github.com/openshift-evangelist/crd-code-generation/pkg/client \\ github.com/openshift-evangelist/crd-code-generation/pkg/apis \\ example.com:v1  运行这个命令后生成的目录如下所示：\n所有的 APIs 都被创建到 pkg/apis 目录下，clientsets，informers 以及 listers 被创建到 pkg/client 这个目录下。总而言之，pkg/client 文件夹完全都是生成的，并且包括zz_generated.deepcopy.go这个文件。两者都不应该手动修改，而是通过运行以下命令创建：\n$ hack/update-codegen.sh  这个脚本的附近还有一个 hack/verify-codegen.sh 脚本，如果生成的任何文件不是最新的，该脚本都将以非零的返回码终止。这对于放入CI脚本中非常有帮助：如果开发人员无意中修改了文件，或者如果文件刚刚过时，CI会注意到并报错。\n控制生成的代码–标签 如上所述，虽然代码生成器的某些行为是通过命令行标志（尤其是要处理的程序包）来控制的，但更多的属性是通过golang文件中的标签来控制的。\n标签有两种：\n Global tags 相关的在package目录下的doc.go文件中 Local tags 在它需要处理的类型中定义  标签通常是 // +tag-name 或者 // +tag-name=value 这两种形式，写在注释中。根据标签，注释的位置变得很重要。大部分的注释的标签应该直接标注在类型上（对于全局标签来说应该在package行中），其他的必须与类型分开，中间至少要有一条空线。\nGLOBAL TAGS Global tags 被写入到包的 doc.go 文件中，一个典型的pkg/apis/\u0026lt;apigroup\u0026gt;/\u0026lt;version\u0026gt;/doc.go文件如下所示：\n// +k8s:deepcopy-gen=package,register // Package v1 is the v1 version of the API. // +groupName=example.com package v1  它告诉deepcopy-gen默认为该包中的每种类型创建Deepcopy方法。如果你的类型中不需要生成deepcopy方法，可以使用local tag来关闭// +k8s:deepcopy-gen=false。如果你不在包的声明中使用生成 deepcopy 方法，就需要通过在每个类型的注释中定义 // +k8s:deepcopy-gen=false。\n 注意：上例中的值中的 register 关键字将使deepcopy方法注册到该scheme中。在Kubernetes 1.9中被取消了，因为该scheme将不再负责执行runtime.Objects的深层复制。取而代之的是调用 yourobject.DeepCopy() 或者 yourobject.DeepCopyObject()。\nScheme定义了序列化和反序列化API对象的方法，用于将group、版本和类型信息转换为Go模式和从Go模式转换为Go模式的类型注册表，以及不同版本的Go模式之间的映射。\n 最后，// +groupName=example.com 定义了API 组的全限定名称。如果你写错了这个名称，client-gen 会生成错误的代码。另外这个标签必须在包上的注释块中。\nLACAL TAGS 本地标记要么直接写在API类型上方，要么写在它上方的第二个注释块中。如下的 types.go 文件所示：\n// +genclient // +genclient:noStatus // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // Database describes a database. type Database struct { metav1.TypeMeta `json:\u0026quot;,inline\u0026quot;` metav1.ObjectMeta `json:\u0026quot;metadata,omitempty\u0026quot;` Spec DatabaseSpec `json:\u0026quot;spec\u0026quot;` } // DatabaseSpec is the spec for a Foo resource type DatabaseSpec struct { User string `json:\u0026quot;user\u0026quot;` Password string `json:\u0026quot;password\u0026quot;` Encoding string `json:\u0026quot;encoding,omitempty\u0026quot;` } // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // DatabaseList is a list of Database resources type DatabaseList struct { metav1.TypeMeta `json:\u0026quot;,inline\u0026quot;` metav1.ListMeta `json:\u0026quot;metadata\u0026quot;` Items []Database `json:\u0026quot;items\u0026quot;` }  请注意，默认情况下，我们为所有类型启用了deepcopy，也就是说，可以选择不使用 deepcopy。但是，这些类型都是API类型，需要深度复制。因此，在此示例types.go中，我们不必打开或关闭deepcopy，而仅在doc.go中的程序包范围内即可。\nruntime.Object and DeepCopyObject 有一个特殊的 deepcopy 标签，需要更多说明：\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object  如果您尝试将CustomResources与基于Kubernetes 1.8的客户端一起使用，有些人可能已经很高兴，因为他们不小心出售了master分支的k8s.op/apimachinery，您遇到了由于CustomResource类型未实现runtime.Object而导致的编译器错误，因为未在您的类型上定义DeepCopyObject（）runtime.Object。原因是在1.8中，runtime.Object接口使用此方法签名进行了扩展，因此每个runtime.Object都必须实现DeepCopyObject。 DeepCopyObject（）runtime.Object的实现很简单：\nfunc (in *T) DeepCopyObject() runtime.Object { if c := in.DeepCopy(); c != nil { return c } else { return nil } }  但幸运的是，您不必为每种类型都实现此功能，而只需将以下本地标记放在顶级API类型的上方：\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object  在上面的示例中，Database 和DatabaseList都是顶级类型，因为它们被用作runtime.Objects。根据经验，顶级类型是那些嵌入了metav1.TypeMeta的类型。同样，这些是客户端使用client-gen创建的类型。\n请注意，// + k8s：deepcopy-gen：interfaces 标记可以并且也应该在定义具有某些接口类型的字段（例如，field SomeInterface）的API类型的情况下使用。然后// + k8s：deepcopy-gen：interfaces=example.com/pkg/apis/example.SomeInterface将导致DeepeepSomeInterface（）SomeInterface方法的生成。这允许它以类型正确的方式对这些字段进行深度复制。\nClient-gen 标签 最后，有许多标记可控制client-gen，在我们的示例中可以看到其中两个：\n// +genclient // +genclient:noStatus  第一个标记告诉client-gen为该类型创建一个客户端（始终启用）。请注意，您不必将其放在API对象的列表类型上方。\n第二个标记告诉client-gen该类型未通过/status子资源使用规范状态分隔。生成的客户端将没有UpdateStatus方法（client-gen一旦在您的结构中找到Status字段，就会盲目生成该方法）。 /status子资源仅在1.8中才适用于本地（在golang中）实现的资源。但是，随着PR 913中为CustomResources讨论子资源，这种情况可能很快就会改变。\n对于群集范围的资源，必须使用标签：\n// +genclient:nonNamespaced  对于特殊用途的客户端，您可能还希望详细控制客户端提供哪些HTTP方法。可以使用几个标签来完成此操作，例如：\n// +genclient:noVerbs // +genclient:onlyVerbs=create,delete // +genclient:skipVerbs=get,list,create,update,patch,delete,deleteCollection,watch // +genclient:method=Create,verb=create,result=k8s.io/apimachinery/pkg/apis/meta/v1.Status  前三个应该是不言自明的，但是最后一个需要一些解释。上面写入此标记的类型将是仅创建的，并且不会返回API类型本身，而是metav1.Status。对于CustomResources来说，这没有多大意义，但是对于用golang编写的用户提供的API服务器，这些资源可以存在，并且实际上可以在OpenShift API中使用。\n使用类型客户端的主要功能 尽管大多数基于Kubernetes 1.7和更早版本的示例都使用了Client-go dynamic client 作为CustomResources，但在很长一段时间内，本地Kubernetes API类型的类型化客户端都更加方便。在1.8版中进行了更改：如上所述，client-gen还为您的自定义类型创建了native，功能齐全且易于使用的类型化客户端。实际上，client-gen不知道您是将其应用于CustomResource类型还是native类型。 因此，使用此客户端与使用客户端Gober客户端完全等效。这是一个非常简单的示例：\nimport ( ... metav1 \u0026quot;k8s.io/apimachinery/pkg/apis/meta/v1\u0026quot; \u0026quot;k8s.io/client-go/tools/clientcmd\u0026quot; examplecomclientset \u0026quot;github.com/openshift-evangelist/crd-code-generation/pkg/client/clientset/versioned\u0026quot; ) var ( kuberconfig = flag.String(\u0026quot;kubeconfig\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;Path to a kubeconfig. Only required if out-of-cluster.\u0026quot;) master = flag.String(\u0026quot;master\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.\u0026quot;) ) func main() { flag.Parse() cfg, err := clientcmd.BuildConfigFromFlags(*master, *kuberconfig) if err != nil { glog.Fatalf(\u0026quot;Error building kubeconfig: %v\u0026quot;, err) } exampleClient, err := examplecomclientset.NewForConfig(cfg) if err != nil { glog.Fatalf(\u0026quot;Error building example clientset: %v\u0026quot;, err) } list, err := exampleClient.ExampleV1().Databases(\u0026quot;default\u0026quot;).List(metav1.ListOpti ons{}) if err != nil { glog.Fatalf(\u0026quot;Error listing all databases: %v\u0026quot;, err) } for _, db := range list.Items { fmt.Printf(\u0026quot;database %s with user %q\\n\u0026quot;, db.Name, db.Spec.User) } }  它与kubeconfig文件一起使用，实际上可以与kubectl和Kubernetes客户端一起使用。\n与动态客户端使用的旧版TPR或CustomResource代码相比，您无需进行类型转换。相反，实际的客户端调用看起来完全是本地的，它是：\nlist, err := exampleClient.ExampleV1().Databases(\u0026quot;default\u0026quot;).List(metav1.ListOptions{})  在此示例中，结果是群集中所有数据库的DatabaseList。如果您将类型切换为集群范围（即没有命名空间；请不要忘记使用// + genclientnonNamespaced标记告诉client-gen！），调用将变成\nlist, err := exampleClient.ExampleV1().Databases().List(metav1.ListOptions{})  以编程方式在GOLANG创建自定义资源 由于这个问题经常出现，因此请您谈谈如何从您的golang代码中以编程方式创建CRD的几句话。\n客户代总是创建所谓的clinetsets。客户端集将一个或多个API组捆绑到一个客户端中。通常，这些API组来自一个存储库，并位于一个基本程序包中，例如，如本博文示例中的pkg/apis；对于Kubernetes，则来自k8s.io/api。\nCustomResourceDefinitions由 kubernetes/apiextensions-apiserver存储库。该API服务器（也可以独立启动）是由kube-apiserver嵌入的，因此CRD在每个Kubernetes群集上都可用。但是创建CRD的客户端会创建到apiextensions-apiserver存储库中，当然也要使用client-gen。阅读此博客后，您可以在kubernetes/apiextensions-apiserver/tree/master/pkg/client上找到客户端也不会感到惊讶，创建客户端实例以及如何创建CRD看起来也不奇怪：\nimport ( ... apiextensionsclientset \u0026quot;k8s.io/apiextensions-apiserver/pkg/client/clientset/clientset” ) apiextensionsClient, err := apiextensionsclientset.NewForConfig(cfg) ... createdCRD, err := apiextensionsClient.ApiextensionsV1beta1().CustomResourceDefinitions().Create(yourCRD)  请注意，创建完成后，您将必须等待在新CRD上设置“已建立”条件。只有这样，kube-apiserver才会开始提供资源。如果您不等待该条件，则每次CR操作都会返回404 HTTP状态代码。\n","date":"2021年03月12日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter3/code-generation-for-customresources/","summary":"\u003cblockquote\u003e\n\u003cp\u003e原文链接：\u003ca href=\"https://www.openshift.com/blog/kubernetes-deep-dive-code-generation-customresources\"\u003eKubernetes Deep Dive: Code Generation for CustomResources\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e","title":"[译]Kubernetes深入研究：CustomResources的代码生成"},{"contents":" 原文链接：Kubernetes Deep Dive: Code Generation for CustomResources\n CustomResourceDefinitions (CRDs)是kubernetes 1.7 中引入的，并在1.8中从alpha版本升级为beta版本了，最新版本的kubernetes 1.20 中，CRDs的版本已经是v1了。由于CRDs的易用性，在很多实现了控制器模式的例子中都可以看到它的身影。\n在Kubernetes 1.8中， CRDs 在基于golang的项目中的使用也变得更加自然：通过用户提供的 CustomResources，我们可以利用在Kubernetes提供的代码生成工具来生成代码。这篇文章展示了代码生成器的工作方式，以及如何以最少的代码行将其应用到自己的项目中，为您提供了生成的Deepcopy函数，内置的clients，listers和informers，所有这些都带有一个Shell脚本调用和几个代码注释。\n 注：deepcopy，意为”深拷贝“，深拷贝意味着会重新生成对象并拷贝对象中的所有字段、地址等数据；浅拷贝仅仅是对象的引用，并没有生成新的对象。\n 为什么要使用代码生成？ 那些在golang中原生使用ThirdPartyResources或CustomResourceDefinition的人可能会惊讶于突然在Kubernetes 1.8中需要生成client-go。更具体地说，client-go要求 runtime.Object 类型（golang中的CustomResources必须实现runtime.Object接口）必须具有DeepCopy方法。这里的代码生成通过deepcopy-gen生成器起作用，可以在k8s.io/code-generator存储库中找到。\n除了deepcopy-gen 生成器外，还有几个代码生成器是大多数CustomResources用户都想使用的：\n deepcopy-gen - 为每个T类型的方法创建func (t* T) DeepCopy() *T 函数 client-gen - 为 CustomResource APIGroups 创建内置的客户端集 informer-gen - 为CustomResources创建informer，该informer提供基于事件的界面以对服务器上CustomResources的更改做出反应 lister-gen - 为CustomResources创建listers，该listers为GET和LIST请求提供只读缓存层。  最后两个是构建控制器（或Operators）的基础。在后续博客中，我们将更详细地介绍控制器。这四个代码生成器使用与Kubernetes上游控制器所使用的相同的机制和软件包，构成了构建功能齐全，可用于生产环境的控制器的强大基础。\nk8s.io/code-generator 中还有其他用于其他上下文的生成器，例如，如果您构建自己的聚合API服务器，则除了版本化类型外，还将使用内部类型。Conversion-gen 将在这些内部和外部类型之间创建转换函数。Defaulter-gen将负责默认某些字段。\n在你的项目中调用代码生成器 所有的Kubernetes代码生成器都是在k8s.io/gengo之上实现的。它们共享许多公共命令行标志。基本上，所有生成器都获得输入包（--input-dirs）的列表，它们通过类型进行检查，并输出生成的代码。生成的代码：\n 要么进入与输入文件相同的目录，例如deepcopy-gen （ --output-file-base \u0026quot;zz_generated.deepcopy\u0026quot;定义文件名） 或者它们生成一个或多个输出包(--output-package)例如 client-, informer- 和 lister-gen 所做的（通常将生成的代码放在 pkg/client 目录下）  k8s.io/code-generator附带了一个shell脚本generator-group.sh，这个脚本会帮我们处理这些繁重的工作，通常只要调用hack/update-codegen.sh就行，例如：\n$ vendor/k8s.io/code-generator/generate-groups.sh all \\ github.com/openshift-evangelist/crd-code-generation/pkg/client \\ github.com/openshift-evangelist/crd-code-generation/pkg/apis \\ example.com:v1  运行这个命令后生成的目录如下所示：\n所有的 APIs 都被创建到 pkg/apis 目录下，clientsets，informers 以及 listers 被创建到 pkg/client 这个目录下。总而言之，pkg/client 文件夹完全都是生成的，并且包括zz_generated.deepcopy.go这个文件。两者都不应该手动修改，而是通过运行以下命令创建：\n$ hack/update-codegen.sh  这个脚本的附近还有一个 hack/verify-codegen.sh 脚本，如果生成的任何文件不是最新的，该脚本都将以非零的返回码终止。这对于放入CI脚本中非常有帮助：如果开发人员无意中修改了文件，或者如果文件刚刚过时，CI会注意到并报错。\n控制生成的代码–标签 如上所述，虽然代码生成器的某些行为是通过命令行标志（尤其是要处理的程序包）来控制的，但更多的属性是通过golang文件中的标签来控制的。\n标签有两种：\n Global tags 相关的在package目录下的doc.go文件中 Local tags 在它需要处理的类型中定义  标签通常是 // +tag-name 或者 // +tag-name=value 这两种形式，写在注释中。根据标签，注释的位置变得很重要。大部分的注释的标签应该直接标注在类型上（对于全局标签来说应该在package行中），其他的必须与类型分开，中间至少要有一条空线。\nGLOBAL TAGS Global tags 被写入到包的 doc.go 文件中，一个典型的pkg/apis/\u0026lt;apigroup\u0026gt;/\u0026lt;version\u0026gt;/doc.go文件如下所示：\n// +k8s:deepcopy-gen=package,register // Package v1 is the v1 version of the API. // +groupName=example.com package v1  它告诉deepcopy-gen默认为该包中的每种类型创建Deepcopy方法。如果你的类型中不需要生成deepcopy方法，可以使用local tag来关闭// +k8s:deepcopy-gen=false。如果你不在包的声明中使用生成 deepcopy 方法，就需要通过在每个类型的注释中定义 // +k8s:deepcopy-gen=false。\n 注意：上例中的值中的 register 关键字将使deepcopy方法注册到该scheme中。在Kubernetes 1.9中被取消了，因为该scheme将不再负责执行runtime.Objects的深层复制。取而代之的是调用 yourobject.DeepCopy() 或者 yourobject.DeepCopyObject()。\nScheme定义了序列化和反序列化API对象的方法，用于将group、版本和类型信息转换为Go模式和从Go模式转换为Go模式的类型注册表，以及不同版本的Go模式之间的映射。\n 最后，// +groupName=example.com 定义了API 组的全限定名称。如果你写错了这个名称，client-gen 会生成错误的代码。另外这个标签必须在包上的注释块中。\nLACAL TAGS 本地标记要么直接写在API类型上方，要么写在它上方的第二个注释块中。如下的 types.go 文件所示：\n// +genclient // +genclient:noStatus // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // Database describes a database. type Database struct { metav1.TypeMeta `json:\u0026quot;,inline\u0026quot;` metav1.ObjectMeta `json:\u0026quot;metadata,omitempty\u0026quot;` Spec DatabaseSpec `json:\u0026quot;spec\u0026quot;` } // DatabaseSpec is the spec for a Foo resource type DatabaseSpec struct { User string `json:\u0026quot;user\u0026quot;` Password string `json:\u0026quot;password\u0026quot;` Encoding string `json:\u0026quot;encoding,omitempty\u0026quot;` } // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // DatabaseList is a list of Database resources type DatabaseList struct { metav1.TypeMeta `json:\u0026quot;,inline\u0026quot;` metav1.ListMeta `json:\u0026quot;metadata\u0026quot;` Items []Database `json:\u0026quot;items\u0026quot;` }  请注意，默认情况下，我们为所有类型启用了deepcopy，也就是说，可以选择不使用 deepcopy。但是，这些类型都是API类型，需要深度复制。因此，在此示例types.go中，我们不必打开或关闭deepcopy，而仅在doc.go中的程序包范围内即可。\nruntime.Object and DeepCopyObject 有一个特殊的 deepcopy 标签，需要更多说明：\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object  如果您尝试将CustomResources与基于Kubernetes 1.8的客户端一起使用，有些人可能已经很高兴，因为他们不小心出售了master分支的k8s.op/apimachinery，您遇到了由于CustomResource类型未实现runtime.Object而导致的编译器错误，因为未在您的类型上定义DeepCopyObject（）runtime.Object。原因是在1.8中，runtime.Object接口使用此方法签名进行了扩展，因此每个runtime.Object都必须实现DeepCopyObject。 DeepCopyObject（）runtime.Object的实现很简单：\nfunc (in *T) DeepCopyObject() runtime.Object { if c := in.DeepCopy(); c != nil { return c } else { return nil } }  但幸运的是，您不必为每种类型都实现此功能，而只需将以下本地标记放在顶级API类型的上方：\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object  在上面的示例中，Database 和DatabaseList都是顶级类型，因为它们被用作runtime.Objects。根据经验，顶级类型是那些嵌入了metav1.TypeMeta的类型。同样，这些是客户端使用client-gen创建的类型。\n请注意，// + k8s：deepcopy-gen：interfaces 标记可以并且也应该在定义具有某些接口类型的字段（例如，field SomeInterface）的API类型的情况下使用。然后// + k8s：deepcopy-gen：interfaces=example.com/pkg/apis/example.SomeInterface将导致DeepeepSomeInterface（）SomeInterface方法的生成。这允许它以类型正确的方式对这些字段进行深度复制。\nClient-gen 标签 最后，有许多标记可控制client-gen，在我们的示例中可以看到其中两个：\n// +genclient // +genclient:noStatus  第一个标记告诉client-gen为该类型创建一个客户端（始终启用）。请注意，您不必将其放在API对象的列表类型上方。\n第二个标记告诉client-gen该类型未通过/status子资源使用规范状态分隔。生成的客户端将没有UpdateStatus方法（client-gen一旦在您的结构中找到Status字段，就会盲目生成该方法）。 /status子资源仅在1.8中才适用于本地（在golang中）实现的资源。但是，随着PR 913中为CustomResources讨论子资源，这种情况可能很快就会改变。\n对于群集范围的资源，必须使用标签：\n// +genclient:nonNamespaced  对于特殊用途的客户端，您可能还希望详细控制客户端提供哪些HTTP方法。可以使用几个标签来完成此操作，例如：\n// +genclient:noVerbs // +genclient:onlyVerbs=create,delete // +genclient:skipVerbs=get,list,create,update,patch,delete,deleteCollection,watch // +genclient:method=Create,verb=create,result=k8s.io/apimachinery/pkg/apis/meta/v1.Status  前三个应该是不言自明的，但是最后一个需要一些解释。上面写入此标记的类型将是仅创建的，并且不会返回API类型本身，而是metav1.Status。对于CustomResources来说，这没有多大意义，但是对于用golang编写的用户提供的API服务器，这些资源可以存在，并且实际上可以在OpenShift API中使用。\n使用类型客户端的主要功能 尽管大多数基于Kubernetes 1.7和更早版本的示例都使用了Client-go dynamic client 作为CustomResources，但在很长一段时间内，本地Kubernetes API类型的类型化客户端都更加方便。在1.8版中进行了更改：如上所述，client-gen还为您的自定义类型创建了native，功能齐全且易于使用的类型化客户端。实际上，client-gen不知道您是将其应用于CustomResource类型还是native类型。 因此，使用此客户端与使用客户端Gober客户端完全等效。这是一个非常简单的示例：\nimport ( ... metav1 \u0026quot;k8s.io/apimachinery/pkg/apis/meta/v1\u0026quot; \u0026quot;k8s.io/client-go/tools/clientcmd\u0026quot; examplecomclientset \u0026quot;github.com/openshift-evangelist/crd-code-generation/pkg/client/clientset/versioned\u0026quot; ) var ( kuberconfig = flag.String(\u0026quot;kubeconfig\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;Path to a kubeconfig. Only required if out-of-cluster.\u0026quot;) master = flag.String(\u0026quot;master\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.\u0026quot;) ) func main() { flag.Parse() cfg, err := clientcmd.BuildConfigFromFlags(*master, *kuberconfig) if err != nil { glog.Fatalf(\u0026quot;Error building kubeconfig: %v\u0026quot;, err) } exampleClient, err := examplecomclientset.NewForConfig(cfg) if err != nil { glog.Fatalf(\u0026quot;Error building example clientset: %v\u0026quot;, err) } list, err := exampleClient.ExampleV1().Databases(\u0026quot;default\u0026quot;).List(metav1.ListOpti ons{}) if err != nil { glog.Fatalf(\u0026quot;Error listing all databases: %v\u0026quot;, err) } for _, db := range list.Items { fmt.Printf(\u0026quot;database %s with user %q\\n\u0026quot;, db.Name, db.Spec.User) } }  它与kubeconfig文件一起使用，实际上可以与kubectl和Kubernetes客户端一起使用。\n与动态客户端使用的旧版TPR或CustomResource代码相比，您无需进行类型转换。相反，实际的客户端调用看起来完全是本地的，它是：\nlist, err := exampleClient.ExampleV1().Databases(\u0026quot;default\u0026quot;).List(metav1.ListOptions{})  在此示例中，结果是群集中所有数据库的DatabaseList。如果您将类型切换为集群范围（即没有命名空间；请不要忘记使用// + genclientnonNamespaced标记告诉client-gen！），调用将变成\nlist, err := exampleClient.ExampleV1().Databases().List(metav1.ListOptions{})  以编程方式在GOLANG创建自定义资源 由于这个问题经常出现，因此请您谈谈如何从您的golang代码中以编程方式创建CRD的几句话。\n客户代总是创建所谓的clinetsets。客户端集将一个或多个API组捆绑到一个客户端中。通常，这些API组来自一个存储库，并位于一个基本程序包中，例如，如本博文示例中的pkg/apis；对于Kubernetes，则来自k8s.io/api。\nCustomResourceDefinitions由 kubernetes/apiextensions-apiserver存储库。该API服务器（也可以独立启动）是由kube-apiserver嵌入的，因此CRD在每个Kubernetes群集上都可用。但是创建CRD的客户端会创建到apiextensions-apiserver存储库中，当然也要使用client-gen。阅读此博客后，您可以在kubernetes/apiextensions-apiserver/tree/master/pkg/client上找到客户端也不会感到惊讶，创建客户端实例以及如何创建CRD看起来也不奇怪：\nimport ( ... apiextensionsclientset \u0026quot;k8s.io/apiextensions-apiserver/pkg/client/clientset/clientset” ) apiextensionsClient, err := apiextensionsclientset.NewForConfig(cfg) ... createdCRD, err := apiextensionsClient.ApiextensionsV1beta1().CustomResourceDefinitions().Create(yourCRD)  请注意，创建完成后，您将必须等待在新CRD上设置“已建立”条件。只有这样，kube-apiserver才会开始提供资源。如果您不等待该条件，则每次CR操作都会返回404 HTTP状态代码。\n","date":"2021年03月12日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter9/code-generation-for-customresources/","summary":"\u003cblockquote\u003e\n\u003cp\u003e原文链接：\u003ca href=\"https://www.openshift.com/blog/kubernetes-deep-dive-code-generation-customresources\"\u003eKubernetes Deep Dive: Code Generation for CustomResources\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e","title":"[译]Kubernetes深入研究：CustomResources的代码生成"},{"contents":" 本文转自张磊老师的《深入剖析 Kubernetes》课程，本节主要是关于自定义API资源(CRD)。\n 在上一篇文章中，我为你详细讲解了 Kubernetes 声明式 API 的设计、特点，以及使用方式。而在今天这篇文章中，我就来为你讲解一下 Kubernetes 声明式 API 的工作原理，以及如何利用这套 API 机制，在 Kubernetes 里添加自定义的 API 对象。\n你可能一直就很好奇：当我把一个 YAML 文件提交给 Kubernetes 之后，它究竟是如何创建出一个 API 对象的呢？这得从声明式 API 的设计谈起了。\n在 Kubernetes 项目中，一个 API 对象在 Etcd 里的完整资源路径，是由：Group（API 组）、Version（API 版本）和 Resource（API 资源类型）三个部分组成的。\n通过这样的结构，整个 Kubernetes 里的所有 API 对象，实际上就可以用如下的树形结构表示出来：\n在这幅图中，你可以很清楚地看到 Kubernetes 里 API 对象的组织方式，其实是层层递进的。比如，现在我要声明要创建一个 CronJob 对象，那么我的 YAML 文件的开始部分会这么写：\napiVersion: batch/v2alpha1 kind: CronJob ...  在这个 YAML 文件中，“CronJob”就是这个 API 对象的资源类型（Resource），“batch”就是它的组（Group），v2alpha1 就是它的版本（Version）。当我们提交了这个 YAML 文件之后，Kubernetes 就会把这个 YAML 文件里描述的内容，转换成 Kubernetes 里的一个 CronJob 对象。\n那么，Kubernetes 是如何对 Resource、Group 和 Version 进行解析，从而在 Kubernetes 项目里找到 CronJob 对象的定义呢？\n首先，Kubernetes 会匹配 API 对象的组。\n需要明确的是，对于 Kubernetes 里的核心 API 对象，比如：Pod、Node 等，是不需要 Group 的（即：它们的 Group 是“”）。所以，对于这些 API 对象来说，Kubernetes 会直接在 /api 这个层级进行下一步的匹配过程。而对于 CronJob 等非核心 API 对象来说，Kubernetes 就必须在 /apis 这个层级里查找它对应的 Group，进而根据“batch”这个 Group 的名字，找到 /apis/batch。不难发现，这些 API Group 的分类是以对象功能为依据的，比如 Job 和 CronJob 就都属于“batch” （离线业务）这个 Group。\n然后，Kubernetes 会进一步匹配到 API 对象的版本号。\n对于 CronJob 这个 API 对象来说，Kubernetes 在 batch 这个 Group 下，匹配到的版本号就是 v2alpha1。在 Kubernetes 中，同一种 API 对象可以有多个版本，这正是 Kubernetes 进行 API 版本化管理的重要手段。这样，比如在 CronJob 的开发过程中，对于会影响到用户的变更就可以通过升级新版本来处理，从而保证了向后兼容。\n最后，Kubernetes 会匹配 API 对象的资源类型。\n在前面匹配到正确的版本之后，Kubernetes 就知道，我要创建的原来是一个 /apis/batch/v2alpha1 下的 CronJob 对象。这时候，APIServer 就可以继续创建这个 CronJob 对象了。为了方便理解，我为你总结了一个如下所示流程图来阐述这个创建过程：\n首先，当我们发起了创建 CronJob 的 POST 请求之后，我们编写的 YAML 的信息就被提交给了 APIServer。而 APIServer 的第一个功能，就是过滤这个请求，并完成一些前置性的工作，比如授权、超时处理、审计等。\n然后，请求会进入 MUX 和 Routes 流程。如果你编写过 Web Server 的话就会知道，MUX 和 Routes 是 APIServer 完成 URL 和 Handler 绑定的场所。而 APIServer 的 Handler 要做的事情，就是按照我刚刚介绍的匹配过程，找到对应的 CronJob 类型定义。\n接着，APIServer 最重要的职责就来了：根据这个 CronJob 类型定义，使用用户提交的 YAML 文件里的字段，创建一个 CronJob 对象。而在这个过程中，APIServer 会进行一个 Convert 工作，即：把用户提交的 YAML 文件，转换成一个叫作 Super Version 的对象，它正是该 API 资源类型所有版本的字段全集。这样用户提交的不同版本的 YAML 文件，就都可以用这个 Super Version 对象来进行处理了。\n接下来，APIServer 会先后进行 Admission() 和 Validation() 操作。比如，我在上一篇文章中提到的 Admission Controller 和 Initializer，就都属于 Admission 的内容。而 Validation，则负责验证这个对象里的各个字段是否合法。这个被验证过的 API 对象，都保存在了 APIServer 里一个叫作 Registry 的数据结构中。也就是说，只要一个 API 对象的定义能在 Registry 里查到，它就是一个有效的 Kubernetes API 对象。\n最后，APIServer 会把验证过的 API 对象转换成用户最初提交的版本，进行序列化操作，并调用 Etcd 的 API 把它保存起来。\n由此可见，声明式 API 对于 Kubernetes 来说非常重要。所以，APIServer 这样一个在其他项目里“平淡无奇”的组件，却成了 Kubernetes 项目的重中之重。它不仅是 Google Borg 设计思想的集中体现，也是 Kubernetes 项目里唯一一个被 Google 公司和 RedHat 公司双重控制、其他势力根本无法参与其中的组件。\n此外，由于同时要兼顾性能、API 完备性、版本化、向后兼容等很多工程化指标，所以 Kubernetes 团队在 APIServer 项目里大量使用了 Go 语言的代码生成功能，来自动化诸如 Convert、DeepCopy 等与 API 资源相关的操作。这部分自动生成的代码，曾一度占到 Kubernetes 项目总代码的 20%~30%。这也是为何，在过去很长一段时间里，在这样一个极其“复杂”的 APIServer 中，添加一个 Kubernetes 风格的 API 资源类型，是一个非常困难的工作。不过，在 Kubernetes v1.7 之后，这个工作就变得轻松得多了。这，当然得益于一个全新的 API 插件机制：CRD。\nCRD 的全称是 Custom Resource Definition。顾名思义，它指的就是，允许用户在 Kubernetes 中添加一个跟 Pod、Node 类似的、新的 API 资源类型，即：自定义 API 资源。\n举个例子，我现在要为 Kubernetes 添加一个名叫 Network 的 API 资源类型。它的作用是，一旦用户创建一个 Network 对象，那么 Kubernetes 就应该使用这个对象定义的网络参数，调用真实的网络插件，比如 Neutron 项目，为用户创建一个真正的“网络”。这样，将来用户创建的 Pod，就可以声明使用这个“网络”了。\n这个 Network 对象的 YAML 文件，名叫 example-network.yaml，它的内容如下所示：\napiVersion: samplecrd.k8s.io/v1 kind: Network metadata: name: example-network spec: cidr: \u0026quot;192.168.0.0/16\u0026quot; gateway: \u0026quot;192.168.0.1\u0026quot;  可以看到，我想要描述“网络”的 API 资源类型是 Network；API 组是samplecrd.k8s.io；API 版本是 v1。\n那么，Kubernetes 又该如何知道这个 API（samplecrd.k8s.io/v1/network）的存在呢？\n其实，上面的这个 YAML 文件，就是一个具体的“自定义 API 资源”实例，也叫 CR（Custom Resource）。而为了能够让 Kubernetes 认识这个 CR，你就需要让 Kubernetes 明白这个 CR 的宏观定义是什么，也就是 CRD（Custom Resource Definition）。\n这就好比，你想让计算机认识各种兔子的照片，就得先让计算机明白，兔子的普遍定义是什么。比如，兔子“是哺乳动物”“有长耳朵，三瓣嘴”。\n所以，接下来，我就先编写一个 CRD 的 YAML 文件，它的名字叫作 network.yaml，内容如下所示：\napiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: networks.samplecrd.k8s.io spec: group: samplecrd.k8s.io version: v1 names: kind: Network plural: networks scope: Namespaced  可以看到，在这个 CRD 中，我指定了“group: samplecrd.k8s.io”“version: v1”这样的 API 信息，也指定了这个 CR 的资源类型叫作 Network，复数（plural）是 networks。然后，我还声明了它的 scope 是 Namespaced，即：我们定义的这个 Network 是一个属于 Namespace 的对象，类似于 Pod。这就是一个 Network API 资源类型的 API 部分的宏观定义了。这就等同于告诉了计算机：“兔子是哺乳动物”。所以这时候，Kubernetes 就能够认识和处理所有声明了 API 类型是“samplecrd.k8s.io/v1/network”的 YAML 文件了。接下来，我还需要让 Kubernetes“认识”这种 YAML 文件里描述的“网络”部分，比如“cidr”（网段），“gateway”（网关）这些字段的含义。这就相当于我要告诉计算机：“兔子有长耳朵和三瓣嘴”。这时候呢，我就需要稍微做些代码工作了。\n首先，我要在 GOPATH 下，创建一个结构如下的项目：\n$ tree $GOPATH/src/github.com/\u0026lt;your-name\u0026gt;/k8s-controller-custom-resource . ├── controller.go ├── crd │ └── network.yaml ├── example │ └── example-network.yaml ├── main.go └── pkg └── apis └── samplecrd ├── register.go └── v1 ├── doc.go ├── register.go └── types.go  其中，pkg/apis/samplecrd 就是 API 组的名字，v1 是版本，而 v1 下面的 types.go 文件里，则定义了 Network 对象的完整描述。我已经把这个项目上传到了 GitHub 上，你可以随时参考。\n然后，我在 pkg/apis/samplecrd 目录下创建了一个 register.go 文件，用来放置后面要用到的全局变量。这个文件的内容如下所示：\npackage samplecrd const ( GroupName = \u0026quot;samplecrd.k8s.io\u0026quot; Version = \u0026quot;v1\u0026quot; )  接着，我需要在 pkg/apis/samplecrd 目录下添加一个 doc.go 文件（Golang 的文档源文件）。这个文件里的内容如下所示：\n// +k8s:deepcopy-gen=package // +groupName=samplecrd.k8s.io package v1  在这个文件中，你会看到 +\u0026lt;tag_name\u0026gt;[=value]格式的注释，这就是 Kubernetes 进行代码生成要用的 Annotation 风格的注释。其中，+k8s:deepcopy-gen=package 意思是，请为整个 v1 包里的所有类型定义自动生成 DeepCopy 方法；而+groupName=samplecrd.k8s.io，则定义了这个包对应的 API 组的名字。可以看到，这些定义在 doc.go 文件的注释，起到的是全局的代码生成控制的作用，所以也被称为 Global Tags。\n接下来，我需要添加 types.go 文件。顾名思义，它的作用就是定义一个 Network 类型到底有哪些字段（比如，spec 字段里的内容）。这个文件的主要内容如下所示：\npackage v1 ... // +genclient // +genclient:noStatus // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // Network describes a Network resource type Network struct { // TypeMeta is the metadata for the resource, like kind and apiversion metav1.TypeMeta `json:\u0026quot;,inline\u0026quot;` // ObjectMeta contains the metadata for the particular object, including // things like... // - name // - namespace // - self link // - labels // - ... etc ... metav1.ObjectMeta `json:\u0026quot;metadata,omitempty\u0026quot;` Spec networkspec `json:\u0026quot;spec\u0026quot;` } // networkspec is the spec for a Network resource type networkspec struct { Cidr string `json:\u0026quot;cidr\u0026quot;` Gateway string `json:\u0026quot;gateway\u0026quot;` } // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // NetworkList is a list of Network resources type NetworkList struct { metav1.TypeMeta `json:\u0026quot;,inline\u0026quot;` metav1.ListMeta `json:\u0026quot;metadata\u0026quot;` Items []Network `json:\u0026quot;items\u0026quot;` }  在上面这部分代码里，你可以看到 Network 类型定义方法跟标准的 Kubernetes 对象一样，都包括了 TypeMeta（API 元数据）和 ObjectMeta（对象元数据）字段。而其中的 Spec 字段，就是需要我们自己定义的部分。所以，在 networkspec 里，我定义了 Cidr 和 Gateway 两个字段。其中，每个字段最后面的部分比如json:\u0026ldquo;cidr\u0026rdquo;，指的就是这个字段被转换成 JSON 格式之后的名字，也就是 YAML 文件里的字段名字。\n此外，除了定义 Network 类型，你还需要定义一个 NetworkList 类型，用来描述一组 Network 对象应该包括哪些字段。之所以需要这样一个类型，是因为在 Kubernetes 中，获取所有 X 对象的 List() 方法，返回值都是List 类型，而不是 X 类型的数组。这是不一样的。同样地，在 Network 和 NetworkList 类型上，也有代码生成注释。\n其中，+genclient 的意思是：请为下面这个 API 资源类型生成对应的 Client 代码（这个 Client，我马上会讲到）。而 +genclient:noStatus 的意思是：这个 API 资源类型定义里，没有 Status 字段。否则，生成的 Client 就会自动带上 UpdateStatus 方法。\n如果你的类型定义包括了 Status 字段的话，就不需要这句 +genclient:noStatus 注释了。比如下面这个例子：\n// +genclient // Network is a specification for a Network resource type Network struct { metav1.TypeMeta `json:\u0026quot;,inline\u0026quot;` metav1.ObjectMeta `json:\u0026quot;metadata,omitempty\u0026quot;` Spec NetworkSpec `json:\u0026quot;spec\u0026quot;` Status NetworkStatus `json:\u0026quot;status\u0026quot;` }  需要注意的是，+genclient 只需要写在 Network 类型上，而不用写在 NetworkList 上。因为 NetworkList 只是一个返回值类型，Network 才是“主类型”。\n而由于我在 Global Tags 里已经定义了为所有类型生成 DeepCopy 方法，所以这里就不需要再显式地加上 +k8s:deepcopy-gen=true 了。当然，这也就意味着你可以用 +k8s:deepcopy-gen=false 来阻止为某些类型生成 DeepCopy。\n你可能已经注意到，在这两个类型上面还有一句+k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object的注释。它的意思是，请在生成 DeepCopy 的时候，实现 Kubernetes 提供的 runtime.Object 接口。否则，在某些版本的 Kubernetes 里，你的这个类型定义会出现编译错误。这是一个固定的操作，记住即可。\n不过，你或许会有这样的顾虑：这些代码生成注释这么灵活，我该怎么掌握呢？其实，上面我所讲述的内容，已经足以应对 99% 的场景了。当然，如果你对代码生成感兴趣的话，我推荐你阅读这篇博客，它详细地介绍了 Kubernetes 的代码生成语法。\n最后，我需要再编写一个 pkg/apis/samplecrd/v1/register.go 文件。\n在前面对 APIServer 工作原理的讲解中，我已经提到，“registry”的作用就是注册一个类型（Type）给 APIServer。其中，Network 资源类型在服务器端注册的工作，APIServer 会自动帮我们完成。但与之对应的，我们还需要让客户端也能“知道”Network 资源类型的定义。这就需要我们在项目里添加一个 register.go 文件。它最主要的功能，就是定义了如下所示的 addKnownTypes() 方法：\npackage v1 ... // addKnownTypes adds our types to the API scheme by registering // Network and NetworkList func addKnownTypes(scheme *runtime.Scheme) error { scheme.AddKnownTypes( SchemeGroupVersion, \u0026amp;Network{}, \u0026amp;NetworkList{}, ) // register the type in the scheme metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil }  有了这个方法，Kubernetes 就能够在后面生成客户端的时候，“知道”Network 以及 NetworkList 类型的定义了。像上面这种 register.go 文件里的内容其实是非常固定的，你以后可以直接使用我提供的这部分代码做模板，然后把其中的资源类型、GroupName 和 Version 替换成你自己的定义即可。\n这样，Network 对象的定义工作就全部完成了。可以看到，它其实定义了两部分内容：\n 第一部分是，自定义资源类型的 API 描述，包括：组（Group）、版本（Version）、资源类型（Resource）等。这相当于告诉了计算机：兔子是哺乳动物。 第二部分是，自定义资源类型的对象描述，包括：Spec、Status 等。这相当于告诉了计算机：兔子有长耳朵和三瓣嘴。  接下来，我就要使用 Kubernetes 提供的代码生成工具，为上面定义的 Network 资源类型自动生成 clientset、informer 和 lister。其中，clientset 就是操作 Network 对象所需要使用的客户端，而 informer 和 lister 这两个包的主要功能，我会在下一篇文章中重点讲解。\n这个代码生成工具名叫k8s.io/code-generator，使用方法如下所示：\n# 代码生成的工作目录，也就是我们的项目路径 $ ROOT_PACKAGE=\u0026quot;github.com/resouer/k8s-controller-custom-resource\u0026quot; # API Group $ CUSTOM_RESOURCE_NAME=\u0026quot;samplecrd\u0026quot; # API Version $ CUSTOM_RESOURCE_VERSION=\u0026quot;v1\u0026quot; # 安装k8s.io/code-generator $ go get -u k8s.io/code-generator/... $ cd $GOPATH/src/k8s.io/code-generator # 执行代码自动生成，其中pkg/client是生成目标目录，pkg/apis是类型定义目录 $ ./generate-groups.sh all \u0026quot;$ROOT_PACKAGE/pkg/client\u0026quot; \u0026quot;$ROOT_PACKAGE/pkg/apis\u0026quot; \u0026quot;$CUSTOM_RESOURCE_NAME:$CUSTOM_RESOURCE_VERSION\u0026quot;  代码生成工作完成之后，我们再查看一下这个项目的目录结构：\n$ tree . ├── controller.go ├── crd │ └── network.yaml ├── example │ └── example-network.yaml ├── main.go └── pkg ├── apis │ └── samplecrd │ ├── constants.go │ └── v1 │ ├── doc.go │ ├── register.go │ ├── types.go │ └── zz_generated.deepcopy.go └── client ├── clientset ├── informers └── listers  其中，pkg/apis/samplecrd/v1 下面的 zz_generated.deepcopy.go 文件，就是自动生成的 DeepCopy 代码文件。而整个 client 目录，以及下面的三个包（clientset、informers、 listers），都是 Kubernetes 为 Network 类型生成的客户端库，这些库会在后面编写自定义控制器的时候用到。可以看到，到目前为止的这些工作，其实并不要求你写多少代码，主要考验的是“复制、粘贴、替换”这样的“基本功”。而有了这些内容，现在你就可以在 Kubernetes 集群里创建一个 Network 类型的 API 对象了。我们不妨一起来试验下。\n首先，使用 network.yaml 文件，在 Kubernetes 中创建 Network 对象的 CRD（Custom Resource Definition）：\n$ kubectl apply -f crd/network.yaml customresourcedefinition.apiextensions.k8s.io/networks.samplecrd.k8s.io created  这个操作，就告诉了 Kubernetes，我现在要添加一个自定义的 API 对象。而这个对象的 API 信息，正是 network.yaml 里定义的内容。我们可以通过 kubectl get 命令，查看这个 CRD：\n$ kubectl get crd NAME CREATED AT networks.samplecrd.k8s.io 2018-09-15T10:57:12Z  然后，我们就可以创建一个 Network 对象了，这里用到的是 example-network.yaml：\n$ kubectl apply -f example/example-network.yaml network.samplecrd.k8s.io/example-network created  通过这个操作，你就在 Kubernetes 集群里创建了一个 Network 对象。它的 API 资源路径是samplecrd.k8s.io/v1/networks。这时候，你就可以通过 kubectl get 命令，查看到新创建的 Network 对象：\n$ kubectl get network NAME AGE example-network 8s  你还可以通过 kubectl describe 命令，看到这个 Network 对象的细节：\n$ kubectl describe network example-network Name: example-network Namespace: default Labels: \u0026lt;none\u0026gt; ...API Version: samplecrd.k8s.io/v1 Kind: Network Metadata: ... Generation: 1 Resource Version: 468239 ... Spec: Cidr: 192.168.0.0/16 Gateway: 192.168.0.1  当然 ，你也可以编写更多的 YAML 文件来创建更多的 Network 对象，这和创建 Pod、Deployment 的操作，没有任何区别。\n总结\n在今天这篇文章中，我为你详细解析了 Kubernetes 声明式 API 的工作原理，讲解了如何遵循声明式 API 的设计，为 Kubernetes 添加一个名叫 Network 的 API 资源类型。从而达到了通过标准的 kubectl create 和 get 操作，来管理自定义 API 对象的目的。不过，创建出这样一个自定义 API 对象，我们只是完成了 Kubernetes 声明式 API 的一半工作。接下来的另一半工作是：为这个 API 对象编写一个自定义控制器（Custom Controller）。这样， Kubernetes 才能根据 Network API 对象的“增、删、改”操作，在真实环境中做出相应的响应。比如，“创建、删除、修改”真正的 Neutron 网络。而这，正是 Network 这个 API 对象所关注的“业务逻辑”。这个业务逻辑的实现过程，以及它所使用的 Kubernetes API 编程库的工作原理，就是我要在下一篇文章中讲解的主要内容。\n思考题\n在了解了 CRD 的定义方法之后，你是否已经在考虑使用 CRD（或者已经使用了 CRD）来描述现实中的某种实体了呢？能否分享一下你的思路？（举个例子：某技术团队使用 CRD 描述了“宿主机”，然后用 Kubernetes 部署了 Kubernetes）\n参考\nk8s build\n","date":"2021年03月12日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter3/api-object/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文转自张磊老师的《深入剖析 Kubernetes》课程，本节主要是关于自定义API资源(CRD)。\u003c/p\u003e\n\u003c/blockquote\u003e","title":"深入解析声明式API（一）：API对象的奥秘"},{"contents":" 本文转自张磊老师的《深入剖析 Kubernetes》课程，本节主要是关于声明式 API 与 Kubernetes 编程范式。\n 在前面的几篇文章中，我和你分享了很多 Kubernetes 的 API 对象。这些 API 对象，有的是用来描述应用，有的则是为应用提供各种各样的服务。但是，无一例外地，为了使用这些 API 对象提供的能力，你都需要编写一个对应的 YAML 文件交给 Kubernetes。\n这个 YAML 文件，正是 Kubernetes 声明式 API 所必须具备的一个要素。不过，是不是只要用 YAML 文件代替了命令行操作，就是声明式 API 了呢？\n举个例子。我们知道，Docker Swarm 的编排操作都是基于命令行的，比如：\n$ docker service create --name nginx --replicas 2 nginx $ docker service update --image nginx:1.7.9 nginx  像这样的两条命令，就是用 Docker Swarm 启动了两个 Nginx 容器实例。其中，第一条 create 命令创建了这两个容器，而第二条 update 命令则把它们“滚动更新”成了一个新的镜像。\n对于这种使用方式，我们称为命令式命令行操作。\n那么，像上面这样的创建和更新两个 Nginx 容器的操作，在 Kubernetes 里又该怎么做呢？这个流程，相信你已经非常熟悉了：我们需要在本地编写一个 Deployment 的 YAML 文件：\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80  然后，我们还需要使用 kubectl create 命令在 Kubernetes 里创建这个 Deployment 对象：\n$ kubectl create -f nginx.yaml  这样，两个 Nginx 的 Pod 就会运行起来了。而如果要更新这两个 Pod 使用的 Nginx 镜像，该怎么办呢？我们前面曾经使用过 kubectl set image 和 kubectl edit 命令，来直接修改 Kubernetes 里的 API 对象。不过，相信很多人都有这样的想法，我能不能通过修改本地 YAML 文件来完成这个操作呢？这样我的改动就会体现在这个本地 YAML 文件里了。当然可以。比如，我们可以修改这个 YAML 文件里的 Pod 模板部分，把 Nginx 容器的镜像改成 1.7.9，如下所示：\n... spec: containers: - name: nginx image: nginx:1.7.9  而接下来，我们就可以执行一句 kubectl replace 操作，来完成这个 Deployment 的更新：\n$ kubectl replace -f nginx.yaml  可是，上面这种基于 YAML 文件的操作方式，是“声明式 API”吗？并不是。对于上面这种先 kubectl create，再 replace 的操作，我们称为命令式配置文件操作。也就是说，它的处理方式，其实跟前面 Docker Swarm 的两句命令，没什么本质上的区别。只不过，它是把 Docker 命令行里的参数，写在了配置文件里而已。\n那么，到底什么才是“声明式 API”呢？\n答案是，kubectl apply 命令。\n在前面的文章中，我曾经提到过这个 kubectl apply 命令，并推荐你使用它来代替 kubectl create 命令,现在，我就使用 kubectl apply 命令来创建这个 Deployment：\n$ kubectl apply -f nginx.yaml  这样，Nginx 的 Deployment 就被创建了出来，这看起来跟 kubectl create 的效果一样。\n然后，我再修改一下 nginx.yaml 里定义的镜像：\n... spec: containers: - name: nginx image: nginx:1.7.9  这时候，关键来了。在修改完这个 YAML 文件之后，我不再使用 kubectl replace 命令进行更新，而是继续执行一条 kubectl apply 命令，即：\n$ kubectl apply -f nginx.yaml  这时，Kubernetes 就会立即触发这个 Deployment 的“滚动更新”。可是，它跟 kubectl replace 命令有什么本质区别吗？\n实际上，你可以简单地理解为，kubectl replace 的执行过程，是使用新的 YAML 文件中的 API 对象，替换原有的 API 对象；而 kubectl apply，则是执行了一个对原有 API 对象的 PATCH 操作。\n更进一步地，这意味着 kube-apiserver 在响应命令式请求（比如，kubectl replace）的时候，一次只能处理一个写请求，否则会有产生冲突的可能。而对于声明式请求（比如，kubectl apply），一次能处理多个写操作，并且具备 Merge 能力。\n这种区别，可能乍一听起来没那么重要。而且，正是由于要照顾到这样的 API 设计，做同样一件事情，Kubernetes 需要的步骤往往要比其他项目多不少。但是，如果你仔细思考一下 Kubernetes 项目的工作流程，就不难体会到这种声明式 API 的独到之处。\n接下来，我就以 Istio 项目为例，来为你讲解一下声明式 API 在实际使用时的重要意义。\n在 2017 年 5 月，Google、IBM 和 Lyft 公司，共同宣布了 Istio 开源项目的诞生。很快，这个项目就在技术圈儿里，掀起了一阵名叫“微服务”的热潮，把 Service Mesh 这个新的编排概念推到了风口浪尖。而 Istio 项目，实际上就是一个基于 Kubernetes 项目的微服务治理框架。它的架构非常清晰，如下所示：\n在上面这个架构图中，我们不难看到 Istio 项目架构的核心所在。Istio 最根本的组件，是运行在每一个应用 Pod 里的 Envoy 容器。这个 Envoy 项目是 Lyft 公司推出的一个高性能 C++ 网络代理，也是 Lyft 公司对 Istio 项目的唯一贡献。而 Istio 项目，则把这个代理服务以 sidecar 容器的方式，运行在了每一个被治理的应用 Pod 中。我们知道，Pod 里的所有容器都共享同一个 Network Namespace。所以，Envoy 容器就能够通过配置 Pod 里的 iptables 规则，把整个 Pod 的进出流量接管下来。这时候，Istio 的控制层（Control Plane）里的 Pilot 组件，就能够通过调用每个 Envoy 容器的 API，对这个 Envoy 代理进行配置，从而实现微服务治理。\n我们一起来看一个例子。假设这个 Istio 架构图左边的 Pod 是已经在运行的应用，而右边的 Pod 则是我们刚刚上线的应用的新版本。这时候，Pilot 通过调节这两 Pod 里的 Envoy 容器的配置，从而将 90% 的流量分配给旧版本的应用，将 10% 的流量分配给新版本应用，并且，还可以在后续的过程中随时调整。这样，一个典型的“灰度发布”的场景就完成了。比如，Istio 可以调节这个流量从 90%-10%，改到 80%-20%，再到 50%-50%，最后到 0%-100%，就完成了这个灰度发布的过程。更重要的是，在整个微服务治理的过程中，无论是对 Envoy 容器的部署，还是像上面这样对 Envoy 代理的配置，用户和应用都是完全“无感”的。\n这时候，你可能会有所疑惑：Istio 项目明明需要在每个 Pod 里安装一个 Envoy 容器，又怎么能做到“无感”的呢？实际上，Istio 项目使用的，是 Kubernetes 中的一个非常重要的功能，叫作 Dynamic Admission Control。\n在 Kubernetes 项目中，当一个 Pod 或者任何一个 API 对象被提交给 APIServer 之后，总有一些“初始化”性质的工作需要在它们被 Kubernetes 项目正式处理之前进行。比如，自动为所有 Pod 加上某些标签（Labels）。而这个“初始化”操作的实现，借助的是一个叫作 Admission 的功能。它其实是 Kubernetes 项目里一组被称为 Admission Controller 的代码，可以选择性地被编译进 APIServer 中，在 API 对象创建之后会被立刻调用到。\n但这就意味着，如果你现在想要添加一些自己的规则到 Admission Controller，就会比较困难。因为，这要求重新编译并重启 APIServer。显然，这种使用方法对 Istio 来说，影响太大了。所以，Kubernetes 项目为我们额外提供了一种“热插拔”式的 Admission 机制，它就是 Dynamic Admission Control，也叫作：Initializer。\n现在，我给你举个例子。比如，我有如下所示的一个应用 Pod：\napiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 3600']  可以看到，这个 Pod 里面只有一个用户容器，叫作：myapp-container。接下来，Istio 项目要做的，就是在这个 Pod YAML 被提交给 Kubernetes 之后，在它对应的 API 对象里自动加上 Envoy 容器的配置，使这个对象变成如下所示的样子：\napiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 3600'] - name: envoy image: lyft/envoy:845747b88f102c0fd262ab234308e9e22f693a1 command: [\u0026quot;/usr/local/bin/envoy\u0026quot;] ...  可以看到，被 Istio 处理后的这个 Pod 里，除了用户自己定义的 myapp-container 容器之外，多出了一个叫作 envoy 的容器，它就是 Istio 要使用的 Envoy 代理。那么，Istio 又是如何在用户完全不知情的前提下完成这个操作的呢？Istio 要做的，就是编写一个用来为 Pod“自动注入”Envoy 容器的 Initializer。\n首先，Istio 会将这个 Envoy 容器本身的定义，以 ConfigMap 的方式保存在 Kubernetes 当中。这个 ConfigMap（名叫：envoy-initializer）的定义如下所示：\napiVersion: v1 kind: ConfigMap metadata: name: envoy-initializer data: config: | containers: - name: envoy image: lyft/envoy:845747db88f102c0fd262ab234308e9e22f693a1 command: [\u0026quot;/usr/local/bin/envoy\u0026quot;] args: - \u0026quot;--concurrency 4\u0026quot; - \u0026quot;--config-path /etc/envoy/envoy.json\u0026quot; - \u0026quot;--mode serve\u0026quot; ports: - containerPort: 80 protocol: TCP resources: limits: cpu: \u0026quot;1000m\u0026quot; memory: \u0026quot;512Mi\u0026quot; requests: cpu: \u0026quot;100m\u0026quot; memory: \u0026quot;64Mi\u0026quot; volumeMounts: - name: envoy-conf mountPath: /etc/envoy volumes: - name: envoy-conf configMap: name: envoy  相信你已经注意到了，这个 ConfigMap 的 data 部分，正是一个 Pod 对象的一部分定义。其中，我们可以看到 Envoy 容器对应的 containers 字段，以及一个用来声明 Envoy 配置文件的 volumes 字段。不难想到，Initializer 要做的工作，就是把这部分 Envoy 相关的字段，自动添加到用户提交的 Pod 的 API 对象里。可是，用户提交的 Pod 里本来就有 containers 字段和 volumes 字段，所以 Kubernetes 在处理这样的更新请求时，就必须使用类似于 git merge 这样的操作，才能将这两部分内容合并在一起。所以说，在 Initializer 更新用户的 Pod 对象的时候，必须使用 PATCH API 来完成。而这种 PATCH API，正是声明式 API 最主要的能力。\n接下来，Istio 将一个编写好的 Initializer，作为一个 Pod 部署在 Kubernetes 中。这个 Pod 的定义非常简单，如下所示：\napiVersion: v1 kind: Pod metadata: labels: app: envoy-initializer name: envoy-initializer spec: containers: - name: envoy-initializer image: envoy-initializer:0.0.1 imagePullPolicy: Always  我们可以看到，这个 envoy-initializer 使用的 envoy-initializer:0.0.1 镜像，就是一个事先编写好的“自定义控制器”（Custom Controller），我将会在下一篇文章中讲解它的编写方法。而在这里，我要先为你解释一下这个控制器的主要功能。我曾在第 16 篇文章《编排其实很简单：谈谈“控制器”模型》中和你分享过，一个 Kubernetes 的控制器，实际上就是一个“死循环”：它不断地获取“实际状态”，然后与“期望状态”作对比，并以此为依据决定下一步的操作。而 Initializer 的控制器，不断获取到的“实际状态”，就是用户新创建的 Pod。而它的“期望状态”，则是：这个 Pod 里被添加了 Envoy 容器的定义。我还是用一段 Go 语言风格的伪代码，来为你描述这个控制逻辑，如下所示：\nfor { // 获取新创建的Pod pod := client.GetLatestPod() // Diff一下，检查是否已经初始化过 if !isInitialized(pod) { // 没有？那就来初始化一下 doSomething(pod) } }   如果这个 Pod 里面已经添加过 Envoy 容器，那么就“放过”这个 Pod，进入下一个检查周期。 而如果还没有添加过 Envoy 容器的话，它就要进行 Initialize 操作了，即：修改该 Pod 的 API 对象（doSomething 函数）。  这时候，你应该立刻能想到，Istio 要往这个 Pod 里合并的字段，正是我们之前保存在 envoy-initializer 这个 ConfigMap 里的数据（即：它的 data 字段的值）。所以，在 Initializer 控制器的工作逻辑里，它首先会从 APIServer 中拿到这个 ConfigMap：\nfunc doSomething(pod) { cm := client.Get(ConfigMap, \u0026quot;envoy-initializer\u0026quot;) }  然后，把这个 ConfigMap 里存储的 containers 和 volumes 字段，直接添加进一个空的 Pod 对象里：\nfunc doSomething(pod) { cm := client.Get(ConfigMap, \u0026quot;envoy-initializer\u0026quot;) newPod := Pod{} newPod.Spec.Containers = cm.Containers newPod.Spec.Volumes = cm.Volumes }  现在，关键来了。Kubernetes 的 API 库，为我们提供了一个方法，使得我们可以直接使用新旧两个 Pod 对象，生成一个 TwoWayMergePatch：\nfunc doSomething(pod) { cm := client.Get(ConfigMap, \u0026quot;envoy-initializer\u0026quot;) newPod := Pod{} newPod.Spec.Containers = cm.Containers newPod.Spec.Volumes = cm.Volumes // 生成patch数据 patchBytes := strategicpatch.CreateTwoWayMergePatch(pod, newPod) // 发起PATCH请求，修改这个pod对象 client.Patch(pod.Name, patchBytes) }  有了这个 TwoWayMergePatch 之后，Initializer 的代码就可以使用这个 patch 的数据，调用 Kubernetes 的 Client，发起一个 PATCH 请求。\n这样，一个用户提交的 Pod 对象里，就会被自动加上 Envoy 容器相关的字段。当然，Kubernetes 还允许你通过配置，来指定要对什么样的资源进行这个 Initialize 操作，比如下面这个例子：\napiVersion: admissionregistration.k8s.io/v1alpha1 kind: InitializerConfiguration metadata: name: envoy-config initializers: // 这个名字必须至少包括两个 \u0026quot;.\u0026quot; - name: envoy.initializer.kubernetes.io rules: - apiGroups: - \u0026quot;\u0026quot; // 前面说过， \u0026quot;\u0026quot;就是core API Group的意思 apiVersions: - v1 resources: - pods  这个配置，就意味着 Kubernetes 要对所有的 Pod 进行这个 Initialize 操作，并且，我们指定了负责这个操作的 Initializer，名叫：envoy-initializer。而一旦这个 InitializerConfiguration 被创建，Kubernetes 就会把这个 Initializer 的名字，加在所有新创建的 Pod 的 Metadata 上，格式如下所示：\napiVersion: v1 kind: Pod metadata: initializers: pending: - name: envoy.initializer.kubernetes.io name: myapp-pod labels: app: myapp ...  可以看到，每一个新创建的 Pod，都会自动携带了 metadata.initializers.pending 的 Metadata 信息。这个 Metadata，正是接下来 Initializer 的控制器判断这个 Pod 有没有执行过自己所负责的初始化操作的重要依据（也就是前面伪代码中 isInitialized() 方法的含义）。\n这也就意味着，当你在 Initializer 里完成了要做的操作后，一定要记得将这个 metadata.initializers.pending 标志清除掉。这一点，你在编写 Initializer 代码的时候一定要非常注意。\n此外，除了上面的配置方法，你还可以在具体的 Pod 的 Annotation 里添加一个如下所示的字段，从而声明要使用某个 Initializer：\napiVersion: v1 kind: Pod metadata annotations: \u0026quot;initializer.kubernetes.io/envoy\u0026quot;: \u0026quot;true\u0026quot; ...  在这个 Pod 里，我们添加了一个 Annotation，写明： initializer.kubernetes.io/envoy=true。这样，就会使用到我们前面所定义的 envoy-initializer 了。\n以上，就是关于 Initializer 最基本的工作原理和使用方法了。相信你此时已经明白，Istio 项目的核心，就是由无数个运行在应用 Pod 中的 Envoy 容器组成的服务代理网格。这也正是 Service Mesh 的含义。\n而这个机制得以实现的原理，正是借助了 Kubernetes 能够对 API 对象进行在线更新的能力，这也正是 Kubernetes“声明式 API”的独特之处：\n 首先，所谓“声明式”，指的就是我只需要提交一个定义好的 API 对象来“声明”，我所期望的状态是什么样子。 其次，“声明式 API”允许有多个 API 写端，以 PATCH 的方式对 API 对象进行修改，而无需关心本地原始 YAML 文件的内容。 最后，也是最重要的，有了上述两个能力，Kubernetes 项目才可以基于对 API 对象的增、删、改、查，在完全无需外界干预的情况下，完成对“实际状态”和“期望状态”的调谐（Reconcile）过程。  所以说，声明式 API，才是 Kubernetes 项目编排能力“赖以生存”的核心所在，希望你能够认真理解。\n此外，不难看到，无论是对 sidecar 容器的巧妙设计，还是对 Initializer 的合理利用，Istio 项目的设计与实现，其实都依托于 Kubernetes 的声明式 API 和它所提供的各种编排能力。可以说，Istio 是在 Kubernetes 项目使用上的一位“集大成者”。\n 要知道，一个 Istio 项目部署完成后，会在 Kubernetes 里创建大约 43 个 API 对象。\n 所以，Kubernetes 社区也看得很明白：Istio 项目有多火热，就说明 Kubernetes 这套“声明式 API”有多成功。这，既是 Google Cloud 喜闻乐见的事情，也是 Istio 项目一推出就被 Google 公司和整个技术圈儿热捧的重要原因。\n而在使用 Initializer 的流程中，最核心的步骤，莫过于 Initializer“自定义控制器”的编写过程。它遵循的，正是标准的“Kubernetes 编程范式”， 即：\n 如何使用控制器模式，同 Kubernetes 里 API 对象的“增、删、改、查”进行协作，进而完成用户业务逻辑的编写过程\n 这，也正是我要在后面文章中为你详细讲解的内容。\n总结\n在今天这篇文章中，我为你重点讲解了 Kubernetes 声明式 API 的含义。并且，通过对 Istio 项目的剖析，我为你说明了它使用 Kubernetes 的 Initializer 特性，完成 Envoy 容器“自动注入”的原理。事实上，从“使用 Kubernetes 部署代码”，到“使用 Kubernetes 编写代码”的蜕变过程，正是你从一个 Kubernetes 用户，到 Kubernetes 玩家的晋级之路。而，如何理解“Kubernetes 编程范式”，如何为 Kubernetes 添加自定义 API 对象，编写自定义控制器，正是这个晋级过程中的关键点，也是我要在后面几篇文章中分享的核心内容。此外，基于今天这篇文章所讲述的 Istio 的工作原理，尽管 Istio 项目一直宣称它可以运行在非 Kubernetes 环境中，但我并不建议你花太多时间去做这个尝试。毕竟，无论是从技术实现还是在社区运作上，Istio 与 Kubernetes 项目之间都是紧密的、唇齿相依的关系。如果脱离了 Kubernetes 项目这个基础，那么这条原本就不算平坦的“微服务”之路，恐怕会更加困难重重。\n参考\n谈 Kubernetes 的架构设计与实现原理\n","date":"2021年03月12日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter3/declarative-api/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文转自张磊老师的《深入剖析 Kubernetes》课程，本节主要是关于声明式 API 与 Kubernetes 编程范式。\u003c/p\u003e\n\u003c/blockquote\u003e","title":"声明式API与Kubernetes编程范式"},{"contents":" 本文转自张磊老师的《深入剖析Kubernetes》课程.\n 实际上，它们主要编排的对象，都是“在线业务”，即：Long Running Task（长作业）。比如，我在前面举例时常用的 Nginx、Tomcat，以及 MySQL 等等。这些应用一旦运行起来，除非出错或者停止，它的容器进程会一直保持在 Running 状态。\n但是，有一类作业显然不满足这样的条件，这就是“离线业务”，或者叫作 Batch Job（计算业务）。这种业务在计算完成后就直接退出了，而此时如果你依然用 Deployment 来管理这种业务的话，就会发现 Pod 会在计算结束后退出，然后被 Deployment Controller 不断地重启；而像“滚动更新”这样的编排功能，更无从谈起了。\n所以，早在 Borg 项目中，Google 就已经对作业进行了分类处理，提出了 LRS（Long Running Service）和 Batch Jobs 两种作业形态，对它们进行“分别管理”和“混合调度”。\n不过，在 2015 年 Borg 论文刚刚发布的时候，Kubernetes 项目并不支持对 Batch Job 的管理。直到 v1.4 版本之后，社区才逐步设计出了一个用来描述离线业务的 API 对象，它的名字就是：Job。\nJob API 对象的定义非常简单，我来举个例子，如下所示：\napiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: resouer/ubuntu-bc command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;echo 'scale=10000; 4*a(1)' | bc -l \u0026quot;] restartPolicy: Never backoffLimit: 4  此时，相信你对 Kubernetes 的 API 对象已经不再陌生了。在这个 Job 的 YAML 文件里，你肯定一眼就会看到一位“老熟人”：Pod 模板，即 spec.template 字段。\n在这个 Pod 模板中，我定义了一个 Ubuntu 镜像的容器（准确地说，是一个安装了 bc 命令的 Ubuntu 镜像），它运行的程序是：\necho \u0026quot;scale=10000; 4*a(1)\u0026quot; | bc -l  其中，bc 命令是 Linux 里的“计算器”；-l 表示，我现在要使用标准数学库；而 a(1)，则是调用数学库中的 arctangent 函数，计算 atan(1)。这是什么意思呢？\n中学知识告诉我们：tan(π/4) = 1。所以，4*atan(1)正好就是π，也就是 3.1415926…。\n所以，这其实就是一个计算π值的容器。而通过 scale=10000，我指定了输出的小数点后的位数是 10000。在我的计算机上，这个计算大概用时 1 分 54 秒。\n但是，跟其他控制器不同的是，Job 对象并不要求你定义一个 spec.selector 来描述要控制哪些 Pod。具体原因，我马上会讲解到。\n现在，我们就可以创建这个 Job 了：\n$ kubectl create -f job.yaml  在成功创建后，我们来查看一下这个 Job 对象，如下所示：\n$ kubectl describe jobs/pi Name: pi Namespace: default Selector: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 Labels: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 job-name=pi Annotations: \u0026lt;none\u0026gt; Parallelism: 1 Completions: 1 .. Pods Statuses: 0 Running / 1 Succeeded / 0 Failed Pod Template: Labels: controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495 job-name=pi Containers: ... Volumes: \u0026lt;none\u0026gt; Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 1m 1m 1 {job-controller } Normal SuccessfulCreate Created pod: pi-rq5rl  可以看到，这个 Job 对象在创建后，它的 Pod 模板，被自动加上了一个 controller-uid=\u0026lt; 一个随机字符串 \u0026gt; 这样的 Label。而这个 Job 对象本身，则被自动加上了这个 Label 对应的 Selector，从而 保证了 Job 与它所管理的 Pod 之间的匹配关系。\n而 Job Controller 之所以要使用这种携带了 UID 的 Label，就是为了避免不同 Job 对象所管理的 Pod 发生重合。需要注意的是，这种自动生成的 Label 对用户来说并不友好，所以不太适合推广到 Deployment 等长作业编排对象上。\n接下来，我们可以看到这个 Job 创建的 Pod 进入了 Running 状态，这意味着它正在计算 Pi 的值。\n$ kubectl get pods NAME READY STATUS RESTARTS AGE pi-rq5rl 1/1 Running 0 10s  而几分钟后计算结束，这个 Pod 就会进入 Completed 状态：\n$ kubectl get pods NAME READY STATUS RESTARTS AGE pi-rq5rl 0/1 Completed 0 4m  这也是我们需要在 Pod 模板中定义 restartPolicy=Never 的原因：离线计算的 Pod 永远都不应该被重启，否则它们会再重新计算一遍。\n 事实上，restartPolicy 在 Job 对象里只允许被设置为 Never 和 OnFailure；而在 Deployment 对象里，restartPolicy 则只允许被设置为 Always。\n 此时，我们通过 kubectl logs 查看一下这个 Pod 的日志，就可以看到计算得到的 Pi 值已经被打印了出来：\n$ kubectl logs pi-rq5rl 3.141592653589793238462643383279...  这时候，你一定会想到这样一个问题，如果这个离线作业失败了要怎么办？\n比如，我们在这个例子中定义了 restartPolicy=Never，那么离线作业失败后 Job Controller 就会不断地尝试创建一个新 Pod，如下所示：\n$ kubectl get pods NAME READY STATUS RESTARTS AGE pi-55h89 0/1 ContainerCreating 0 2s pi-tqbcz 0/1 Error 0 5s  可以看到，这时候会不断地有新 Pod 被创建出来。\n当然，这个尝试肯定不能无限进行下去。所以，我们就在 Job 对象的 spec.backoffLimit 字段里定义了重试次数为 4（即，backoffLimit=4），而这个字段的默认值是 6。\n需要注意的是，Job Controller 重新创建 Pod 的间隔是呈指数增加的，即下一次重新创建 Pod 的动作会分别发生在 10 s、20 s、40 s …后。\n而如果你定义的 restartPolicy=OnFailure，那么离线作业失败后，Job Controller 就不会去尝试创建新的 Pod。但是，它会不断地尝试重启 Pod 里的容器。这也正好对应了 restartPolicy 的含义（。\n如前所述，当一个 Job 的 Pod 运行结束后，它会进入 Completed 状态。但是，如果这个 Pod 因为某种原因一直不肯结束呢？\n在 Job 的 API 对象里，有一个 spec.activeDeadlineSeconds 字段可以设置最长运行时间，比如：\nspec: backoffLimit: 5 activeDeadlineSeconds: 100  一旦运行超过了 100 s，这个 Job 的所有 Pod 都会被终止。并且，你可以在 Pod 的状态里看到终止的原因是 reason: DeadlineExceeded。以上，就是一个 Job API 对象最主要的概念和用法了。不过，离线业务之所以被称为 Batch Job，当然是因为它们可以以“Batch”，也就是并行的方式去运行。\n接下来，我就来为你讲解一下Job Controller 对并行作业的控制方法。\n在 Job 对象中，负责并行控制的参数有两个：\n spec.parallelism，它定义的是一个 Job 在任意时间最多可以启动多少个 Pod 同时运行； spec.completions，它定义的是 Job 至少要完成的 Pod 数目，即 Job 的最小完成数。  这两个参数听起来有点儿抽象，所以我准备了一个例子来帮助你理解。\n现在，我在之前计算 Pi 值的 Job 里，添加这两个参数：\napiVersion: batch/v1 kind: Job metadata: name: pi spec: parallelism: 2 completions: 4 template: spec: containers: - name: pi image: resouer/ubuntu-bc command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;echo 'scale=5000; 4*a(1)' | bc -l \u0026quot;] restartPolicy: Never backoffLimit: 4  这样，我们就指定了这个 Job 最大的并行数是 2，而最小的完成数是 4。接下来，我们来创建这个 Job 对象：\n$ kubectl create -f job.yaml  可以看到，这个 Job 其实也维护了两个状态字段，即 DESIRED 和 SUCCESSFUL，如下所示：\n$ kubectl get job NAME DESIRED SUCCESSFUL AGE pi 4 0 3s  其中，DESIRED 的值，正是 completions 定义的最小完成数。然后，我们可以看到，这个 Job 首先创建了两个并行运行的 Pod 来计算 Pi：\n$ kubectl get pods NAME READY STATUS RESTARTS AGE pi-5mt88 1/1 Running 0 6s pi-gmcq5 1/1 Running 0 6s  而在 40 s 后，这两个 Pod 相继完成计算。\n这时我们可以看到，每当有一个 Pod 完成计算进入 Completed 状态时，就会有一个新的 Pod 被自动创建出来，并且快速地从 Pending 状态进入到 ContainerCreating 状态：\n$ kubectl get pods NAME READY STATUS RESTARTS AGE pi-gmcq5 0/1 Completed 0 40s pi-84ww8 0/1 Pending 0 0s pi-5mt88 0/1 Completed 0 41s pi-62rbt 0/1 Pending 0 0s $ kubectl get pods NAME READY STATUS RESTARTS AGE pi-gmcq5 0/1 Completed 0 40s pi-84ww8 0/1 ContainerCreating 0 0s pi-5mt88 0/1 Completed 0 41s pi-62rbt 0/1 ContainerCreating 0 0s  紧接着，Job Controller 第二次创建出来的两个并行的 Pod 也进入了 Running 状态：\n$ kubectl get pods NAME READY STATUS RESTARTS AGE pi-5mt88 0/1 Completed 0 54s pi-62rbt 1/1 Running 0 13s pi-84ww8 1/1 Running 0 14s pi-gmcq5 0/1 Completed 0 54s  最终，后面创建的这两个 Pod 也完成了计算，进入了 Completed 状态。这时，由于所有的 Pod 均已经成功退出，这个 Job 也就执行完了，所以你会看到它的 SUCCESSFUL 字段的值变成了 4：\n$ kubectl get pods NAME READY STATUS RESTARTS AGE pi-5mt88 0/1 Completed 0 5m pi-62rbt 0/1 Completed 0 4m pi-84ww8 0/1 Completed 0 4m pi-gmcq5 0/1 Completed 0 5m $ kubectl get job NAME DESIRED SUCCESSFUL AGE pi 4 4 5m  通过上述 Job 的 DESIRED 和 SUCCESSFUL 字段的关系，我们就可以很容易地理解Job Controller 的工作原理了。\n首先，Job Controller 控制的对象，直接就是 Pod。\n其次，Job Controller 在控制循环中进行的调谐（Reconcile）操作，是根据实际在 Running 状态 Pod 的数目、已经成功退出的 Pod 的数目，以及 parallelism、completions 参数的值共同计算出在这个周期里，应该创建或者删除的 Pod 数目，然后调用 Kubernetes API 来执行这个操作。\n以创建 Pod 为例。在上面计算 Pi 值的这个例子中，当 Job 一开始创建出来时，实际处于 Running 状态的 Pod 数目 =0，已经成功退出的 Pod 数目 =0，而用户定义的 completions，也就是最终用户需要的 Pod 数目 =4。\n所以，在这个时刻，需要创建的 Pod 数目 = 最终需要的 Pod 数目 - 实际在 Running 状态 Pod 数目 - 已经成功退出的 Pod 数目 = 4 - 0 - 0= 4。也就是说，Job Controller 需要创建 4 个 Pod 来纠正这个不一致状态。\n可是，我们又定义了这个 Job 的 parallelism=2。也就是说，我们规定了每次并发创建的 Pod 个数不能超过 2 个。所以，Job Controller 会对前面的计算结果做一个修正，修正后的期望创建的 Pod 数目应该是：2 个。\n这时候，Job Controller 就会并发地向 kube-apiserver 发起两个创建 Pod 的请求。\n类似地，如果在这次调谐周期里，Job Controller 发现实际在 Running 状态的 Pod 数目，比 parallelism 还大，那么它就会删除一些 Pod，使两者相等。\n综上所述，Job Controller 实际上控制了，作业执行的并行度，以及总共需要完成的任务数这两个重要参数。而在实际使用时，你需要根据作业的特性，来决定并行度（parallelism）和任务数（completions）的合理取值。\n接下来，我再和你分享三种常用的、使用 Job 对象的方法。\n第一种用法，也是最简单粗暴的用法：外部管理器 +Job 模板。\n这种模式的特定用法是：把 Job 的 YAML 文件定义为一个“模板”，然后用一个外部工具控制这些“模板”来生成 Job。这时，Job 的定义方式如下所示：\napiVersion: batch/v1 kind: Job metadata: name: process-item-$ITEM labels: jobgroup: jobexample spec: template: metadata: name: jobexample labels: jobgroup: jobexample spec: containers: - name: c image: busybox command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;echo Processing item $ITEM \u0026amp;\u0026amp; sleep 5\u0026quot;] restartPolicy: Never  可以看到，我们在这个 Job 的 YAML 里，定义了 $ITEM 这样的“变量”。\n所以，在控制这种 Job 时，我们只要注意如下两个方面即可：\n 创建 Job 时，替换掉 $ITEM 这样的变量； 所有来自于同一个模板的 Job，都有一个 jobgroup: jobexample 标签，也就是说这一组 Job 使用这样一个相同的标识。  而做到第一点非常简单。比如，你可以通过这样一句 shell 把 $ITEM 替换掉：\n$ mkdir ./jobs $ for i in apple banana cherry do cat job-tmpl.yaml | sed \u0026quot;s/\\$ITEM/$i/\u0026quot; \u0026gt; ./jobs/job-$i.yaml done  这样，一组来自于同一个模板的不同 Job 的 yaml 就生成了。接下来，你就可以通过一句 kubectl create 指令创建这些 Job 了：\n$ kubectl create -f ./jobs $ kubectl get pods -l jobgroup=jobexample NAME READY STATUS RESTARTS AGE process-item-apple-kixwv 0/1 Completed 0 4m process-item-banana-wrsf7 0/1 Completed 0 4m process-item-cherry-dnfu9 0/1 Completed 0 4m  这个模式看起来虽然很“傻”，但却是 Kubernetes 社区里使用 Job 的一个很普遍的模式。\n原因很简单：大多数用户在需要管理 Batch Job 的时候，都已经有了一套自己的方案，需要做的往往就是集成工作。这时候，Kubernetes 项目对这些方案来说最有价值的，就是 Job 这个 API 对象。所以，你只需要编写一个外部工具（等同于我们这里的 for 循环）来管理这些 Job 即可。\n这种模式最典型的应用，就是 TensorFlow 社区的 KubeFlow 项目。\n很容易理解，在这种模式下使用 Job 对象，completions 和 parallelism 这两个字段都应该使用默认值 1，而不应该由我们自行设置。而作业 Pod 的并行控制，应该完全交由外部工具来进行管理（比如，KubeFlow）。\n第二种用法：拥有固定任务数目的并行 Job。\n这种模式下，我只关心最后是否有指定数目（spec.completions）个任务成功退出。至于执行时的并行度是多少，我并不关心。\n比如，我们这个计算 Pi 值的例子，就是这样一个典型的、拥有固定任务数目（completions=4）的应用场景。 它的 parallelism 值是 2；或者，你可以干脆不指定 parallelism，直接使用默认的并行度（即：1）。\n此外，你还可以使用一个工作队列（Work Queue）进行任务分发。这时，Job 的 YAML 文件定义如下所示：\napiVersion: batch/v1 kind: Job metadata: name: job-wq-1 spec: completions: 8 parallelism: 2 template: metadata: name: job-wq-1 spec: containers: - name: c image: myrepo/job-wq-1 env: - name: BROKER_URL value: amqp://guest:guest@rabbitmq-service:5672 - name: QUEUE value: job1 restartPolicy: OnFailure  我们可以看到，它的 completions 的值是：8，这意味着我们总共要处理的任务数目是 8 个。也就是说，总共会有 8 个任务会被逐一放入工作队列里（你可以运行一个外部小程序作为生产者，来提交任务）。在这个实例中，我选择充当工作队列的是一个运行在 Kubernetes 里的 RabbitMQ。\n所以，我们需要在 Pod 模板里定义 BROKER_URL，来作为消费者。所以，一旦你用 kubectl create 创建了这个 Job，它就会以并发度为 2 的方式，每两个 Pod 一组，创建出 8 个 Pod。每个 Pod 都会去连接 BROKER_URL，从 RabbitMQ 里读取任务，然后各自进行处理。这个 Pod 里的执行逻辑，我们可以用这样一段伪代码来表示：\n/* job-wq-1的伪代码 */ queue := newQueue($BROKER_URL, $QUEUE) task := queue.Pop() process(task) exit  可以看到，每个 Pod 只需要将任务信息读取出来，处理完成，然后退出即可。而作为用户，我只关心最终一共有 8 个计算任务启动并且退出，只要这个目标达到，我就认为整个 Job 处理完成了。所以说，这种用法，对应的就是“任务总数固定”的场景。\n第三种用法，也是很常用的一个用法：指定并行度（parallelism），但不设置固定的 completions 的值。\n此时，你就必须自己想办法，来决定什么时候启动新 Pod，什么时候 Job 才算执行完成。在这种情况下，任务的总数是未知的，所以你不仅需要一个工作队列来负责任务分发，还需要能够判断工作队列已经为空（即：所有的工作已经结束了）。\n这时候，Job 的定义基本上没变化，只不过是不再需要定义 completions 的值了而已：\napiVersion: batch/v1 kind: Job metadata: name: job-wq-2 spec: parallelism: 2 template: metadata: name: job-wq-2 spec: containers: - name: c image: gcr.io/myproject/job-wq-2 env: - name: BROKER_URL value: amqp://guest:guest@rabbitmq-service:5672 - name: QUEUE value: job2 restartPolicy: OnFailure  而对应的 Pod 的逻辑会稍微复杂一些，我可以用这样一段伪代码来描述：\n/* job-wq-2的伪代码 */ for !queue.IsEmpty($BROKER_URL, $QUEUE) { task := queue.Pop() process(task) } print(\u0026quot;Queue empty, exiting\u0026quot;) exit  由于任务数目的总数不固定，所以每一个 Pod 必须能够知道，自己什么时候可以退出。比如，在这个例子中，我简单地以“队列为空”，作为任务全部完成的标志。所以说，这种用法，对应的是“任务总数不固定”的场景。\n不过，在实际的应用中，你需要处理的条件往往会非常复杂。比如，任务完成后的输出、每个任务 Pod 之间是不是有资源的竞争和协同等等。\n所以，在今天这篇文章中，我就不再展开 Job 的用法了。因为，在实际场景里，要么干脆就用第一种用法来自己管理作业；要么，这些任务 Pod 之间的关系就不那么“单纯”，甚至还是“有状态应用”（比如，任务的输入 / 输出是在持久化数据卷里）。在这种情况下，我在后面要重点讲解的 Operator，加上 Job 对象一起，可能才能更好地满足实际离线任务的编排需求。\n最后，我再来和你分享一个非常有用的 Job 对象，叫作：CronJob。\n顾名思义，CronJob 描述的，正是定时任务。它的 API 对象，如下所示：\napiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \u0026quot;*/1 * * * *\u0026quot; jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure  在这个 YAML 文件中，最重要的关键词就是 jobTemplate。看到它，你一定恍然大悟，原来 CronJob 是一个 Job 对象的控制器（Controller）！\n没错，CronJob 与 Job 的关系，正如同 Deployment 与 ReplicaSet 的关系一样。CronJob 是一个专门用来管理 Job 对象的控制器。只不过，它创建和删除 Job 的依据，是 schedule 字段定义的、一个标准的Unix Cron格式的表达式。\n比如，\u0026quot;*/1 * * * *\u0026quot;。这个 Cron 表达式里 */1 中的 * 表示从 0 开始，/ 表示“每”，1 表示偏移量。所以，它的意思就是：从 0 开始，每 1 个时间单位执行一次。那么，时间单位又是什么呢？Cron 表达式中的五个部分分别代表：分钟、小时、日、月、星期。所以，上面这句 Cron 表达式的意思是：从当前开始，每分钟执行一次。而这里要执行的内容，就是 jobTemplate 定义的 Job 了。\n所以，这个 CronJob 对象在创建 1 分钟后，就会有一个 Job 产生了，如下所示：\n$ kubectl create -f ./cronjob.yaml cronjob \u0026quot;hello\u0026quot; created # 一分钟后 $ kubectl get jobs NAME DESIRED SUCCESSFUL AGE hello-4111706356 1 1 2s  此时，CronJob 对象会记录下这次 Job 执行的时间：\n$ kubectl get cronjob hello NAME SCHEDULE SUSPEND ACTIVE LAST-SCHEDULE hello */1 * * * * False 0 Thu, 6 Sep 2018 14:34:00 -070  需要注意的是，由于定时任务的特殊性，很可能某个 Job 还没有执行完，另外一个新 Job 就产生了。这时候，你可以通过 spec.concurrencyPolicy 字段来定义具体的处理策略。比如：\n concurrencyPolicy=Allow，这也是默认情况，这意味着这些 Job 可以同时存在； concurrencyPolicy=Forbid，这意味着不会创建新的 Pod，该创建周期被跳过； concurrencyPolicy=Replace，这意味着新产生的 Job 会替换旧的、没有执行完的 Job。  而如果某一次 Job 创建失败，这次创建就会被标记为“miss”。当在指定的时间窗口内，miss 的数目达到 100 时，那么 CronJob 会停止再创建这个 Job。\n这个时间窗口，可以由 spec.startingDeadlineSeconds 字段指定。比如 startingDeadlineSeconds=200，意味着在过去 200 s 里，如果 miss 的数目达到了 100 次，那么这个 Job 就不会被创建执行了。\n总结\n在今天这篇文章中，我主要和你分享了 Job 这个离线业务的编排方法，讲解了 completions 和 parallelism 字段的含义，以及 Job Controller 的执行原理。\n紧接着，我通过实例和你分享了 Job 对象三种常见的使用方法。但是，根据我在社区和生产环境中的经验，大多数情况下用户还是更倾向于自己控制 Job 对象。所以，相比于这些固定的“模式”，掌握 Job 的 API 对象，和它各个字段的准确含义会更加重要。\n最后，我还介绍了一种 Job 的控制器，叫作：CronJob。这也印证了我在前面的分享中所说的：用一个对象控制另一个对象，是 Kubernetes 编排的精髓所在。\n","date":"2021年03月09日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter3/job/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文转自张磊老师的《深入剖析Kubernetes》课程.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"撬动离线业务：Job与CronJob"},{"contents":" 本文转自张磊老师的《深入剖析 Kubernetes》课程，本节内容主要介绍 StatefulSet 的存储状态\n 而在今天这篇文章中，我将继续为你解读 StatefulSet 对存储状态的管理机制。这个机制，主要使用的是一个叫作 Persistent Volume Claim 的功能。\n在前面介绍 Pod 的时候，我曾提到过，要在一个 Pod 里声明 Volume，只要在 Pod 里加上 spec.volumes 字段即可。然后，你就可以在这个字段里定义一个具体类型的 Volume 了，比如：hostPath。\n可是，你有没有想过这样一个场景：如果你并不知道有哪些 Volume 类型可以用，要怎么办呢？\n更具体地说，作为一个应用开发者，我可能对持久化存储项目（比如 Ceph、GlusterFS 等）一窍不通，也不知道公司的 Kubernetes 集群里到底是怎么搭建出来的，我也自然不会编写它们对应的 Volume 定义文件。所谓“术业有专攻”，这些关于 Volume 的管理和远程持久化存储的知识，不仅超越了开发者的知识储备，还会有暴露公司基础设施秘密的风险。\n比如，下面这个例子，就是一个声明了 Ceph RBD 类型 Volume 的 Pod：\napiVersion: v1 kind: Pod metadata: name: rbd spec: containers: - image: kubernetes/pause name: rbd-rw volumeMounts: - name: rbdpd mountPath: /mnt/rbd volumes: - name: rbdpd rbd: monitors: - '10.16.154.78:6789' - '10.16.154.82:6789' - '10.16.154.83:6789' pool: kube image: foo fsType: ext4 readOnly: true user: admin keyring: /etc/ceph/keyring imageformat: \u0026quot;2\u0026quot; imagefeatures: \u0026quot;layering\u0026quot;  其一，如果不懂得 Ceph RBD 的使用方法，那么这个 Pod 里 Volumes 字段，你十有八九也完全看不懂。其二，这个 Ceph RBD 对应的存储服务器的地址、用户名、授权文件的位置，也都被轻易地暴露给了全公司的所有开发人员，这是一个典型的信息被“过度暴露”的例子。\n这也是为什么，在后来的演化中，Kubernetes 项目引入了一组叫作 Persistent Volume Claim（PVC）和 Persistent Volume（PV）的 API 对象，大大降低了用户声明和使用持久化 Volume 的门槛。\n举个例子，有了 PVC 之后，一个开发人员想要使用一个 Volume，只需要简单的两步即可。\n第一步：定义一个 PVC，声明想要的 Volume 的属性：\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pv-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi  可以看到，在这个 PVC 对象里，不需要任何关于 Volume 细节的字段，只有描述性的属性和定义。比如，storage: 1Gi，表示我想要的 Volume 大小至少是 1 GiB；accessModes: ReadWriteOnce，表示这个 Volume 的挂载方式是可读写，并且只能被挂载在一个节点上而非被多个节点共享。\n 备注：关于哪种类型的 Volume 支持哪种类型的 AccessMode，你可以查看 Kubernetes 项目官方文档中的详细列表。\n 第二步：在应用的 Pod 中，声明使用这个 PVC：\napiVersion: v1 kind: Pod metadata: name: pv-pod spec: containers: - name: pv-container image: nginx ports: - containerPort: 80 name: \u0026quot;http-server\u0026quot; volumeMounts: - mountPath: \u0026quot;/usr/share/nginx/html\u0026quot; name: pv-storage volumes: - name: pv-storage persistentVolumeClaim: claimName: pv-claim  可以看到，在这个 Pod 的 Volumes 定义中，我们只需要声明它的类型是 persistentVolumeClaim，然后指定 PVC 的名字，而完全不必关心 Volume 本身的定义。这时候，只要我们创建这个 PVC 对象，Kubernetes 就会自动为它绑定一个符合条件的 Volume。可是，这些符合条件的 Volume 又是从哪里来的呢？答案是，它们来自于由运维人员维护的 PV（Persistent Volume）对象。接下来，我们一起看一个常见的 PV 对象的 YAML 文件：\nkind: PersistentVolume apiVersion: v1 metadata: name: pv-volume labels: type: local spec: capacity: storage: 10Gi accessModes: - ReadWriteOnce rbd: monitors: # 使用 kubectl get pods -n rook-ceph 查看 rook-ceph-mon- 开头的 POD IP 即可得下面的列表 - '10.16.154.78:6789' - '10.16.154.82:6789' - '10.16.154.83:6789' pool: kube image: foo fsType: ext4 readOnly: true user: admin keyring: /etc/ceph/keyring  可以看到，这个 PV 对象的 spec.rbd 字段，正是我们前面介绍过的 Ceph RBD Volume 的详细定义。而且，它还声明了这个 PV 的容量是 10 GiB。这样，Kubernetes 就会为我们刚刚创建的 PVC 对象绑定这个 PV。\n所以，Kubernetes 中 PVC 和 PV 的设计，实际上类似于“接口”和“实现”的思想。开发者只要知道并会使用“接口”，即：PVC；而运维人员则负责给“接口”绑定具体的实现，即：PV。\n这种解耦，就避免了因为向开发者暴露过多的存储系统细节而带来的隐患。此外，这种职责的分离，往往也意味着出现事故时可以更容易定位问题和明确责任，从而避免“扯皮”现象的出现。而 PVC、PV 的设计，也使得 StatefulSet 对存储状态的管理成为了可能。\napiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026quot;nginx\u0026quot; replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.9.1 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi  这次，我们为这个 StatefulSet 额外添加了一个 volumeClaimTemplates 字段。从名字就可以看出来，它跟 Deployment 里 Pod 模板（PodTemplate）的作用类似。也就是说，凡是被这个 StatefulSet 管理的 Pod，都会声明一个对应的 PVC；而这个 PVC 的定义，就来自于 volumeClaimTemplates 这个模板字段。更重要的是，这个 PVC 的名字，会被分配一个与这个 Pod 完全一致的编号。\n这个自动创建的 PVC，与 PV 绑定成功后，就会进入 Bound 状态，这就意味着这个 Pod 可以挂载并使用这个 PV 了。如果你还是不太理解 PVC 的话，可以先记住这样一个结论：PVC 其实就是一种特殊的 Volume。只不过一个 PVC 具体是什么类型的 Volume，要在跟某个 PV 绑定之后才知道。\n关于 PV、PVC 更详细的知识，我会在容器存储部分做进一步解读。当然，PVC 与 PV 的绑定得以实现的前提是，运维人员已经在系统里创建好了符合条件的 PV（比如，我们在前面用到的 pv-volume）；或者，你的 Kubernetes 集群运行在公有云上，这样 Kubernetes 就会通过 Dynamic Provisioning 的方式，自动为你创建与 PVC 匹配的 PV。\n所以，我们在使用 kubectl create 创建了 StatefulSet 之后，就会看到 Kubernetes 集群里出现了两个 PVC：\n$ kubectl create -f statefulset.yaml $ kubectl get pvc -l app=nginx NAME STATUS VOLUME CAPACITY ACCESSMODES AGE www-web-0 Bound pvc-15c268c7-b507-11e6-932f-42010a800002 1Gi RWO 48s www-web-1 Bound pvc-15c79307-b507-11e6-932f-42010a800002 1Gi RWO 48s  可以看到，这些 PVC，都以“\u0026lt;PVC 名字 \u0026gt;-\u0026lt;StatefulSet 名字 \u0026gt;-\u0026lt; 编号 \u0026gt;”的方式命名，并且处于 Bound 状态。\n我们前面已经讲到过，这个 StatefulSet 创建出来的所有 Pod，都会声明使用编号的 PVC。比如，在名叫 web-0 的 Pod 的 volumes 字段，它会声明使用名叫 www-web-0 的 PVC，从而挂载到这个 PVC 所绑定的 PV。所以，我们就可以使用如下所示的指令，在 Pod 的 Volume 目录里写入一个文件，来验证一下上述 Volume 的分配情况：\n$ for i in 0 1; do kubectl exec web-$i -- sh -c 'echo hello $(hostname) \u0026gt; /usr/share/nginx/html/admin.html'; done  如上所示，通过 kubectl exec 指令，我们在每个 Pod 的 Volume 目录里，写入了一个 index.html 文件。这个文件的内容，正是 Pod 的 hostname。比如，我们在 web-0 的 index.html 里写入的内容就是\u0026quot;hello web-0\u0026quot;。\n此时，如果你在这个 Pod 容器里访问“http://localhost”，你实际访问到的就是 Pod 里 Nginx 服务器进程，而它会为你返回 /usr/share/nginx/html/index.html 里的内容。这个操作的执行方法如下所示：\n$ for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done hello web-0 hello web-1  现在，关键来了。如果你使用 kubectl delete 命令删除这两个 Pod，这些 Volume 里的文件会不会丢失呢？\n$ kubectl delete pod -l app=nginx pod \u0026quot;web-0\u0026quot; deleted pod \u0026quot;web-1\u0026quot; deleted  可以看到，正如我们前面介绍过的，在被删除之后，这两个 Pod 会被按照编号的顺序被重新创建出来。而这时候，如果你在新创建的容器里通过访问“http://localhost”的方式去访问 web-0 里的 Nginx 服务：\n# 在被重新创建出来的Pod容器里访问http://localhost $ kubectl exec -it web-0 -- curl localhost hello web-0  这是怎么做到的呢？\n其实，我和你分析一下 StatefulSet 控制器恢复这个 Pod 的过程，你就可以很容易理解了。首先，当你把一个 Pod，比如 web-0，删除之后，这个 Pod 对应的 PVC 和 PV，并不会被删除，而这个 Volume 里已经写入的数据，也依然会保存在远程存储服务里（比如，我们在这个例子里用到的 Ceph 服务器）。此时，StatefulSet 控制器发现，一个名叫 web-0 的 Pod 消失了。所以，控制器就会重新创建一个新的、名字还是叫作 web-0 的 Pod 来，“纠正”这个不一致的情况。需要注意的是，在这个新的 Pod 对象的定义里，它声明使用的 PVC 的名字，还是叫作：www-web-0。这个 PVC 的定义，还是来自于 PVC 模板（volumeClaimTemplates），这是 StatefulSet 创建 Pod 的标准流程。所以，在这个新的 web-0 Pod 被创建出来之后，Kubernetes 为它查找名叫 www-web-0 的 PVC 时，就会直接找到旧 Pod 遗留下来的同名的 PVC，进而找到跟这个 PVC 绑定在一起的 PV。这样，新的 Pod 就可以挂载到旧 Pod 对应的那个 Volume，并且获取到保存在 Volume 里的数据。\n通过这种方式，Kubernetes 的 StatefulSet 就实现了对应用存储状态的管理。\n看到这里，你是不是已经大致理解了 StatefulSet 的工作原理呢？现在，我再为你详细梳理一下吧。\n首先，StatefulSet 的控制器直接管理的是 Pod。这是因为，StatefulSet 里的不同 Pod 实例，不再像 ReplicaSet 中那样都是完全一样的，而是有了细微区别的。比如，每个 Pod 的 hostname、名字等都是不同的、携带了编号的。而 StatefulSet 区分这些实例的方式，就是通过在 Pod 的名字里加上事先约定好的编号。\n其次，Kubernetes 通过 Headless Service，为这些有编号的 Pod，在 DNS 服务器中生成带有同样编号的 DNS 记录。只要 StatefulSet 能够保证这些 Pod 名字里的编号不变，那么 Service 里类似于 web-0.nginx.default.svc.cluster.local 这样的 DNS 记录也就不会变，而这条记录解析出来的 Pod 的 IP 地址，则会随着后端 Pod 的删除和再创建而自动更新。这当然是 Service 机制本身的能力，不需要 StatefulSet 操心。\n最后，StatefulSet 还为每一个 Pod 分配并创建一个同样编号的 PVC。这样，Kubernetes 就可以通过 Persistent Volume 机制为这个 PVC 绑定上对应的 PV，从而保证了每一个 Pod 都拥有一个独立的 Volume。\n在这种情况下，即使 Pod 被删除，它所对应的 PVC 和 PV 依然会保留下来。所以当这个 Pod 被重新创建出来之后，Kubernetes 会为它找到同样编号的 PVC，挂载这个 PVC 对应的 Volume，从而获取到以前保存在 Volume 里的数据。\n这么一看，原本非常复杂的 StatefulSet，是不是也很容易理解了呢？\n总结\n在今天这篇文章中，我为你详细介绍了 StatefulSet 处理存储状态的方法。然后，以此为基础，我为你梳理了 StatefulSet 控制器的工作原理。\n从这些讲述中，我们不难看出 StatefulSet 的设计思想：StatefulSet 其实就是一种特殊的 Deployment，而其独特之处在于，它的每个 Pod 都被编号了。而且，这个编号会体现在 Pod 的名字和 hostname 等标识信息上，这不仅代表了 Pod 的创建顺序，也是 Pod 的重要网络标识（即：在整个集群里唯一的、可被访问的身份）。\n有了这个编号后，StatefulSet 就使用 Kubernetes 里的两个标准功能：Headless Service 和 PV/PVC，实现了对 Pod 的拓扑状态和存储状态的维护。\n实际上，在下一篇文章的“有状态应用”实践环节，以及后续的讲解中，你就会逐渐意识到，StatefulSet 可以说是 Kubernetes 中作业编排的“集大成者”。\n因为，几乎每一种 Kubernetes 的编排功能，都可以在编写 StatefulSet 的 YAML 文件时被用到。\n","date":"2021年03月09日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter3/statefulset-2/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文转自张磊老师的《深入剖析 Kubernetes》课程，本节内容主要介绍 StatefulSet 的存储状态\u003c/p\u003e\n\u003c/blockquote\u003e","title":"深入理解StatefulSet（二）：存储状态"},{"contents":" 本文转自张磊老师的《深入剖析 Kubernetes》课程，本节内容主要介绍 Daemonset\n 顾名思义，DaemonSet 的主要作用，是让你在 Kubernetes 集群里，运行一个 Daemon Pod。 所以，这个 Pod 有如下三个特征：\n 这个 Pod 运行在 Kubernetes 集群里的每一个节点（Node）上； 每个节点上只有一个这样的 Pod 实例； 当有新的节点加入 Kubernetes 集群后，该 Pod 会自动地在新节点上被创建出来；而当旧节点被删除后，它上面的 Pod 也相应地会被回收掉。  这个机制听起来很简单，但 Daemon Pod 的意义确实是非常重要的。我随便给你列举几个例子：\n 各种网络插件的 Agent 组件，都必须运行在每一个节点上，用来处理这个节点上的容器网络； 各种存储插件的 Agent 组件，也必须运行在每一个节点上，用来在这个节点上挂载远程存储目录，操作容器的 Volume 目录； 各种监控组件和日志组件，也必须运行在每一个节点上，负责这个节点上的监控信息和日志搜集。更重要的是，跟其他编排对象不一样，DaemonSet 开始运行的时机，很多时候比整个 Kubernetes 集群出现的时机都要早。  更重要的是，跟其他编排对象不一样，DaemonSet 开始运行的时机，很多时候比整个 Kubernetes 集群出现的时机都要早。这个乍一听起来可能有点儿奇怪。但其实你来想一下：如果这个 DaemonSet 正是一个网络插件的 Agent 组件呢？这个时候，整个 Kubernetes 集群里还没有可用的容器网络，所有 Worker 节点的状态都是 NotReady（NetworkReady=false）。这种情况下，普通的 Pod 肯定不能运行在这个集群上。所以，这也就意味着 DaemonSet 的设计，必须要有某种“过人之处”才行。为了弄清楚 DaemonSet 的工作原理，我们还是按照老规矩，先从它的 API 对象的定义说起。\napiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: k8s.gcr.io/fluentd-elasticsearch:1.20 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers  这个 DaemonSet，管理的是一个 fluentd-elasticsearch 镜像的 Pod。这个镜像的功能非常实用：通过 fluentd 将 Docker 容器里的日志转发到 ElasticSearch 中。可以看到，DaemonSet 跟 Deployment 其实非常相似，只不过是没有 replicas 字段；它也使用 selector 选择管理所有携带了 name=fluentd-elasticsearch 标签的 Pod。而这些 Pod 的模板，也是用 template 字段定义的。在这个字段中，我们定义了一个使用 fluentd-elasticsearch:1.20 镜像的容器，而且这个容器挂载了两个 hostPath 类型的 Volume，分别对应宿主机的 /var/log 目录和 /var/lib/docker/containers 目录。显然，fluentd 启动之后，它会从这两个目录里搜集日志信息，并转发给 ElasticSearch 保存。这样，我们通过 ElasticSearch 就可以很方便地检索这些日志了。需要注意的是，Docker 容器里应用的日志，默认会保存在宿主机的 /var/lib/docker/containers/{{. 容器 ID}}/{{. 容器 ID}}-json.log 文件里，所以这个目录正是 fluentd 的搜集目标。\n那么，DaemonSet 又是如何保证每个 Node 上有且只有一个被管理的 Pod 呢？\n显然，这是一个典型的“控制器模型”能够处理的问题。\nDaemonSet Controller，首先从 Etcd 里获取所有的 Node 列表，然后遍历所有的 Node。这时，它就可以很容易地去检查，当前这个 Node 上是不是有一个携带了 name=fluentd-elasticsearch 标签的 Pod 在运行。而检查的结果，可能有这么三种情况：\n 没有这种 Pod，那么就意味着要在这个 Node 上创建这样一个 Pod； 有这种 Pod，但是数量大于 1，那就说明要把多余的 Pod 从这个 Node 上删除掉； 正好只有一个这种 Pod，那说明这个节点是正常的。  其中，删除节点（Node）上多余的 Pod 非常简单，直接调用 Kubernetes API 就可以了。但是，如何在指定的 Node 上创建新 Pod 呢？\n如果你已经熟悉了 Pod API 对象的话，那一定可以立刻说出答案：用 nodeSelector，选择 Node 的名字即可。\nnodeSelector: name: \u0026lt;Node名字\u0026gt;  没错。不过，在 Kubernetes 项目里，nodeSelector 其实已经是一个将要被废弃的字段了。因为，现在有了一个新的、功能更完善的字段可以代替它，即：nodeAffinity。我来举个例子：\napiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: #必须在每次调度的时候予以考虑 requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: #只允许运行在 metadata.name 是 node-geektime 的节点上 - matchExpressions: - key: metadata.name operator: In values: - node-geektime  在这个 Pod 里，我声明了一个 spec.affinity 字段，然后定义了一个 nodeAffinity。其中，spec.affinity 字段，是 Pod 里跟调度相关的一个字段。关于它的完整内容，我会在讲解调度策略的时候再详细阐述。\n而在这里，我定义的 nodeAffinity 的含义是：requiredDuringSchedulingIgnoredDuringExecution：它的意思是说，这个 nodeAffinity 必须在每次调度的时候予以考虑。同时，这也意味着你可以设置在某些情况下不考虑这个 nodeAffinity；这个 Pod，将来只允许运行在“metadata.name”是“node-geektime”的节点上。\n在这里，你应该注意到 nodeAffinity 的定义，可以支持更加丰富的语法，比如 operator: In（即：部分匹配；如果你定义 operator: Equal，就是完全匹配），这也正是 nodeAffinity 会取代 nodeSelector 的原因之一。\n 备注：其实在大多数时候，这些 Operator 语义没啥用处。所以说，在学习开源项目的时候，一定要学会抓住“主线”。不要顾此失彼。\n 所以，**我们的 DaemonSet Controller 会在创建 Pod 的时候，自动在这个 Pod 的 API 对象里，加上这样一个 nodeAffinity 定义。**其中，需要绑定的节点名字，正是当前正在遍历的这个 Node。\n当然，DaemonSet 并不需要修改用户提交的 YAML 文件里的 Pod 模板，而是在向 Kubernetes 发起请求之前，直接修改根据模板生成的 Pod 对象。这个思路，也正是我在前面讲解 Pod 对象时介绍过的。\n此外，DaemonSet 还会给这个 Pod 自动加上另外一个与调度相关的字段，叫作 tolerations。这个字段意味着这个 Pod，会“容忍”（Toleration）某些 Node 的“污点”（Taint）。而 DaemonSet 自动加上的 tolerations 字段，格式如下所示：\napiVersion: v1 kind: Pod metadata: name: with-toleration spec: tolerations: - key: node.kubernetes.io/unschedulable operator: Exists effect: NoSchedule  这个 Toleration 的含义是：“容忍”所有被标记为 unschedulable“污点”的 Node；“容忍”的效果是允许调度。\n 备注：关于如何给一个 Node 标记上“污点”，以及这里具体的语法定义，我会在后面介绍调度器的时候做详细介绍。这里，你可以简单地把“污点”理解为一种特殊的 Label。\n 而在正常情况下，被标记了 unschedulable“污点”的 Node，是不会有任何 Pod 被调度上去的（effect: NoSchedule）。可是，DaemonSet 自动地给被管理的 Pod 加上了这个特殊的 Toleration，就使得这些 Pod 可以忽略这个限制，继而保证每个节点上都会被调度一个 Pod。当然，如果这个节点有故障的话，这个 Pod 可能会启动失败，而 DaemonSet 则会始终尝试下去，直到 Pod 启动成功。\n这时，你应该可以猜到，我在前面介绍到的 DaemonSet 的“过人之处”，其实就是依靠 Toleration 实现的。\n假如当前 DaemonSet 管理的，是一个网络插件的 Agent Pod，那么你就必须在这个 DaemonSet 的 YAML 文件里，给它的 Pod 模板加上一个能够“容忍”node.kubernetes.io/network-unavailable“污点”的 Toleration。正如下面这个例子所示：\n... template: metadata: labels: name: network-plugin-agent spec: tolerations: - key: node.kubernetes.io/network-unavailable operator: Exists effect: NoSchedule  在 Kubernetes 项目中，当一个节点的网络插件尚未安装时，这个节点就会被自动加上名为node.kubernetes.io/network-unavailable的“污点”。\n而通过这样一个 Toleration，调度器在调度这个 Pod 的时候，就会忽略当前节点上的“污点”，从而成功地将网络插件的 Agent 组件调度到这台机器上启动起来。\n这种机制，正是我们在部署 Kubernetes 集群的时候，能够先部署 Kubernetes 本身、再部署网络插件的根本原因：因为当时我们所创建的 Weave 的 YAML，实际上就是一个 DaemonSet。\n至此，通过上面这些内容，你应该能够明白，DaemonSet 其实是一个非常简单的控制器。在它的控制循环中，只需要遍历所有节点，然后根据节点上是否有被管理 Pod 的情况，来决定是否要创建或者删除一个 Pod。\n只不过，在创建每个 Pod 的时候，DaemonSet 会自动给这个 Pod 加上一个 nodeAffinity，从而保证这个 Pod 只会在指定节点上启动。同时，它还会自动给这个 Pod 加上一个 Toleration，从而忽略节点的 unschedulable“污点”。\n当然，你也可以在 Pod 模板里加上更多种类的 Toleration，从而利用 DaemonSet 达到自己的目的。比如，在这个 fluentd-elasticsearch DaemonSet 里，我就给它加上了这样的 Toleration：\ntolerations: - key: node-role.kubernetes.io/master effect: NoSchedule  这是因为在默认情况下，Kubernetes 集群不允许用户在 Master 节点部署 Pod。因为，Master 节点默认携带了一个叫作node-role.kubernetes.io/master的“污点”。所以，为了能在 Master 节点上部署 DaemonSet 的 Pod，我就必须让这个 Pod“容忍”这个“污点”。在理解了 DaemonSet 的工作原理之后，接下来我就通过一个具体的实践来帮你更深入地掌握 DaemonSet 的使用方法。\n首先，创建这个 DaemonSet 对象：\n$ kubectl create -f fluentd-elasticsearch.yaml  需要注意的是，在 DaemonSet 上，我们一般都应该加上 resources 字段，来限制它的 CPU 和内存使用，防止它占用过多的宿主机资源。而创建成功后，你就能看到，如果有 N 个节点，就会有 N 个 fluentd-elasticsearch Pod 在运行。比如在我们的例子里，会有两个 Pod，如下所示：\n$ kubectl get pod -n kube-system -l name=fluentd-elasticsearch NAME READY STATUS RESTARTS AGE fluentd-elasticsearch-dqfv9 1/1 Running 0 53m fluentd-elasticsearch-pf9z5 1/1 Running 0 53m  而如果你此时通过 kubectl get 查看一下 Kubernetes 集群里的 DaemonSet 对象：\n$ kubectl get ds -n kube-system fluentd-elasticsearch NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE fluentd-elasticsearch 2 2 2 2 2 \u0026lt;none\u0026gt; 1h   备注：Kubernetes 里比较长的 API 对象都有短名字，比如 DaemonSet 对应的是 ds，Deployment 对应的是 deploy。\n 就会发现 DaemonSet 和 Deployment 一样，也有 DESIRED、CURRENT 等多个状态字段。这也就意味着，DaemonSet 可以像 Deployment 那样，进行版本管理。这个版本，可以使用 kubectl rollout history 看到：\n$ kubectl rollout history daemonset fluentd-elasticsearch -n kube-system daemonsets \u0026quot;fluentd-elasticsearch\u0026quot; REVISION CHANGE-CAUSE 1 \u0026lt;none\u0026gt;  接下来，我们来把这个 DaemonSet 的容器镜像版本到 v2.2.0：\n$ kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --record -n=kube-system  这个 kubectl set image 命令里，第一个 fluentd-elasticsearch 是 DaemonSet 的名字，第二个 fluentd-elasticsearch 是容器的名字。\n这时候，我们可以使用 kubectl rollout status 命令看到这个“滚动更新”的过程，如下所示：\n$ kubectl rollout status ds/fluentd-elasticsearch -n kube-system Waiting for daemon set \u0026quot;fluentd-elasticsearch\u0026quot; rollout to finish: 0 out of 2 new pods have been updated... Waiting for daemon set \u0026quot;fluentd-elasticsearch\u0026quot; rollout to finish: 0 out of 2 new pods have been updated... Waiting for daemon set \u0026quot;fluentd-elasticsearch\u0026quot; rollout to finish: 1 of 2 updated pods are available... daemon set \u0026quot;fluentd-elasticsearch\u0026quot; successfully rolled out  注意，由于这一次我在升级命令后面加上了–record 参数，所以这次升级使用到的指令就会自动出现在 DaemonSet 的 rollout history 里面，如下所示：\n$ kubectl rollout history daemonset fluentd-elasticsearch -n kube-system daemonsets \u0026quot;fluentd-elasticsearch\u0026quot; REVISION CHANGE-CAUSE 1 \u0026lt;none\u0026gt; 2 kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --namespace=kube-system --record=true  有了版本号，你也就可以像 Deployment 一样，将 DaemonSet 回滚到某个指定的历史版本了\n而我在前面的文章中讲解 Deployment 对象的时候，曾经提到过，Deployment 管理这些版本，靠的是“一个版本对应一个 ReplicaSet 对象”。可是，DaemonSet 控制器操作的直接就是 Pod，不可能有 ReplicaSet 这样的对象参与其中。那么，它的这些版本又是如何维护的呢？\n所谓，一切皆对象！在 Kubernetes 项目中，任何你觉得需要记录下来的状态，都可以被用 API 对象的方式实现。当然，“版本”也不例外。Kubernetes v1.7 之后添加了一个 API 对象，名叫 ControllerRevision，专门用来记录某种 Controller 对象的版本。\n比如，你可以通过如下命令查看 fluentd-elasticsearch 对应的 ControllerRevision：\n$ kubectl get controllerrevision -n kube-system -l name=fluentd-elasticsearch NAME CONTROLLER REVISION AGE fluentd-elasticsearch-64dc6799c9 daemonset.apps/fluentd-elasticsearch 2 1h  而如果你使用 kubectl describe 查看这个 ControllerRevision 对象：\n$ kubectl describe controllerrevision fluentd-elasticsearch-64dc6799c9 -n kube-system Name: fluentd-elasticsearch-64dc6799c9 Namespace: kube-system Labels: controller-revision-hash=2087235575 name=fluentd-elasticsearch Annotations: deprecated.daemonset.template.generation=2 kubernetes.io/change-cause=kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --record=true --namespace=kube-system API Version: apps/v1 Data: Spec: Template: $ Patch: replace Metadata: Creation Timestamp: \u0026lt;nil\u0026gt; Labels: Name: fluentd-elasticsearch Spec: Containers: Image: k8s.gcr.io/fluentd-elasticsearch:v2.2.0 Image Pull Policy: IfNotPresent Name: fluentd-elasticsearch ... Revision: 2 Events: \u0026lt;none\u0026gt;  就会看到，这个 ControllerRevision 对象，实际上是在 Data 字段保存了该版本对应的完整的 DaemonSet 的 API 对象。并且，在 Annotation 字段保存了创建这个对象所使用的 kubectl 命令。\n接下来，我们可以尝试将这个 DaemonSet 回滚到 Revision=1 时的状态：\n$ kubectl rollout undo daemonset fluentd-elasticsearch --to-revision=1 -n kube-system daemonset.extensions/fluentd-elasticsearch rolled back  这个 kubectl rollout undo 操作，实际上相当于读取到了 Revision=1 的 ControllerRevision 对象保存的 Data 字段。而这个 Data 字段里保存的信息，就是 Revision=1 时这个 DaemonSet 的完整 API 对象。\n所以，现在 DaemonSet Controller 就可以使用这个历史 API 对象，对现有的 DaemonSet 做一次 PATCH 操作（等价于执行一次 kubectl apply -f “旧的 DaemonSet 对象”），从而把这个 DaemonSet“更新”到一个旧版本。\n这也是为什么，在执行完这次回滚完成后，你会发现，DaemonSet 的 Revision 并不会从 Revision=2 退回到 1，而是会增加成 Revision=3。这是因为，一个新的 ControllerRevision 被创建了出来。\n总结\n在今天这篇文章中，我首先简单介绍了 StatefulSet 的“滚动更新”，然后重点讲解了本专栏的第三个重要编排对象：DaemonSet。\n相比于 Deployment，DaemonSet 只管理 Pod 对象，然后通过 nodeAffinity 和 Toleration 这两个调度器的小功能，保证了每个节点上有且只有一个 Pod。这个控制器的实现原理简单易懂，希望你能够快速掌握。\n与此同时，DaemonSet 使用 ControllerRevision，来保存和管理自己对应的“版本”。这种“面向 API 对象”的设计思路，大大简化了控制器本身的逻辑，也正是 Kubernetes 项目“声明式 API”的优势所在。\n而且，相信聪明的你此时已经想到了，StatefulSet 也是直接控制 Pod 对象的，那么它是不是也在使用 ControllerRevision 进行版本管理呢？\n没错。在 Kubernetes 项目里，ControllerRevision 其实是一个通用的版本管理对象。这样，Kubernetes 项目就巧妙地避免了每种控制器都要维护一套冗余的代码和逻辑的问题。\n思考题\n我在文中提到，在 Kubernetes v1.11 之前，DaemonSet 所管理的 Pod 的调度过程，实际上都是由 DaemonSet Controller 自己而不是由调度器完成的。你能说出这其中有哪些原因吗？\n 查了一下v1.11 的 release notes。scheduler关于affinity谓词的性能大大提高了。\n查阅了Ds用默认调度器代替controller的设计文档 之前的做法是： controller判断调度谓词，符合的话直接在controller中直接设置spec.hostName去调度。 目前的做法是： controller不再判断调度条件，给每个pode设置NodeAffinity。控制器根据NodeAffinity去检查每个node上是否启动了相应的Pod。并且可以利用调度优先级去优先调度关键的ds pods。\n","date":"2021年03月09日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter3/daemonset/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文转自张磊老师的《深入剖析 Kubernetes》课程，本节内容主要介绍 Daemonset\u003c/p\u003e\n\u003c/blockquote\u003e","title":"容器化守护进程的意义：DaemonSet"},{"contents":" 本文转自张磊老师的《深入剖析 Kubernetes》课程，本节内容主要介绍 StatefulSet 的拓扑状态\n 在上一篇文章中，我在结尾处讨论到了 Deployment 实际上并不足以覆盖所有的应用编排问题。\n造成这个问题的根本原因，在于 Deployment 对应用做了一个简单化假设。它认为，一个应用的所有 Pod，是完全一样的。所以，它们互相之间没有顺序，也无所谓运行在哪台宿主机上。需要的时候，Deployment 就可以通过 Pod 模板创建新的 Pod；不需要的时候，Deployment 就可以“杀掉”任意一个 Pod。\n但是，在实际的场景中，并不是所有的应用都可以满足这样的要求。\n尤其是分布式应用，它的多个实例之间，往往有依赖关系，比如：主从关系、主备关系。\n还有就是数据存储类应用，它的多个实例，往往都会在本地磁盘上保存一份数据。而这些实例一旦被杀掉，即便重建出来，实例与数据之间的对应关系也已经丢失，从而导致应用失败。\n所以，这种实例之间有不对等关系，以及实例对外部数据有依赖关系的应用，就被称为“有状态应用”（Stateful Application）。\n容器技术诞生后，大家很快发现，它用来封装“无状态应用”（Stateless Application），尤其是 Web 服务，非常好用。但是，一旦你想要用容器运行“有状态应用”，其困难程度就会直线上升。而且，这个问题解决起来，单纯依靠容器技术本身已经无能为力，这也就导致了很长一段时间内，“有状态应用”几乎成了容器技术圈子的“忌讳”，大家一听到这个词，就纷纷摇头。\n不过，Kubernetes 项目还是成为了“第一个吃螃蟹的人”。得益于“控制器模式”的设计思想，Kubernetes 项目很早就在 Deployment 的基础上，扩展出了对“有状态应用”的初步支持。这个编排功能，就是：StatefulSet。\nStatefulSet 的设计其实非常容易理解。它把真实世界里的应用状态，抽象为了两种情况：\n **拓扑状态。**这种情况意味着，应用的多个实例之间不是完全对等的关系。这些应用实例，必须按照某些顺序启动，比如应用的主节点 A 要先于从节点 B 启动。而如果你把 A 和 B 两个 Pod 删除掉，它们再次被创建出来时也必须严格按照这个顺序才行。并且，新创建出来的 Pod，必须和原来 Pod 的网络标识一样，这样原先的访问者才能使用同样的方法，访问到这个新 Pod。 **存储状态。**这种情况意味着，应用的多个实例分别绑定了不同的存储数据。对于这些应用实例来说，Pod A 第一次读取到的数据，和隔了十分钟之后再次读取到的数据，应该是同一份，哪怕在此期间 Pod A 被重新创建过。这种情况最典型的例子，就是一个数据库应用的多个存储实例。  所以，StatefulSet 的核心功能，就是通过某种方式记录这些状态，然后在 Pod 被重新创建时，能够为新 Pod 恢复这些状态。\n在开始讲述 StatefulSet 的工作原理之前，我就必须先为你讲解一个 Kubernetes 项目中非常实用的概念：Headless Service。\n我在和你一起讨论 Kubernetes 架构的时候就曾介绍过，Service 是 Kubernetes 项目中用来将一组 Pod 暴露给外界访问的一种机制。比如，一个 Deployment 有 3 个 Pod，那么我就可以定义一个 Service。然后，用户只要能访问到这个 Service，它就能访问到某个具体的 Pod。那么，这个 Service 又是如何被访问的呢？\n**第一种方式，是以 Service 的 VIP（Virtual IP，即：虚拟 IP）方式。**比如：当我访问 10.0.23.1 这个 Service 的 IP 地址时，10.0.23.1 其实就是一个 VIP，它会把请求转发到该 Service 所代理的某一个 Pod 上。这里的具体原理，我会在后续的 Service 章节中进行详细介绍。\n**第二种方式，就是以 Service 的 DNS 方式。**比如：这时候，只要我访问“my-svc.my-namespace.svc.cluster.local”这条 DNS 记录，就可以访问到名叫 my-svc 的 Service 所代理的某一个 Pod。\n而在第二种 Service DNS 的方式下，具体还可以分为两种处理方法：\n第一种处理方法，是 Normal Service。这种情况下，你访问“my-svc.my-namespace.svc.cluster.local”解析到的，正是 my-svc 这个 Service 的 VIP，后面的流程就跟 VIP 方式一致了。\n而第二种处理方法，正是 Headless Service。这种情况下，你访问“my-svc.my-namespace.svc.cluster.local”解析到的，直接就是 my-svc 代理的某一个 Pod 的 IP 地址。可以看到，这里的区别在于，Headless Service 不需要分配一个 VIP，而是可以直接以 DNS 记录的方式解析出被代理 Pod 的 IP 地址。\n那么，这样的设计又有什么作用呢？想要回答这个问题，我们需要从 Headless Service 的定义方式看起。\n下面是一个标准的 Headless Service 对应的 YAML 文件：\napiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx  可以看到，所谓的 Headless Service，其实仍是一个标准 Service 的 YAML 文件。只不过，它的 clusterIP 字段的值是：None，即：这个 Service，没有一个 VIP 作为“头”。这也就是 Headless 的含义。所以，这个 Service 被创建后并不会被分配一个 VIP，而是会以 DNS 记录的方式暴露出它所代理的 Pod。\n而它所代理的 Pod，依然是采用我在前面第 12 篇文章《牛刀小试：我的第一个容器化应用》中提到的 Label Selector 机制选择出来的，即：所有携带了 app=nginx 标签的 Pod，都会被这个 Service 代理起来。然后关键来了。当你按照这样的方式创建了一个 Headless Service 之后，它所代理的所有 Pod 的 IP 地址，都会被绑定一个这样格式的 DNS 记录，如下所示：\n\u0026lt;pod-name\u0026gt;.\u0026lt;svc-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local  这个 DNS 记录，正是 Kubernetes 项目为 Pod 分配的唯一的“可解析身份”（Resolvable Identity）。\n有了这个“可解析身份”，只要你知道了一个 Pod 的名字，以及它对应的 Service 的名字，你就可以非常确定地通过这条 DNS 记录访问到 Pod 的 IP 地址。\n那么，StatefulSet 又是如何使用这个 DNS 记录来维持 Pod 的拓扑状态的呢？为了回答这个问题，现在我们就来编写一个 StatefulSet 的 YAML 文件，如下所示：\napiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \u0026quot;nginx\u0026quot; replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.9.1 ports: - containerPort: 80 name: web  这个 YAML 文件，和我们在前面文章中用到的 nginx-deployment 的唯一区别，就是多了一个 serviceName=nginx 字段。这个字段的作用，就是告诉 StatefulSet 控制器，在执行控制循环（Control Loop）的时候，请使用 nginx 这个 Headless Service 来保证 Pod 的“可解析身份”。所以，当你通过 kubectl create 创建了上面这个 Service 和 StatefulSet 之后，就会看到如下两个对象：\n$ kubectl create -f svc.yaml $ kubectl get service nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP None \u0026lt;none\u0026gt; 80/TCP 10s $ kubectl create -f statefulset.yaml $ kubectl get statefulset web NAME DESIRED CURRENT AGE web 2 1 19s  这时候，如果你手比较快的话，还可以通过 kubectl 的 -w 参数，即：Watch 功能，实时查看 StatefulSet 创建两个有状态实例的过程：\n 备注：如果手不够快的话，Pod 很快就创建完了。不过，你依然可以通过这个 StatefulSet 的 Events 看到这些信息。\n $ kubectl get pods -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 19s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 20s  通过上面这个 Pod 的创建过程，我们不难看到，StatefulSet 给它所管理的所有 Pod 的名字，进行了编号，编号规则是：-。\n而且这些编号都是从 0 开始累加，与 StatefulSet 的每个 Pod 实例一一对应，绝不重复。\n更重要的是，这些 Pod 的创建，也是严格按照编号顺序进行的。比如，在 web-0 进入到 Running 状态、并且细分状态（Conditions）成为 Ready 之前，web-1 会一直处于 Pending 状态。\n当这两个 Pod 都进入了 Running 状态之后，你就可以查看到它们各自唯一的“网络身份”了。\n我们使用 kubectl exec 命令进入到容器中查看它们的 hostname：\n$ kubectl exec web-0 -- sh -c 'hostname' web-0 $ kubectl exec web-1 -- sh -c 'hostname' web-1  可以看到，这两个 Pod 的 hostname 与 Pod 名字是一致的，都被分配了对应的编号。接下来，我们再试着以 DNS 的方式，访问一下这个 Headless Service：\n$ kubectl run -i --tty --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh  通过这条命令，我们启动了一个一次性的 Pod，因为–rm 意味着 Pod 退出后就会被删除掉。然后，在这个 Pod 的容器里面，我们尝试用 nslookup 命令，解析一下 Pod 对应的 Headless Service：\n$ kubectl run -i --tty --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh $ nslookup web-0.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 10.244.1.7 $ nslookup web-1.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 10.244.2.7  从 nslookup 命令的输出结果中，我们可以看到，在访问 web-0.nginx 的时候，最后解析到的，正是 web-0 这个 Pod 的 IP 地址；而当访问 web-1.nginx 的时候，解析到的则是 web-1 的 IP 地址。\n这时候，如果你在另外一个 Terminal 里把这两个“有状态应用”的 Pod 删掉：\n$ kubectl delete pod -l app=nginx pod \u0026quot;web-0\u0026quot; deleted pod \u0026quot;web-1\u0026quot; deleted  然后，再在当前 Terminal 里 Watch 一下这两个 Pod 的状态变化，就会发现一个有趣的现象：\n$ kubectl get pod -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 0/1 ContainerCreating 0 0s NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 2s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 32s  可以看到，当我们把这两个 Pod 删除之后，Kubernetes 会按照原先编号的顺序，创建出了两个新的 Pod。并且，Kubernetes 依然为它们分配了与原来相同的“网络身份”：web-0.nginx 和 web-1.nginx。\n通过这种严格的对应规则，StatefulSet 就保证了 Pod 网络标识的稳定性。\n比如，如果 web-0 是一个需要先启动的主节点，web-1 是一个后启动的从节点，那么只要这个 StatefulSet 不被删除，你访问 web-0.nginx 时始终都会落在主节点上，访问 web-1.nginx 时，则始终都会落在从节点上，这个关系绝对不会发生任何变化。所以，如果我们再用 nslookup 命令，查看一下这个新 Pod 对应的 Headless Service 的话：\n$ kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh $ nslookup web-0.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 10.244.1.8 $ nslookup web-1.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 10.244.2.8  我们可以看到，在这个 StatefulSet 中，这两个新 Pod 的“网络标识”（比如：web-0.nginx 和 web-1.nginx），再次解析到了正确的 IP 地址（比如：web-0 Pod 的 IP 地址 10.244.1.8）。\n通过这种方法，Kubernetes 就成功地将 Pod 的拓扑状态（比如：哪个节点先启动，哪个节点后启动），按照 Pod 的“名字 + 编号”的方式固定了下来。此外，Kubernetes 还为每一个 Pod 提供了一个固定并且唯一的访问入口，即：这个 Pod 对应的 DNS 记录。这些状态，在 StatefulSet 的整个生命周期里都会保持不变，绝不会因为对应 Pod 的删除或者重新创建而失效。\n不过，相信你也已经注意到了，尽管 web-0.nginx 这条记录本身不会变，但它解析到的 Pod 的 IP 地址，并不是固定的。这就意味着，对于“有状态应用”实例的访问，你必须使用 DNS 记录或者 hostname 的方式，而绝不应该直接访问这些 Pod 的 IP 地址。\n总结\n在今天这篇文章中，我首先和你分享了 StatefulSet 的基本概念，解释了什么是应用的“状态”。\n紧接着 ，我为你分析了 StatefulSet 如何保证应用实例之间“拓扑状态”的稳定性。\n如果用一句话来总结的话，你可以这么理解这个过程：\n StatefulSet 这个控制器的主要作用之一，就是使用 Pod 模板创建 Pod 的时候，对它们进行编号，并且按照编号顺序逐一完成创建工作。而当 StatefulSet 的“控制循环”发现 Pod 的“实际状态”与“期望状态”不一致，需要新建或者删除 Pod 进行“调谐”的时候，它会严格按照这些 Pod 编号的顺序，逐一完成这些操作。\n 所以，StatefulSet 其实可以认为是对 Deployment 的改良。\n与此同时，通过 Headless Service 的方式，StatefulSet 为每个 Pod 创建了一个固定并且稳定的 DNS 记录，来作为它的访问入口。实际上，在部署“有状态应用”的时候，应用的每个实例拥有唯一并且稳定的“网络标识”，是一个非常重要的假设。在下一篇文章中，我将会继续为你剖析 StatefulSet 如何处理存储状态。\n","date":"2021年03月03日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter3/statefulset-1/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文转自张磊老师的《深入剖析 Kubernetes》课程，本节内容主要介绍 StatefulSet 的拓扑状态\u003c/p\u003e\n\u003c/blockquote\u003e","title":"深入理解StatefulSet（一）：拓扑状态"},{"contents":" 本文转自张磊老师的《深入剖析 Kubernetes》课程\n 作业副本 水平扩容   Deployment 看似简单，但实际上，它实现了 Kubernetes 项目中一个非常重要的功能：Pod 的“水平扩展 / 收缩”（horizontal scaling out/in）。这个功能，是从 PaaS 时代开始，一个平台级项目就必须具备的编排能力。\n举个例子，如果你更新了 Deployment 的 Pod 模板（比如，修改了容器的镜像），那么 Deployment 就需要遵循一种叫作“滚动更新”（rolling update）的方式，来升级现有的容器。而这个能力的实现，依赖的是 Kubernetes 项目中的一个非常重要的概念（API 对象）：ReplicaSet。\nReplicaSet 的结构非常简单，我们可以通过这个 YAML 文件查看一下：\napiVersion: apps/v1 kind: ReplicaSet metadata: name: nginx-set labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9  从这个 YAML 文件中，我们可以看到，一个 ReplicaSet 对象，**其实就是由副本数目的定义和一个 Pod 模板组成的。不难发现，它的定义其实是 Deployment 的一个子集。更重要的是，Deployment 控制器实际操纵的，正是这样的 ReplicaSet 对象，而不是 Pod 对象。**还记不记得我在上一篇文章《编排其实很简单：谈谈“控制器”模型》中曾经提出过这样一个问题：对于一个 Deployment 所管理的 Pod，它的 ownerReference 是谁？所以，这个问题的答案就是：ReplicaSet。\n明白了这个原理，我再来和你一起分析一个如下所示的 Deployment：\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80  可以看到，这就是一个我们常用的 nginx-deployment，它定义的 Pod 副本个数是 3（spec.replicas=3）。那么，在具体的实现上，这个 Deployment，与 ReplicaSet，以及 Pod 的关系是怎样的呢？我们可以用一张图把它描述出来：\n通过这张图，我们就很清楚地看到，一个定义了 replicas=3 的 Deployment，与它的 ReplicaSet，以及 Pod 的关系，实际上是一种“层层控制”的关系。其中，ReplicaSet 负责通过“控制器模式”，保证系统中 Pod 的个数永远等于指定的个数（比如，3 个）。这也正是 Deployment 只允许容器的 restartPolicy=Always 的主要原因：只有在容器能保证自己始终是 Running 状态的前提下，ReplicaSet 调整 Pod 的个数才有意义。而在此基础上，Deployment 同样通过“控制器模式”，来操作 ReplicaSet 的个数和属性，进而实现“水平扩展 / 收缩”和“滚动更新”这两个编排动作。\n其中，“水平扩展 / 收缩”非常容易实现，Deployment Controller 只需要修改它所控制的 ReplicaSet 的 Pod 副本个数就可以了。比如，把这个值从 3 改成 4，那么 Deployment 所对应的 ReplicaSet，就会根据修改后的值自动创建一个新的 Pod。这就是“水平扩展”了；“水平收缩”则反之。而用户想要执行这个操作的指令也非常简单，就是 kubectl scale，比如：\n$ kubectl scale deployment nginx-deployment --replicas=4 deployment.apps/nginx-deployment scaled  那么，“滚动更新”又是什么意思，是如何实现的呢？\n接下来，我还以这个 Deployment 为例，来为你讲解“滚动更新”的过程。\n首先，我们来创建这个 nginx-deployment：\n$ kubectl create -f nginx-deployment.yaml --record  然后，我们来检查一下 nginx-deployment 创建后的状态信息：\n$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 0 0 0 1s  在返回结果中，我们可以看到四个状态字段，它们的含义如下所示。\n DESIRED：用户期望的 Pod 副本个数（spec.replicas 的值）； CURRENT：当前处于 Running 状态的 Pod 的个数； UP-TO-DATE：当前处于最新版本的 Pod 的个数，所谓最新版本指的是 Pod 的 Spec 部分与 Deployment 里 Pod 模板里定义的完全一致； AVAILABLE：当前已经可用的 Pod 的个数，即：既是 Running 状态，又是最新版本，并且已经处于 Ready（健康检查正确）状态的 Pod 的个数。  可以看到，只有这个 AVAILABLE 字段，描述的才是用户所期望的最终状态。而 Kubernetes 项目还为我们提供了一条指令，让我们可以实时查看 Deployment 对象的状态变化。这个指令就是 kubectl rollout status：\n$ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment.apps/nginx-deployment successfully rolled out  在这个返回结果中，“2 out of 3 new replicas have been updated”意味着已经有 2 个 Pod 进入了 UP-TO-DATE 状态。继续等待一会儿，我们就能看到这个 Deployment 的 3 个 Pod，就进入到了 AVAILABLE 状态：\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 20s  此时，你可以尝试查看一下这个 Deployment 所控制的 ReplicaSet：\n$ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-3167673210 3 3 3 20s  如上所示，在用户提交了一个 Deployment 对象后，Deployment Controller 就会立即创建一个 Pod 副本个数为 3 的 ReplicaSet。这个 ReplicaSet 的名字，则是由 Deployment 的名字和一个随机字符串共同组成。\n这个随机字符串叫作 pod-template-hash，在我们这个例子里就是：3167673210。ReplicaSet 会把这个随机字符串加在它所控制的所有 Pod 的标签里，从而保证这些 Pod 不会与集群里的其他 Pod 混淆。\n而 ReplicaSet 的 DESIRED、CURRENT 和 READY 字段的含义，和 Deployment 中是一致的。所以，相比之下，Deployment 只是在 ReplicaSet 的基础上，添加了 UP-TO-DATE 这个跟版本有关的状态字段。\n这个时候，如果我们修改了 Deployment 的 Pod 模板，“滚动更新”就会被自动触发。修改 Deployment 有很多方法。比如，我可以直接使用 kubectl edit 指令编辑 Etcd 里的 API 对象。\n$ kubectl edit deployment/nginx-deployment ... spec: containers: - name: nginx image: nginx:1.9.1 # 1.7.9 -\u0026gt; 1.9.1 ports: - containerPort: 80 ... deployment.extensions/nginx-deployment edited  这个 kubectl edit 指令，会帮你直接打开 nginx-deployment 的 API 对象。然后，你就可以修改这里的 Pod 模板部分了。比如，在这里，我将 nginx 镜像的版本升级到了 1.9.1。\n 备注：kubectl edit 并不神秘，它不过是把 API 对象的内容下载到了本地文件，让你修改完成后再提交上去。\n kubectl edit 指令编辑完成后，保存退出，Kubernetes 就会立刻触发“滚动更新”的过程。你还可以通过 kubectl rollout status 指令查看 nginx-deployment 的状态变化：\n$ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment.extensions/nginx-deployment successfully rolled out  这时，你可以通过查看 Deployment 的 Events，看到这个“滚动更新”的流程：\n$ kubectl describe deployment nginx-deployment ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- ... Normal ScalingReplicaSet 24s deployment-controller Scaled up replica set nginx-deployment-1764197365 to 1 Normal ScalingReplicaSet 22s deployment-controller Scaled down replica set nginx-deployment-3167673210 to 2 Normal ScalingReplicaSet 22s deployment-controller Scaled up replica set nginx-deployment-1764197365 to 2 Normal ScalingReplicaSet 19s deployment-controller Scaled down replica set nginx-deployment-3167673210 to 1 Normal ScalingReplicaSet 19s deployment-controller Scaled up replica set nginx-deployment-1764197365 to 3 Normal ScalingReplicaSet 14s deployment-controller Scaled down replica set nginx-deployment-3167673210 to 0  可以看到，首先，当你修改了 Deployment 里的 Pod 定义之后，Deployment Controller 会使用这个修改后的 Pod 模板，创建一个新的 ReplicaSet（hash=1764197365），这个新的 ReplicaSet 的初始 Pod 副本数是：0。然后，在 Age=24 s 的位置，Deployment Controller 开始将这个新的 ReplicaSet 所控制的 Pod 副本数从 0 个变成 1 个，即：“水平扩展”出一个副本。紧接着，在 Age=22 s 的位置，Deployment Controller 又将旧的 ReplicaSet（hash=3167673210）所控制的旧 Pod 副本数减少一个，即：“水平收缩”成两个副本。如此交替进行，新 ReplicaSet 管理的 Pod 副本数，从 0 个变成 1 个，再变成 2 个，最后变成 3 个。而旧的 ReplicaSet 管理的 Pod 副本数则从 3 个变成 2 个，再变成 1 个，最后变成 0 个。\n这样，就完成了这一组 Pod 的版本升级过程。像这样，将一个集群中正在运行的多个 Pod 版本，交替地逐一升级的过程，就是“滚动更新”。\n在这个“滚动更新”过程完成之后，你可以查看一下新、旧两个 ReplicaSet 的最终状态：\n$ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-1764197365 3 3 3 6s nginx-deployment-3167673210 0 0 0 30s  其中，旧 ReplicaSet（hash=3167673210）已经被“水平收缩”成了 0 个副本。\n这种“滚动更新”的好处是显而易见的。比如，在升级刚开始的时候，集群里只有 1 个新版本的 Pod。如果这时，新版本 Pod 有问题启动不起来，那么“滚动更新”就会停止，从而允许开发和运维人员介入。而在这个过程中，由于应用本身还有两个旧版本的 Pod 在线，所以服务并不会受到太大的影响。当然，这也就要求你一定要使用 Pod 的 Health Check 机制检查应用的运行状态，而不是简单地依赖于容器的 Running 状态。要不然的话，虽然容器已经变成 Running 了，但服务很有可能尚未启动，“滚动更新”的效果也就达不到了。\n而为了进一步保证服务的连续性，Deployment Controller 还会确保，在任何时间窗口内，只有指定比例的 Pod 处于离线状态。同时，它也会确保，在任何时间窗口内，只有指定比例的新 Pod 被创建出来。这两个比例的值都是可以配置的，默认都是 DESIRED 值的 25%。\n所以，在上面这个 Deployment 的例子中，它有 3 个 Pod 副本，那么控制器在“滚动更新”的过程中永远都会确保至少有 2 个 Pod 处于可用状态，至多只有 4 个 Pod 同时存在于集群中。这个策略，是 Deployment 对象的一个字段，名叫 RollingUpdateStrategy，如下所示：\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: ... strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1  在上面这个 RollingUpdateStrategy 的配置中，maxSurge 指定的是除了 DESIRED 数量之外，在一次“滚动”中，Deployment 控制器还可以创建多少个新 Pod；而 maxUnavailable 指的是，在一次“滚动”中，Deployment 控制器可以删除多少个旧 Pod。\n同时，这两个配置还可以用前面我们介绍的百分比形式来表示，比如：maxUnavailable=50%，指的是我们最多可以一次删除“50%*DESIRED 数量”个 Pod。\n结合以上讲述，现在我们可以扩展一下 Deployment、ReplicaSet 和 Pod 的关系图了。\n如上所示，Deployment 的控制器，实际上控制的是 ReplicaSet 的数目，以及每个 ReplicaSet 的属性。而一个应用的版本，对应的正是一个 ReplicaSet；这个版本应用的 Pod 数量，则由 ReplicaSet 通过它自己的控制器（ReplicaSet Controller）来保证。通过这样的多个 ReplicaSet 对象，Kubernetes 项目就实现了对多个“应用版本”的描述。\n而明白了“应用版本和 ReplicaSet 一一对应”的设计思想之后，我就可以为你讲解一下Deployment 对应用进行版本控制的具体原理了。\n这一次，我会使用一个叫 kubectl set image 的指令，直接修改 nginx-deployment 所使用的镜像。这个命令的好处就是，你可以不用像 kubectl edit 那样需要打开编辑器。不过这一次，我把这个镜像名字修改成为了一个错误的名字，比如：nginx:1.91。这样，这个 Deployment 就会出现一个升级失败的版本。我们一起来实践一下：\n$ kubectl set image deployment/nginx-deployment nginx=nginx:1.91 deployment.extensions/nginx-deployment image updated  由于这个 nginx:1.91 镜像在 Docker Hub 中并不存在，所以这个 Deployment 的“滚动更新”被触发后，会立刻报错并停止。这时，我们来检查一下 ReplicaSet 的状态，如下所示：\n$ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-1764197365 2 2 2 24s nginx-deployment-3167673210 0 0 0 35s nginx-deployment-2156724341 2 2 0 7s  通过这个返回结果，我们可以看到，新版本的 ReplicaSet（hash=2156724341）的“水平扩展”已经停止。而且此时，它已经创建了两个 Pod，但是它们都没有进入 READY 状态。这当然是因为这两个 Pod 都拉取不到有效的镜像。与此同时，旧版本的 ReplicaSet（hash=1764197365）的“水平收缩”，也自动停止了。此时，已经有一个旧 Pod 被删除，还剩下两个旧 Pod。\n那么问题来了， 我们如何让这个 Deployment 的 3 个 Pod，都回滚到以前的旧版本呢？\n我们只需要执行一条 kubectl rollout undo 命令，就能把整个 Deployment 回滚到上一个版本：\n$ kubectl rollout undo deployment/nginx-deployment deployment.extensions/nginx-deployment  很容易想到，在具体操作上，Deployment 的控制器，其实就是让这个旧 ReplicaSet（hash=1764197365）再次“扩展”成 3 个 Pod，而让新的 ReplicaSet（hash=2156724341）重新“收缩”到 0 个 Pod。更进一步地，如果我想回滚到更早之前的版本，要怎么办呢？\n首先，我需要使用 kubectl rollout history 命令，查看每次 Deployment 变更对应的版本。而由于我们在创建这个 Deployment 的时候，指定了–record 参数，所以我们创建这些版本时执行的 kubectl 命令，都会被记录下来。这个操作的输出如下所示：\n$ kubectl rollout history deployment/nginx-deployment deployments \u0026quot;nginx-deployment\u0026quot; REVISION CHANGE-CAUSE 1 kubectl create -f nginx-deployment.yaml --record 2 kubectl edit deployment/nginx-deployment 3 kubectl set image deployment/nginx-deployment nginx=nginx:1.91  可以看到，我们前面执行的创建和更新操作，分别对应了版本 1 和版本 2，而那次失败的更新操作，则对应的是版本 3。\n当然，你还可以通过这个 kubectl rollout history 指令，看到每个版本对应的 Deployment 的 API 对象的细节，具体命令如下所示：\n$ kubectl rollout history deployment/nginx-deployment --revision=2  然后，我们就可以在 kubectl rollout undo 命令行最后，加上要回滚到的指定版本的版本号，就可以回滚到指定版本了。这个指令的用法如下：\n$ kubectl rollout undo deployment/nginx-deployment --to-revision=2 deployment.extensions/nginx-deployment  这样，Deployment Controller 还会按照“滚动更新”的方式，完成对 Deployment 的降级操作。不过，你可能已经想到了一个问题：我们对 Deployment 进行的每一次更新操作，都会生成一个新的 ReplicaSet 对象，是不是有些多余，甚至浪费资源呢？没错。所以，Kubernetes 项目还提供了一个指令，使得我们对 Deployment 的多次更新操作，最后 只生成一个 ReplicaSet。具体的做法是，在更新 Deployment 前，你要先执行一条 kubectl rollout pause 指令。它的用法如下所示：\n$ kubectl rollout pause deployment/nginx-deployment deployment.extensions/nginx-deployment paused  这个 kubectl rollout pause 的作用，是让这个 Deployment 进入了一个“暂停”状态。所以接下来，你就可以随意使用 kubectl edit 或者 kubectl set image 指令，修改这个 Deployment 的内容了。由于此时 Deployment 正处于“暂停”状态，所以我们对 Deployment 的所有修改，都不会触发新的“滚动更新”，也不会创建新的 ReplicaSet。而等到我们对 Deployment 修改操作都完成之后，只需要再执行一条 kubectl rollout resume 指令，就可以把这个 Deployment“恢复”回来，如下所示：\n$ kubectl rollout resume deployment/nginx-deployment deployment.extensions/nginx-deployment resumed  而在这个 kubectl rollout resume 指令执行之前，在 kubectl rollout pause 指令之后的这段时间里，我们对 Deployment 进行的所有修改，最后只会触发一次“滚动更新”。当然，我们可以通过检查 ReplicaSet 状态的变化，来验证一下 kubectl rollout pause 和 kubectl rollout resume 指令的执行效果，如下所示：\n$ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-1764197365 0 0 0 2m nginx-3196763511 3 3 3 28s  通过返回结果，我们可以看到，只有一个 hash=3196763511 的 ReplicaSet 被创建了出来。不过，即使你像上面这样小心翼翼地控制了 ReplicaSet 的生成数量，随着应用版本的不断增加，Kubernetes 中还是会为同一个 Deployment 保存很多很多不同的 ReplicaSet。那么，我们又该如何控制这些“历史”ReplicaSet 的数量呢？很简单，Deployment 对象有一个字段，叫作 spec.revisionHistoryLimit，就是 Kubernetes 为 Deployment 保留的“历史版本”个数。所以，如果把它设置为 0，你就再也不能做回滚操作了。\n总结\n在今天这篇文章中，我为你详细讲解了 Deployment 这个 Kubernetes 项目中最基本的编排控制器的实现原理和使用方法。\n通过这些讲解，你应该了解到：Deployment 实际上是一个两层控制器。首先，它通过 ReplicaSet 的个数来描述应用的版本；然后，它再通过 ReplicaSet 的属性（比如 replicas 的值），来保证 Pod 的副本数量。\n 备注：Deployment 控制 ReplicaSet（版本），ReplicaSet 控制 Pod（副本数）。这个两层控制关系一定要牢记。\n 不过，相信你也能够感受到，Kubernetes 项目对 Deployment 的设计，实际上是代替我们完成了对“应用”的抽象，使得我们可以使用这个 Deployment 对象来描述应用，使用 kubectl rollout 命令控制应用的版本。\n可是，在实际使用场景中，应用发布的流程往往千差万别，也可能有很多的定制化需求。比如，我的应用可能有会话黏连（session sticky），这就意味着“滚动更新”的时候，哪个 Pod 能下线，是不能随便选择的。\n这种场景，光靠 Deployment 自己就很难应对了。对于这种需求，我在专栏后续文章中重点介绍的“自定义控制器”，就可以帮我们实现一个功能更加强大的 Deployment Controller。\n当然，Kubernetes 项目本身，也提供了另外一种抽象方式，帮我们应对其他一些用 Deployment 无法处理的应用编排场景。这个设计，就是对有状态应用的管理，也是我在下一篇文章中要重点讲解的内容。\n","date":"2021年03月01日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter3/deployment/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文转自张磊老师的《深入剖析 Kubernetes》课程\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e作业副本\u003c/li\u003e\n\u003cli\u003e水平扩容\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/blockquote\u003e","title":"Deployment：作业副本与水平扩容"},{"contents":" 本文转自张磊老师的《深入剖析 Kubernetes》课程\n 实际上，你可能已经有所感悟：Pod 这个看似复杂的 API 对象，实际上就是对容器的进一步抽象和封装而已。\n说得更形象些，“容器”镜像虽然好用，但是容器这样一个“沙盒”的概念，对于描述应用来说，还是太过简单了。这就好比，集装箱固然好用，但是如果它四面都光秃秃的，吊车还怎么把这个集装箱吊起来并摆放好呢？所以，Pod 对象，其实就是容器的升级版。它对容器进行了组合，添加了更多的属性和字段。这就好比给集装箱四面安装了吊环，使得 Kubernetes 这架“吊车”，可以更轻松地操作它。\n而 Kubernetes 操作这些“集装箱”的逻辑，都由控制器（Controller）完成。在前面的第 12 篇文章《牛刀小试：我的第一个容器化应用》中，我们曾经使用过 Deployment 这个最基本的控制器对象。\n现在，我们一起来回顾一下这个名叫 nginx-deployment 的例子：\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80  这个 Deployment 定义的编排动作非常简单，即：确保携带了 app=nginx 标签的 Pod 的个数，永远等于 spec.replicas 指定的个数，即 2 个。\n这就意味着，如果在这个集群中，携带 app=nginx 标签的 Pod 的个数大于 2 的时候，就会有旧的 Pod 被删除；反之，就会有新的 Pod 被创建。\n这时，你也许就会好奇：究竟是 Kubernetes 项目中的哪个组件，在执行这些操作呢？\n我在前面介绍 Kubernetes 架构的时候，曾经提到过一个叫作 kube-controller-manager 的组件。\n实际上，这个组件，就是一系列控制器的集合。我们可以查看一下 Kubernetes 项目的 pkg/controller 目录：\n$ cd kubernetes/pkg/controller/ $ ls -d */ deployment/ job/ podautoscaler/ cloud/ disruption/ namespace/ replicaset/ serviceaccount/ volume/ cronjob/ garbagecollector/ nodelifecycle/ replication/ statefulset/ daemon/ ...  这个目录下面的每一个控制器，都以独有的方式负责某种编排功能。而我们的 Deployment，正是这些控制器中的一种。\n实际上，这些控制器之所以被统一放在 pkg/controller 目录下，就是因为它们都遵循 Kubernetes 项目中的一个通用编排模式，即：控制循环（control loop）。\n比如，现在有一种待编排的对象 X，它有一个对应的控制器。那么，我就可以用一段 Go 语言风格的伪代码，为你描述这个控制循环：\nfor { 实际状态 := 获取集群中对象X的实际状态（Actual State） 期望状态 := 获取集群中对象X的期望状态（Desired State） if 实际状态 == 期望状态{ 什么都不做 } else { 执行编排动作，将实际状态调整为期望状态 } }  在具体实现中，实际状态往往来自于 Kubernetes 集群本身。\n比如，kubelet 通过心跳汇报的容器状态和节点状态，或者监控系统中保存的应用监控数据，或者控制器主动收集的它自己感兴趣的信息，这些都是常见的实际状态的来源。\n而期望状态，一般来自于用户提交的 YAML 文件。\n比如，Deployment 对象中 Replicas 字段的值。很明显，这些信息往往都保存在 Etcd 中。\n接下来，以 Deployment 为例，我和你简单描述一下它对控制器模型的实现：\n Deployment 控制器从 Etcd 中获取到所有携带了“app: nginx”标签的 Pod，然后统计它们的数量，这就是实际状态； Deployment 对象的 Replicas 字段的值就是期望状态； Deployment 控制器将两个状态做比较，然后根据比较结果，确定是创建 Pod，还是删除已有的 Pod（具体如何操作 Pod 对象，我会在下一篇文章详细介绍）。  可以看到，一个 Kubernetes 对象的主要编排逻辑，实际上是在第三步的“对比”阶段完成的。\n这个操作，通常被叫作调谐（Reconcile）。这个调谐的过程，则被称作“Reconcile Loop”（调谐循环）或者“Sync Loop”（同步循环）。\n所以，如果你以后在文档或者社区中碰到这些词，都不要担心，它们其实指的都是同一个东西：控制循环。\n而调谐的最终结果，往往都是对被控制对象的某种写操作。\n比如，增加 Pod，删除已有的 Pod，或者更新 Pod 的某个字段。这也是 Kubernetes 项目“面向 API 对象编程”的一个直观体现。\n其实，像 Deployment 这种控制器的设计原理，就是我们前面提到过的，“用一种对象管理另一种对象”的“艺术”。\n其中，这个控制器对象本身，负责定义被管理对象的期望状态。比如，Deployment 里的 replicas=2 这个字段。而被控制对象的定义，则来自于一个“模板”。比如，Deployment 里的 template 字段。可以看到，Deployment 这个 template 字段里的内容，跟一个标准的 Pod 对象的 API 定义，丝毫不差。而所有被这个 Deployment 管理的 Pod 实例，其实都是根据这个 template 字段的内容创建出来的。像 Deployment 定义的 template 字段，在 Kubernetes 项目中有一个专有的名字，叫作 PodTemplate（Pod 模板）。\n这个概念非常重要，因为后面我要讲解到的大多数控制器，都会使用 PodTemplate 来统一定义它所要管理的 Pod。更有意思的是，我们还会看到其他类型的对象模板，比如 Volume 的模板。至此，我们就可以对 Deployment 以及其他类似的控制器，做一个简单总结了：\n如上图所示，类似 Deployment 这样的一个控制器，实际上都是由上半部分的控制器定义（包括期望状态），加上下半部分的被控制对象的模板组成的。\n这就是为什么，在所有 API 对象的 Metadata 里，都有一个字段叫作 ownerReference，用于保存当前这个 API 对象的拥有者（Owner）的信息。\n那么，对于我们这个 nginx-deployment 来说，它创建出来的 Pod 的 ownerReference 就是 nginx-deployment 吗？或者说，nginx-deployment 所直接控制的，就是 Pod 对象么？这个问题的答案，我就留到下一篇文章时再做详细解释吧。\n总结\n在今天这篇文章中，我以 Deployment 为例，和你详细分享了 Kubernetes 项目如何通过一个称作“控制器模式”（controller pattern）的设计方法，来统一地实现对各种不同的对象或者资源进行的编排操作。\n在后面的讲解中，我还会讲到很多不同类型的容器编排功能，比如 StatefulSet、DaemonSet 等等，它们无一例外地都有这样一个甚至多个控制器的存在，并遵循控制循环（control loop）的流程，完成各自的编排逻辑。\n实际上，跟 Deployment 相似，这些控制循环最后的执行结果，要么就是创建、更新一些 Pod（或者其他的 API 对象、资源），要么就是删除一些已经存在的 Pod（或者其他的 API 对象、资源）。但也正是在这个统一的编排框架下，不同的控制器可以在具体执行过程中，设计不同的业务逻辑，从而达到不同的编排效果。\n这个实现思路，正是 Kubernetes 项目进行容器编排的核心原理。在此后讲解 Kubernetes 编排功能的文章中，我都会遵循这个逻辑展开，并且带你逐步领悟控制器模式在不同的容器化作业中的实现方式。\n思考题\n你能否说出，Kubernetes 使用的这个“控制器模式”，跟我们平常所说的“事件驱动”，有什么区别和联系吗？\n A:\n  “事件驱动”，对于控制器来说是被动，只要触发事件则执行，对执行后不负责，无论成功与否，没有对一次操作的后续进行“监控” “控制器模式”，对于控制器来说是主动的，自身在不断地获取信息，起到事后“监控”作用，知道同步完成，实际状态与期望状态一致\n  事件往往是一次性的，如果操作失败比较难处理，但是控制器是循环一直在尝试的，更符合kubernetes声明式API，最终达到与声明一致。\n  ","date":"2021年02月28日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter3/controller-mode/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文转自张磊老师的《深入剖析 Kubernetes》课程\u003c/p\u003e\n\u003c/blockquote\u003e","title":"谈谈“控制器”模式"},{"contents":"一. 背景 发布容器的时候，需要输入Health Check的检查方式，以便K8S集群能够管理容器的健康状态，做容器的停止和重建的操作。公司发布的容器应用类型包括多种，有Nginx+PHP，OSP，Nginx+Tomcat等，而每个应用的Health Check的方式都各异，因此需要统一各应用的Health Check检查方式。\n二. Health Check方式 先归纳下各种应用Health Check的使用方式\n Nginx+PHP 和 Nginx+Tomcat，以及 ai-agent 类型应用容器，是通过调用{domain-name}:port/_health_check的http接口做Health Check，各应用必须提供这个url OSP服务，Saturn服务以及ai-model类型应用容器，是通过telnet连接指定的端口做Health Check 其他类型的容器，是通过容器自定义脚本做Health Check，如果没有自定义脚本，则直接touch /tmp/health，默认容器健康检查成功  三. 容器化的Health Check机制 K8S的Health Check目的是检查业务的Pod是否可用\n容器统一使用ExecProbe调用脚本/apps/sh/k8s_node/health_check.sh\n容器健康检查的开关机制：\n 检查/apps/sh/no_health_check文件，如果文件存在且其中内容为true，则跳过健康检查，直接返回0。这个功能适用于禁用单个容器的健康检查，可以配合vjdump等命令使用。 检查/docker/logs/\u0026lt;domain_name\u0026gt;/no_health_check文件，如果文件存在且其中内容为true，则跳过健康检查，直接返回0。由于/docker/logs/\u0026lt;domain_name\u0026gt;目录对应到宿主机的/apps/logs/log_receiver/\u0026lt;domain_name\u0026gt;目录，运维可以下发文件来禁用整个域所有容器的健康检查。  1. Http方式的Health Check K8S启动的Nginx+PHP 和 Nginx+Tomcat类型应用容器，以及ai-agent类型应用容器，可以通过以下方式做Health Check\n/apps/sh/k8s_node/health_check.sh http \u0026lt;port\u0026gt; \u0026lt;timeout\u0026gt;\nlivenessProbe: exec: command: - bash - /apps/sh/k8s_node/health_check.sh - http - \u0026quot;80\u0026quot; - \u0026quot;5\u0026quot; failureThreshold: 3 initialDelaySeconds: 300 periodSeconds: 3 successThreshold: 1 timeoutSeconds: 5 name: php-k6zq82 readinessProbe: exec: command: - bash - /apps/sh/k8s_node/health_check.sh - http - \u0026quot;80\u0026quot; - \u0026quot;1\u0026quot; failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 3 successThreshold: 1 timeoutSeconds: 1  默认配置定义了应用类型与其http健康检查对应的端口 probe.httpget.apps=${PROBE_HTTPGET_APPS:{\u0026quot;web\u0026quot;:80,\u0026quot;ai-agent\u0026quot;:8079}}\n可以通过配置中心下发PROBE_HTTPGET_APPS来修改\n2. Tcp方式的Health Check K8S启动的OSP服务，Saturn服务以及ai-model类型应用容器，可以通过以下方式做Health Check\n/apps/sh/k8s_node/health_check.sh tcp \u0026lt;port\u0026gt; \u0026lt;timeout\u0026gt;\nlivenessProbe: exec: command: - bash - /apps/sh/k8s_node/health_check.sh - tcp - \u0026quot;8080\u0026quot; - \u0026quot;5\u0026quot; failureThreshold: 3 initialDelaySeconds: 300 periodSeconds: 3 successThreshold: 1 timeoutSeconds: 5 name: osp-kdo8d6 readinessProbe: exec: command: - bash - /apps/sh/k8s_node/health_check.sh - tcp - \u0026quot;8080\u0026quot; - \u0026quot;1\u0026quot; failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 3 successThreshold: 1 timeoutSeconds: 1  Noah默认配置定义了应用类型与其tcp健康检查对应的端口 probe.tcpsocket.apps=${NOAH_PROBE_TCPSOCKET_APPS:{\u0026quot;osp\u0026quot;:8080,\u0026quot;saturn\u0026quot;:24500,\u0026quot;ai-model\u0026quot;:8500}}\t\n可以通过配置中心下发NOAH_PROBE_TCPSOCKET_APPS来修改\n3. Other方式的Health Check 如果应用类型不在probe.httpget.apps和probe.tcpsocket.apps的配置中，则使用other类型的健康检查\nother类型的健康检查实现为touch /tmp/health，容器健康检查默认成功。\n/apps/sh/k8s_node/health_check.sh other \u0026lt;timeout\u0026gt;\nlivenessProbe: exec: command: - bash - /apps/sh/k8s_node/health_check.sh - other - \u0026quot;5\u0026quot; failureThreshold: 3 initialDelaySeconds: 300 periodSeconds: 3 successThreshold: 1 timeoutSeconds: 5 name: php-k6zq82 readinessProbe: exec: command: - bash - /apps/sh/k8s_node/health_check.sh - other - \u0026quot;1\u0026quot; failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 3 successThreshold: 1 timeoutSeconds: 1  4. 自定义Health Check Noah容器自定义健康检查，在构建容器镜像时可以通过moana extend的方式在容器启动时将custom_health_check.sh放到/apps/sh目录下：\n在http/tcp/other类型的hc过程中，如果容器内存在文件/apps/sh/custom_health_check.sh时，则调用/apps/sh/custom_health_check.sh脚本；入参为timeout，脚本需要实现检查超时；如果脚本输出ok（一定要小写），则代表健康检查成功。\n四. 关于容器ReadinessProbe和LivenessProbe的说明  ReadinessProbe通过，则容器进入Ready状态； Liveness如果连续失败failureThreshold指定的次数，则认为容器不能正常服务，kubelet会重启容器； initialDeplaySeconds设定probe的延迟秒数，即容器启动后等待多少秒开始执行probe； periodSeconds设定probe的间隔，即开始执行probe后每隔多少秒执行一次； timeoutSeconds设定一次probe的超时时间，由于ExecProbe实际上是没有使用这个值的，所以health_check.sh和other类型所用到的custom_health_check.sh需要自己实现超时； ","date":"2021年02月27日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter9/health-check/","summary":"","title":"容器健康检查"},{"contents":" 本文转自张磊老师的《深入剖析 Kubernetes》课程\n ProjectedVolume: Secret, ConfigMap, DownwadAPI ServiceAccount 容器健康检查和恢复机制 Pod 预设置：PodPreset   作为 Kubernetes 项目里最核心的编排对象，Pod 携带的信息非常丰富。其中，资源定义（比如 CPU、内存等），以及调度相关的字段，我会在后面专门讲解调度器时再进行深入的分析。在本篇，我们就先从一种特殊的 Volume 开始，来帮助你更加深入地理解 Pod 对象各个重要字段的含义。\n这种特殊的 Volume，叫作 Projected Volume，你可以把它翻译为“投射数据卷”。\n 备注：Projected Volume 是 Kubernetes v1.11 之后的新特性\n 这是什么意思呢？\n在 Kubernetes 中，有几种特殊的 Volume，它们存在的意义不是为了存放容器里的数据，也不是用来进行容器和宿主机之间的数据交换。这些特殊 Volume 的作用，是为容器提供预先定义好的数据。所以，从容器的角度来看，这些 Volume 里的信息就是仿佛是被 Kubernetes“投射”（Project）进入容器当中的。这正是 Projected Volume 的含义。\nProjected Volume: Secret, ConfigMap, Downward API 到目前为止，Kubernetes 支持的 Projected Volume 一共有四种：\n Secret； ConfigMap； Downward API； ServiceAccountToken。  在今天这篇文章中，我首先和你分享的是 Secret。它的作用，是帮你把 Pod 想要访问的加密数据，存放到 Etcd 中。然后，你就可以通过在 Pod 的容器里挂载 Volume 的方式，访问到这些 Secret 里保存的信息了。\nSecret 最典型的使用场景，莫过于存放数据库的 Credential 信息，比如下面这个例子：\napiVersion: v1 kind: Pod metadata: name: test-projected-volume spec: containers: - name: test-secret-volume image: busybox args: - sleep - \u0026quot;86400\u0026quot; volumeMounts: - name: mysql-cred mountPath: \u0026quot;/projected-volume\u0026quot; readOnly: true volumes: - name: mysql-cred projected: sources: - secret: name: user - secret: name: pass  在这个 Pod 中，我定义了一个简单的容器。它声明挂载的 Volume，并不是常见的 emptyDir 或者 hostPath 类型，而是 projected 类型。而这个 Volume 的数据来源（sources），则是名为 user 和 pass 的 Secret 对象，分别对应的是数据库的用户名和密码。\n这里用到的数据库的用户名、密码，正是以 Secret 对象的方式交给 Kubernetes 保存的。完成这个操作的指令，如下所示：\n$ cat ./username.txt admin $ cat ./password.txt c1oudc0w! $ kubectl create secret generic user --from-file=./username.txt $ kubectl create secret generic pass --from-file=./password.txt  其中，username.txt 和 password.txt 文件里，存放的就是用户名和密码；而 user 和 pass，则是我为 Secret 对象指定的名字。而我想要查看这些 Secret 对象的话，只要执行一条 kubectl get 命令就可以了：\n# kubectl get secrets NAME TYPE DATA AGE default-token-97ss5 kubernetes.io/service-account-token 3 8d pass Opaque 1 9s user Opaque 1 35s  当然，除了使用 kubectl create secret 指令外，我也可以直接通过编写 YAML 文件的方式来创建这个 Secret 对象，比如：\napiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: user: YWRtaW4= pass: MWYyZDFlMmU2N2Rm  可以看到，通过编写 YAML 文件创建出来的 Secret 对象只有一个。但它的 data 字段，却以 Key-Value 的格式保存了两份 Secret 数据。其中，“user”就是第一份数据的 Key，“pass”是第二份数据的 Key。\n需要注意的是，Secret 对象要求这些数据必须是经过 Base64 转码的，以免出现明文密码的安全隐患。这个转码操作也很简单，比如：\n$ echo -n 'admin' | base64 YWRtaW4= $ echo -n '1f2d1e2e67df' | base64 MWYyZDFlMmU2N2Rm  这里需要注意的是，像这样创建的 Secret 对象，它里面的内容仅仅是经过了转码，而并没有被加密。在真正的生产环境中，你需要在 Kubernetes 中开启 Secret 的加密插件，增强数据的安全性。关于开启 Secret 加密插件的内容，我会在后续专门讲解 Secret 的时候，再做进一步说明。\n接下来，我们尝试一下创建这个 Pod：\n$ kubectl create -f test-projected-volume.yaml  当 Pod 变成 Running 状态之后，我们再验证一下这些 Secret 对象是不是已经在容器里了：\n$ kubectl exec -it test-projected-volume -- /bin/sh / $ ls /projected-volume/ password.txt username.txt / $ cat /projected-volume/username.txt admin / $ cat /projected-volume/password.txt cloud0w  从返回结果中，我们可以看到，保存在 Etcd 里的用户名和密码信息，已经以文件的形式出现在了容器的 Volume 目录里。而这个文件的名字，就是 kubectl create secret 指定的 Key，或者说是 Secret 对象的 data 字段指定的 Key。\n更重要的是，像这样通过挂载方式进入到容器里的 Secret，一旦其对应的 Etcd 里的数据被更新，这些 Volume 里的文件内容，同样也会被更新。其实，这是 kubelet 组件在定时维护这些 Volume。\n需要注意的是，这个更新可能会有一定的延时。所以在编写应用程序时，在发起数据库连接的代码处写好重试和超时的逻辑，绝对是个好习惯。\n与 Secret 类似的是 ConfigMap，它与 Secret 的区别在于，ConfigMap 保存的是不需要加密的、应用所需的配置信息。而 ConfigMap 的用法几乎与 Secret 完全相同：你可以使用 kubectl create configmap 从文件或者目录创建 ConfigMap，也可以直接编写 ConfigMap 对象的 YAML 文件。\n比如，一个 Java 应用所需的配置文件（.properties 文件），就可以通过下面这样的方式保存在 ConfigMap 里：\n# .properties文件的内容 $ cat example/ui.properties color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice # 从.properties文件创建ConfigMap $ kubectl create configmap ui-config --from-file=example/ui.properties # 查看这个ConfigMap里保存的信息(data) $ kubectl get configmaps ui-config -o yaml apiVersion: v1 data: ui.properties: | color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice kind: ConfigMap metadata: name: ui-config ...   备注：kubectl get -o yaml 这样的参数，会将指定的 Pod API 对象以 YAML 的方式展示出来。\n **接下来是 Downward API，**它的作用是：让 Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息。\n举个例子：\napiVersion: v1 kind: Pod metadata: name: test-downwardapi-volume labels: zone: us-est-coast cluster: test-cluster1 rack: rack-22 spec: containers: - name: client-container image: k8s.gcr.io/busybox command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;] args: - while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en '\\n\\n'; cat /etc/podinfo/labels; fi; sleep 5; done; volumeMounts: - name: podinfo mountPath: /etc/podinfo readOnly: false volumes: - name: podinfo projected: sources: - downwardAPI: items: - path: \u0026quot;labels\u0026quot; fieldRef: fieldPath: metadata.labels  在这个 Pod 的 YAML 文件中，我定义了一个简单的容器，声明了一个 projected 类型的 Volume。只不过这次 Volume 的数据来源，变成了 Downward API。而这个 Downward API Volume，则声明了要暴露 Pod 的 metadata.labels 信息给容器。\n通过这样的声明方式，当前 Pod 的 Labels 字段的值，就会被 Kubernetes 自动挂载成为容器里的 /etc/podinfo/labels 文件。而这个容器的启动命令，则是不断打印出 /etc/podinfo/labels 里的内容。\n所以，当我创建了这个 Pod 之后，就可以通过 kubectl logs 指令，查看到这些 Labels 字段被打印出来，如下所示：\n$ kubectl create -f dapi-volume.yaml $ kubectl logs test-downwardapi-volume cluster=\u0026quot;test-cluster1\u0026quot; rack=\u0026quot;rack-22\u0026quot; zone=\u0026quot;us-est-coast\u0026quot;  目前，Downward API 支持的字段已经非常丰富了，比如：\n1. 使用fieldRef可以声明使用: spec.nodeName - 宿主机名字 status.hostIP - 宿主机IP metadata.name - Pod的名字 metadata.namespace - Pod的Namespace status.podIP - Pod的IP spec.serviceAccountName - Pod的Service Account的名字 metadata.uid - Pod的UID metadata.labels['\u0026lt;KEY\u0026gt;'] - 指定\u0026lt;KEY\u0026gt;的Label值 metadata.annotations['\u0026lt;KEY\u0026gt;'] - 指定\u0026lt;KEY\u0026gt;的Annotation值 metadata.labels - Pod的所有Label metadata.annotations - Pod的所有Annotation 2. 使用resourceFieldRef可以声明使用: 容器的CPU limit 容器的CPU request 容器的memory limit 容器的memory request  上面这个列表的内容，随着 Kubernetes 项目的发展肯定还会不断增加。所以这里列出来的信息仅供参考，你在使用 Downward API 时，还是要记得去查阅一下官方文档。\n不过，需要注意的是，Downward API 能够获取到的信息，**一定是 Pod 里的容器进程启动之前就能够确定下来的信息。**而如果你想要获取 Pod 容器运行后才会出现的信息，比如，容器进程的 PID，那就肯定不能使用 Downward API 了，而应该考虑在 Pod 里定义一个 sidecar 容器。\n其实，Secret、ConfigMap，以及 Downward API 这三种 Projected Volume 定义的信息，大多还可以通过环境变量的方式出现在容器里。但是，通过环境变量获取这些信息的方式，不具备自动更新的能力。所以，一般情况下，我都建议你使用 Volume 文件的方式获取这些信息。\nService Account 在明白了 Secret 之后，我再为你讲解 Pod 中一个与它密切相关的概念：Service Account。\n相信你一定有过这样的想法：我现在有了一个 Pod，我能不能在这个 Pod 里安装一个 Kubernetes 的 Client，这样就可以从容器里直接访问并且操作这个 Kubernetes 的 API 了呢？这当然是可以的。不过，你首先要解决 API Server 的授权问题。\nService Account 对象的作用，就是 Kubernetes 系统内置的一种“服务账户”，它是 Kubernetes 进行权限分配的对象。比如，Service Account A，可以只被允许对 Kubernetes API 进行 GET 操作，而 Service Account B，则可以有 Kubernetes API 的所有操作权限。像这样的 Service Account 的授权信息和文件，实际上保存在它所绑定的一个特殊的 Secret 对象里的。这个特殊的 Secret 对象，就叫作 ServiceAccountToken。任何运行在 Kubernetes 集群上的应用，都必须使用这个 ServiceAccountToken 里保存的授权信息，也就是 Token，才可以合法地访问 API Server。所以说，Kubernetes 项目的 Projected Volume 其实只有三种，因为第四种 ServiceAccountToken，只是一种特殊的 Secret 而已。\n另外，为了方便使用，Kubernetes 已经为你提供了一个默认“服务账户”（default Service Account）。并且，任何一个运行在 Kubernetes 里的 Pod，都可以直接使用这个默认的 Service Account，而无需显示地声明挂载它。\n这是如何做到的呢？\n当然还是靠 Projected Volume 机制。\n如果你查看一下任意一个运行在 Kubernetes 集群里的 Pod，就会发现，每一个 Pod，都已经自动声明一个类型是 Secret、名为 default-token-xxxx 的 Volume，然后 自动挂载在每个容器的一个固定目录上。比如：\n# kubectl describe pod nginx-deployment-748c6fff66-2mnfj Containers: ... Mounts: /usr/share/nginx/html from nginx-vol (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-97ss5 (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: nginx-vol: Type: EmptyDir (a temporary directory that shares a pod's lifetime) Medium: SizeLimit: \u0026lt;unset\u0026gt; default-token-97ss5: Type: Secret (a volume populated by a Secret) SecretName: default-token-97ss5 Optional: false ......  这个 Secret 类型的 Volume，正是默认 Service Account 对应的 ServiceAccountToken。所以说，Kubernetes 其实在每个 Pod 创建的时候，自动在它的 spec.volumes 部分添加上了默认 ServiceAccountToken 的定义，然后自动给每个容器加上了对应的 volumeMounts 字段。这个过程对于用户来说是完全透明的。这样，一旦 Pod 创建完成，容器里的应用就可以直接从这个默认 ServiceAccountToken 的挂载目录里访问到授权信息和文件。\n这个容器内的路径在 Kubernetes 里是固定的，即：/var/run/secrets/kubernetes.io/serviceaccount ，而这个 Secret 类型的 Volume 里面的内容如下所示：\n# ls /var/run/secrets/kubernetes.io/serviceaccount ca.crt namespace token  所以，你的应用程序只要直接加载这些授权文件，就可以访问并操作 Kubernetes API 了。而且，如果你使用的是 Kubernetes 官方的 Client 包（k8s.io/client-go）的话，它还可以自动加载这个目录下的文件，你不需要做任何配置或者编码操作。\n这种把 Kubernetes 客户端以容器的方式运行在集群里，然后使用 default Service Account 自动授权的方式，被称作“InClusterConfig”，也是我最推荐的进行 Kubernetes API 编程的授权方式。\n当然，考虑到自动挂载默认 ServiceAccountToken 的潜在风险，Kubernetes 允许你设置默认不为 Pod 里的容器自动挂载这个 Volume。\n除了这个默认的 Service Account 外，我们很多时候还需要创建一些我们自己定义的 Service Account，来对应不同的权限设置。这样，我们的 Pod 里的容器就可以通过挂载这些 Service Account 对应的 ServiceAccountToken，来使用这些自定义的授权信息。在后面讲解为 Kubernetes 开发插件的时候，我们将会实践到这个操作。\n容器健康检查和恢复机制 接下来，我们再来看 Pod 的另一个重要的配置：容器健康检查和恢复机制。\n在 Kubernetes 中，你可以为 Pod 里的容器定义一个健康检查“探针”（Probe）。这样，kubelet 就会根据这个 Probe 的返回值决定这个容器的状态，而不是直接以容器镜像是否运行（来自 Docker 返回的信息）作为依据。这种机制，是生产环境中保证应用健康存活的重要手段。\n我们一起来看一个 Kubernetes 文档中的例子。\napiVersion: v1 kind: Pod metadata: labels: test: liveness name: test-liveness-exec spec: containers: - name: liveness image: busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5  在这个 Pod 中，我们定义了一个有趣的容器。它在启动之后做的第一件事，就是在 /tmp 目录下创建了一个 healthy 文件，以此作为自己已经正常运行的标志。而 30 s 过后，它会把这个文件删除掉。与此同时，我们定义了一个这样的 livenessProbe（健康检查）。它的类型是 exec，这意味着，它会在容器启动后，在容器里面执行一条我们指定的命令，比如：“cat /tmp/healthy”。这时，如果这个文件存在，这条命令的返回值就是 0，Pod 就会认为这个容器不仅已经启动，而且是健康的。这个健康检查，在容器启动 5 s 后开始执行（initialDelaySeconds: 5），每 5 s 执行一次（periodSeconds: 5）。\n现在，让我们来具体实践一下这个过程。\n首先，创建这个 Pod：\n$ kubectl create -f test-liveness-exec.yaml  然后，查看这个 Pod 的状态：\n$ kubectl get pod NAME READY STATUS RESTARTS AGE test-liveness-exec 1/1 Running 0 10s  可以看到，由于已经通过了健康检查，这个 Pod 就进入了 Running 状态。\n而 30 s 之后，我们再查看一下 Pod 的 Events：\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- ... Normal Killing 2m11s (x3 over 4m46s) kubelet Container liveness failed liveness probe, will be restarted Warning Unhealthy 2m11s (x9 over 4m56s) kubelet Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory ...  显然，这个健康检查探查到 /tmp/healthy 已经不存在了，所以它报告容器是不健康的。那么接下来会发生什么呢？\n我们不妨再次查看一下这个 Pod 的状态：\n# kubectl get pod test-liveness-exec NAME READY STATUS RESTARTS AGE test-liveness-exec 1/1 Running 4 6m9s  这时我们发现，Pod 并没有进入 Failed 状态，而是保持了 Running 状态。这是为什么呢？\n其实，如果你注意到 RESTARTS 字段从 0 到 1 的变化，就明白原因了：这个异常的容器已经被 Kubernetes 重启了。在这个过程中，Pod 保持 Running 状态不变。\n需要注意的是：Kubernetes 中并没有 Docker 的 Stop 语义。所以虽然是 Restart（重启），但实际却是重新创建了容器。\n这个功能就是 Kubernetes 里的 Pod 恢复机制，也叫 restartPolicy。它是 Pod 的 Spec 部分的一个标准字段（pod.spec.restartPolicy），默认值是 Always，即：任何时候这个容器发生了异常，它一定会被重新创建。\n但一定要强调的是，Pod 的恢复过程，永远都是发生在当前节点上，而不会跑到别的节点上去。事实上，一旦一个 Pod 与一个节点（Node）绑定，除非这个绑定发生了变化（pod.spec.node 字段被修改），否则它永远都不会离开这个节点。这也就意味着，如果这个宿主机宕机了，这个 Pod 也不会主动迁移到其他节点上去。\n而如果你想让 Pod 出现在其他的可用节点上，就必须使用 Deployment 这样的“控制器”来管理 Pod，哪怕你只需要一个 Pod 副本。这就是我在第 12 篇文章《牛刀小试：我的第一个容器化应用》最后给你留的思考题的答案，即一个单 Pod 的 Deployment 与一个 Pod 最主要的区别。\n而作为用户，你还可以通过设置 restartPolicy，改变 Pod 的恢复策略。除了 Always，它还有 OnFailure 和 Never 两种情况：\n Always：在任何情况下，只要容器不在运行状态，就自动重启容器； OnFailure: 只在容器 异常时才自动重启容器； Never: 从来不重启容器。  在实际使用时，我们需要根据应用运行的特性，合理设置这三种恢复策略。\n比如，一个 Pod，它只计算 1+1=2，计算完成输出结果后退出，变成 Succeeded 状态。这时，你如果再用 restartPolicy=Always 强制重启这个 Pod 的容器，就没有任何意义了。\n而如果你要关心这个容器退出后的上下文环境，比如容器退出后的日志、文件和目录，就需要将 restartPolicy 设置为 Never。因为一旦容器被自动重新创建，这些内容就有可能丢失掉了（被垃圾回收了）。\n值得一提的是，Kubernetes 的官方文档，把 restartPolicy 和 Pod 里容器的状态，以及 Pod 状态的对应关系，总结了非常复杂的一大堆情况。实际上，你根本不需要死记硬背这些对应关系，只要记住如下两个基本的设计原理即可：\n  只要 Pod 的 restartPolicy 指定的策略允许重启异常的容器（比如：Always），那么这个 Pod 就会保持 Running 状态，并进行容器重启。否则，Pod 就会进入 Failed 状态 。\n  对于包含多个容器的 Pod，只有它里面所有的容器都进入异常状态后，Pod 才会进入 Failed 状态。在此之前，Pod 都是 Running 状态。此时，Pod 的 READY 字段会显示正常容器的个数，比如：\n$ kubectl get pod test-liveness-exec NAME READY STATUS RESTARTS AGE liveness-exec 0/1 Running 1 1m    所以，假如一个 Pod 里只有一个容器，然后这个容器异常退出了。那么，只有当 restartPolicy=Never 时，这个 Pod 才会进入 Failed 状态。而其他情况下，由于 Kubernetes 都可以重启这个容器，所以 Pod 的状态保持 Running 不变。\n而如果这个 Pod 有多个容器，仅有一个容器异常退出，它就始终保持 Running 状态，哪怕即使 restartPolicy=Never。只有当所有容器也异常退出之后，这个 Pod 才会进入 Failed 状态。\n其他情况，都可以以此类推出来。\n现在，我们一起回到前面提到的 livenessProbe 上来。\n除了在容器中执行命令外，livenessProbe 也可以定义为发起 HTTP 或者 TCP 请求的方式，定义格式如下：\n... livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3  ... livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20  所以，你的 Pod 其实可以暴露一个健康检查 URL（比如 /healthz），或者直接让健康检查去检测应用的监听端口。这两种配置方法，在 Web 服务类的应用中非常常用。\n在 Kubernetes 的 Pod 中，还有一个叫 readinessProbe 的字段。虽然它的用法与 livenessProbe 类似，但作用却大不一样。readinessProbe 检查结果的成功与否，决定的这个 Pod 是不是能被通过 Service 的方式访问到，而并不影响 Pod 的生命周期。这部分内容，我会在讲解 Service 时重点介绍。\n关于容器健康检查的方案，可以参考本博客的文章容器健康检查方案。\n在讲解了这么多字段之后，想必你对 Pod 对象的语义和描述能力，已经有了一个初步的感觉。\n这时，你有没有产生这样一个想法：Pod 的字段这么多，我又不可能全记住，Kubernetes 能不能自动给 Pod 填充某些字段呢？\nPod 预设置 这个需求实际上非常实用。比如，开发人员只需要提交一个基本的、非常简单的 Pod YAML，Kubernetes 就可以自动给对应的 Pod 对象加上其他必要的信息，比如 labels，annotations，volumes 等等。而这些信息，可以是运维人员事先定义好的。\n这么一来，开发人员编写 Pod YAML 的门槛，就被大大降低了。\n所以，这个叫作 PodPreset（Pod 预设置）的功能 已经出现在了 v1.11 版本的 Kubernetes 中。\n举个例子，现在开发人员编写了如下一个 pod.yaml 文件：\napiVersion: v1 kind: Pod metadata: name: website labels: app: website role: frontend spec: containers: - name: website image: nginx ports: - containerPort: 80  作为 Kubernetes 的初学者，你肯定眼前一亮：这不就是我最擅长编写的、最简单的 Pod 嘛。没错，这个 YAML 文件里的字段，想必你现在闭着眼睛也能写出来。可是，如果运维人员看到了这个 Pod，他一定会连连摇头：这种 Pod 在生产环境里根本不能用啊！所以，这个时候，运维人员就可以定义一个 PodPreset 对象。在这个对象中，凡是他想在开发人员编写的 Pod 里追加的字段，都可以预先定义好。比如这个 preset.yaml：\napiVersion: settings.k8s.io/v1alpha1 kind: PodPreset metadata: name: allow-database spec: selector: matchLabels: role: frontend env: - name: DB_PORT value: \u0026quot;6379\u0026quot; volumeMounts: - mountPath: /cache name: cache-volume volumes: - name: cache-volume emptyDir: {}  在这个 PodPreset 的定义中，首先是一个 selector。这就意味着后面这些追加的定义，只会作用于 selector 所定义的、带有“role: frontend”标签的 Pod 对象，这就可以防止“误伤”。\n然后，我们定义了一组 Pod 的 Spec 里的标准字段，以及对应的值。比如，env 里定义了 DB_PORT 这个环境变量，volumeMounts 定义了容器 Volume 的挂载目录，volumes 定义了一个 emptyDir 的 Volume。\n接下来，我们假定运维人员先创建了这个 PodPreset，然后开发人员才创建 Pod：\n$ kubectl create -f preset.yaml $ kubectl create -f pod.yaml  这时，Pod 运行起来之后，我们查看一下这个 Pod 的 API 对象：\n$ kubectl get pod website -o yaml apiVersion: v1 kind: Pod metadata: name: website labels: app: website role: frontend annotations: podpreset.admission.kubernetes.io/podpreset-allow-database: \u0026quot;resource version\u0026quot; spec: containers: - name: website image: nginx volumeMounts: - mountPath: /cache name: cache-volume ports: - containerPort: 80 env: - name: DB_PORT value: \u0026quot;6379\u0026quot; volumes: - name: cache-volume emptyDir: {}  这个时候，我们就可以清楚地看到，这个 Pod 里多了新添加的 labels、env、volumes 和 volumeMount 的定义，它们的配置跟 PodPreset 的内容一样。此外，这个 Pod 还被自动加上了一个 annotation 表示这个 Pod 对象被 PodPreset 改动过。\n需要说明的是，PodPreset 里定义的内容，只会在 Pod API 对象被创建之前追加在这个对象本身上，而不会影响任何 Pod 的控制器的定义。\n比如，我们现在提交的是一个 nginx-deployment，那么这个 Deployment 对象本身是永远不会被 PodPreset 改变的，被修改的只是这个 Deployment 创建出来的所有 Pod。这一点请务必区分清楚。\n这里有一个问题：如果你定义了同时作用于一个 Pod 对象的多个 PodPreset，会发生什么呢？实际上，Kubernetes 项目会帮你合并（Merge）这两个 PodPreset 要做的修改。而如果它们要做的修改有冲突的话，这些冲突字段就不会被修改。\n总结\n在今天这篇文章中，我和你详细介绍了 Pod 对象更高阶的使用方法，希望通过对这些实例的讲解，你可以更深入地理解 Pod API 对象的各个字段。而在学习这些字段的同时，你还应该认真体会一下 Kubernetes“一切皆对象”的设计思想：比如应用是 Pod 对象，应用的配置是 ConfigMap 对象，应用要访问的密码则是 Secret 对象。\n所以，也就自然而然地有了 PodPreset 这样专门用来对 Pod 进行批量化、自动化修改的工具对象。在后面的内容中，我会为你讲解更多的这种对象，还会和你介绍 Kubernetes 项目如何围绕着这些对象进行容器编排。\n在本专栏中，Pod 对象相关的知识点非常重要，它是接下来 Kubernetes 能够描述和编排各种复杂应用的基石所在，希望你能够继续多实践、多体会。\n","date":"2021年02月27日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter3/pod-concept2/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文转自张磊老师的《深入剖析 Kubernetes》课程\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eProjectedVolume: Secret, ConfigMap, DownwadAPI\u003c/li\u003e\n\u003cli\u003eServiceAccount\u003c/li\u003e\n\u003cli\u003e容器健康检查和恢复机制\u003c/li\u003e\n\u003cli\u003ePod 预设置：PodPreset\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/blockquote\u003e","title":"深入解析Pod对象(二): 使用进阶"},{"contents":" 本文转自张磊老师的《深入剖析Kubernetes》课程，第14课时：深入解析Pod对象(一)：基本概念\n 现在，你已经非常清楚：Pod，而不是容器，才是 Kubernetes 项目中的最小编排单位。将这个设计落实到 API 对象上，容器（Container）就成了 Pod 属性里的一个普通的字段。那么，一个很自然的问题就是：到底哪些属性属于 Pod 对象，而又有哪些属性属于 Container 呢？\n要彻底理解这个问题，你就一定要牢记我在上一篇文章中提到的一个结论：Pod 扮演的是传统部署环境里“虚拟机”的角色。这样的设计，是为了使用户从传统环境（虚拟机环境）向 Kubernetes（容器环境）的迁移，更加平滑。而如果你能把 Pod 看成传统环境里的“机器”、把容器看作是运行在这个“机器”里的“用户程序”，那么很多关于 Pod 对象的设计就非常容易理解了。\n比如，凡是调度、网络、存储，以及安全相关的属性，基本上是 Pod 级别的。\n这些属性的共同特征是，它们描述的是“机器”这个整体，而不是里面运行的“程序”。比如，配置这个“机器”的网卡（即：Pod 的网络定义），配置这个“机器”的磁盘（即：Pod 的存储定义），配置这个“机器”的防火墙（即：Pod 的安全定义）。更不用说，这台“机器”运行在哪个服务器之上（即：Pod 的调度）。\n接下来，我就先为你介绍 Pod 中几个重要字段的含义和用法。\nNodeSelector：是一个供用户将 Pod 与 Node 进行绑定的字段，用法如下所示：\napiVersion: v1 kind: Pod ... spec: nodeSelector: disktype: ssd  这样的一个配置，意味着这个 Pod 永远只能运行在携带了“disktype: ssd”标签（Label）的节点上；否则，它将调度失败。\nNodeName：一旦 Pod 的这个字段被赋值，Kubernetes 项目就会被认为这个 Pod 已经经过了调度，调度的结果就是赋值的节点名字。所以，这个字段一般由调度器负责设置，但用户也可以设置它来“骗过”调度器，当然这个做法一般是在测试或者调试的时候才会用到。\nHostAliases：定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容，用法如下：\napiVersion: v1 kind: Pod ... spec: hostAliases: - ip: \u0026quot;10.1.2.3\u0026quot; hostnames: - \u0026quot;foo.remote\u0026quot; - \u0026quot;bar.remote\u0026quot; ...  在这个 Pod 的 YAML 文件中，我设置了一组 IP 和 hostname 的数据。这样，这个 Pod 启动后，/etc/hosts 文件的内容将如下所示：\ncat /etc/hosts # Kubernetes-managed hosts file. 127.0.0.1 localhost ... 10.244.135.10 hostaliases-pod 10.1.2.3 foo.remote 10.1.2.3 bar.remote  其中，最下面两行记录，就是我通过 HostAliases 字段为 Pod 设置的。需要指出的是，在 Kubernetes 项目中，如果要设置 hosts 文件里的内容，一定要通过这种方法。否则，如果直接修改了 hosts 文件的话，在 Pod 被删除重建之后，kubelet 会自动覆盖掉被修改的内容。\n除了上述跟“机器”相关的配置外，你可能也会发现，凡是跟容器的 Linux Namespace 相关的属性，也一定是 Pod 级别的。这个原因也很容易理解：Pod 的设计，就是要让它里面的容器尽可能多地共享 Linux Namespace，仅保留必要的隔离和限制能力。这样，Pod 模拟出的效果，就跟虚拟机里程序间的关系非常类似了。\n举个例子，在下面这个 Pod 的 YAML 文件中，我定义了 shareProcessNamespace=true：\napiVersion: v1 kind: Pod metadata: name: nginx spec: shareProcessNamespace: true containers: - name: nginx image: nginx - name: shell image: busybox stdin: true tty: true  这就意味着这个 Pod 里的容器要共享 PID Namespace。\n而在这个 YAML 文件中，我还定义了两个容器：一个是 nginx 容器，一个是开启了 tty 和 stdin 的 shell 容器。我在前面介绍容器基础时，曾经讲解过什么是 tty 和 stdin。而在 Pod 的 YAML 文件里声明开启它们俩，其实等同于设置了 docker run 里的 -it（-i 即 stdin，-t 即 tty）参数。*如果你还是不太理解它们俩的作用的话，可以直接认为 tty 就是 Linux 给用户提供的一个常驻小程序，用于接收用户的标准输入，返回操作系统的标准输出。*当然，为了能够在 tty 中输入信息，你还需要同时开启 stdin（标准输入流）。\n于是，这个 Pod 被创建后，你就可以使用 shell 容器的 tty 跟这个容器进行交互了。我们一起实践一下：\n$ kubectl create -f nginx.yaml  接下来，我们使用 kubectl attach 命令，连接到 shell 容器的 tty 上：\n$ kubectl attach -it nginx -c shell  这样，我们就可以在 shell 容器里执行 ps 指令，查看所有正在运行的进程：\n$ kubectl attach -it nginx -c shell / # ps ax PID USER TIME COMMAND 1 root 0:00 /pause 8 root 0:00 nginx: master process nginx -g daemon off; 14 101 0:00 nginx: worker process 15 root 0:00 sh 21 root 0:00 ps ax  可以看到，在这个容器里，我们不仅可以看到它本身的 ps ax 指令，还可以看到 nginx 容器的进程，以及 Infra 容器的 /pause 进程。这就意味着，整个 Pod 里的每个容器的进程，对于所有容器来说都是可见的：它们共享了同一个 PID Namespace。\n类似地，凡是 Pod 中的容器要共享宿主机的 Namespace，也一定是 Pod 级别的定义，比如：\napiVersion: v1 kind: Pod metadata: name: nginx spec: hostNetwork: true hostIPC: true hostPID: true containers: - name: nginx image: nginx - name: shell image: busybox stdin: true tty: true  在这个 Pod 中，我定义了共享宿主机的 Network、IPC 和 PID Namespace。这就意味着，这个 Pod 里的所有容器，会直接使用宿主机的网络、直接与宿主机进行 IPC 通信、看到宿主机里正在运行的所有进程。\n当然，除了这些属性，Pod 里最重要的字段当属“Containers”了。而在上一篇文章中，我还介绍过“Init Containers”。其实，这两个字段都属于 Pod 对容器的定义，内容也完全相同，只是 Init Containers 的生命周期，会先于所有的 Containers，并且严格按照定义的顺序执行。\nKubernetes 项目中对 Container 的定义，和 Docker 相比并没有什么太大区别。我在前面的容器技术概念入门系列文章中，和你分享的 Image（镜像）、Command（启动命令）、workingDir（容器的工作目录）、Ports（容器要开放的端口），以及 *volumeMounts（容器要挂载的 Volume）*都是构成 Kubernetes 项目中 Container 的主要字段。不过在这里，还有这么几个属性值得你额外关注。\n**首先，是 ImagePullPolicy 字段。**它定义了镜像拉取的策略。而它之所以是一个 Container 级别的属性，是因为容器镜像本来就是 Container 定义中的一部分。\nImagePullPolicy 的值默认是 Always，即每次创建 Pod 都重新拉取一次镜像。另外，当容器的镜像是类似于 nginx 或者 nginx:latest 这样的名字时，ImagePullPolicy 也会被认为 Always。\n而如果它的值被定义为 Never 或者 IfNotPresent，则意味着 Pod 永远不会主动拉取这个镜像，或者只在宿主机上不存在这个镜像时才拉取。\n其次，是 Lifecycle 字段。它定义的是 Container Lifecycle Hooks。顾名思义，Container Lifecycle Hooks 的作用，是在容器状态发生变化时触发一系列“钩子”。我们来看这样一个例子：\napiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [\u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;echo Hello from the postStart handler \u0026gt; /usr/share/message\u0026quot;] preStop: exec: command: [\u0026quot;/usr/sbin/nginx\u0026quot;,\u0026quot;-s\u0026quot;,\u0026quot;quit\u0026quot;]  这是一个来自 Kubernetes 官方文档的 Pod 的 YAML 文件。它其实非常简单，只是定义了一个 nginx 镜像的容器。不过，在这个 YAML 文件的容器（Containers）部分，你会看到这个容器分别设置了一个 postStart 和 preStop 参数。这是什么意思呢？\n先说 postStart 吧。它指的是，在容器启动后，立刻执行一个指定的操作。需要明确的是，postStart 定义的操作，虽然是在 Docker 容器 ENTRYPOINT 执行之后，但它并不严格保证顺序。也就是说，在 postStart 启动时，ENTRYPOINT 有可能还没有结束。\n当然，如果 postStart 执行超时或者错误，Kubernetes 会在该 Pod 的 Events 中报出该容器启动失败的错误信息，导致 Pod 也处于失败的状态。\n而类似地，preStop 发生的时机，则是容器被杀死之前（比如，收到了 SIGKILL 信号）。而需要明确的是，preStop 操作的执行，是同步的。所以，它会阻塞当前的容器杀死流程，直到这个 Hook 定义操作完成之后，才允许容器被杀死，这跟 postStart 不一样。\n所以，在这个例子中，我们在容器成功启动之后，在 /usr/share/message 里写入了一句“欢迎信息”（即 postStart 定义的操作）。而在这个容器被删除之前，我们则先调用了 nginx 的退出指令（即 preStop 定义的操作），从而实现了容器的“优雅退出”。\n在熟悉了 Pod 以及它的 Container 部分的主要字段之后，我再和你分享一下这样一个的 Pod 对象在 Kubernetes 中的生命周期。\nPod 生命周期的变化，主要体现在 Pod API 对象的 Status 部分，这是它除了 Metadata 和 Spec 之外的第三个重要字段。其中，pod.status.phase，就是 Pod 的当前状态，它有如下几种可能的情况：\n Pending。这个状态意味着，Pod 的 YAML 文件已经提交给了 Kubernetes，API 对象已经被创建并保存在 Etcd 当中。但是，这个 Pod 里有些容器因为某种原因而不能被顺利创建。比如，调度不成功。 Running。这个状态下，Pod 已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创建成功，并且至少有一个正在运行中。 Succeeded。这个状态意味着，Pod 里的所有容器都正常运行完毕，并且已经退出了。这种情况在运行一次性任务时最为常见。 Failed。这个状态下，Pod 里至少有一个容器以不正常的状态（非 0 的返回码）退出。这个状态的出现，意味着你得想办法 Debug 这个容器的应用，比如查看 Pod 的 Events 和日志。 Unknown。这是一个异常状态，意味着 Pod 的状态不能持续地被 kubelet 汇报给 kube-apiserver，这很有可能是主从节点（Master 和 Kubelet）间的通信出现了问题。  更进一步地，Pod 对象的 Status 字段，还可以再细分出一组 Conditions。这些细分状态的值包括：PodScheduled、Ready、Initialized，以及 Unschedulable。它们主要用于描述造成当前 Status 的具体原因是什么。\n比如，Pod 当前的 Status 是 Pending，对应的 Condition 是 Unschedulable，这就意味着它的调度出现了问题。\n而其中，Ready 这个细分状态非常值得我们关注：它意味着 Pod 不仅已经正常启动（Running 状态），而且已经可以对外提供服务了。这两者之间（Running 和 Ready）是有区别的，你不妨仔细思考一下。\nPod 的这些状态信息，是我们判断应用运行情况的重要标准，尤其是 Pod 进入了非“Running”状态后，你一定要能迅速做出反应，根据它所代表的异常情况开始跟踪和定位，而不是去手忙脚乱地查阅文档。\n总结\n在今天这篇文章中，我详细讲解了 Pod API 对象，介绍了 Pod 的核心使用方法，并分析了 Pod 和 Container 在字段上的异同。希望这些讲解能够帮你更好地理解和记忆 Pod YAML 中的核心字段，以及这些字段的准确含义。实际上，Pod API 对象是整个 Kubernetes 体系中最核心的一个概念，也是后面我讲解各种控制器时都要用到的。在学习完这篇文章后，我希望你能仔细阅读 $GOPATH/src/k8s.io/kubernetes/pkg/apis/core/types.go 里，type Pod struct ，尤其是 PodSpec 部分的内容。争取做到下次看到一个 Pod 的 YAML 文件时，不再需要查阅文档，就能做到把常用字段及其作用信手拈来。而在下一篇文章中，我会通过大量的实践，帮助你巩固和进阶关于 Pod API 对象核心字段的使用方法，敬请期待吧。\n","date":"2021年02月26日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter3/pod-concept1/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文转自张磊老师的《深入剖析Kubernetes》课程，第14课时：深入解析Pod对象(一)：基本概念\u003c/p\u003e\n\u003c/blockquote\u003e","title":"深入解析Pod对象(一)：基本概念"},{"contents":" 本文转自张磊老师的《深入剖析Kubernetes》课程，第13课时：为什么我们需要 Pod?\n Pod，是 Kubernetes 项目中最小的 API 对象。如果换一个更专业的说法，我们可以这样描述：Pod，是 Kubernetes 项目的原子调度单位。\n不过，我相信你在学习和使用 Kubernetes 项目的过程中，已经不止一次地想要问这样一个问题：为什么我们会需要 Pod？\n是啊，我们在前面已经花了很多精力去解读 Linux 容器的原理、分析了 Docker 容器的本质，终于，“Namespace 做隔离，Cgroups 做限制，rootfs 做文件系统”这样的“三句箴言”可以朗朗上口了，为什么 Kubernetes 项目又突然搞出一个 Pod 来呢？\n要回答这个问题，我们还是要一起回忆一下我曾经反复强调的一个问题：容器的本质到底是什么？\n你现在应该可以不假思索地回答出来：容器的本质是进程。没错。容器，就是未来云计算系统中的进程；容器镜像就是这个系统里的“.exe”安装包。那么 Kubernetes 呢？你应该也能立刻回答上来：Kubernetes 就是操作系统！\n现在，就让我们登录到一台 Linux 机器里，执行一条如下所示的命令：\n$ pstree -g  这条命令的作用，是展示当前系统中正在运行的进程的树状结构。它的返回结果如下所示：\nsystemd(1)-+-accounts-daemon(1984)-+-{gdbus}(1984) | `-{gmain}(1984) |-acpid(2044) ... |-lxcfs(1936)-+-{lxcfs}(1936) | `-{lxcfs}(1936) |-mdadm(2135) |-ntpd(2358) |-polkitd(2128)-+-{gdbus}(2128) | `-{gmain}(2128) |-rsyslogd(1632)-+-{in:imklog}(1632) | |-{in:imuxsock) S 1(1632) | `-{rs:main Q:Reg}(1632) |-snapd(1942)-+-{snapd}(1942) | |-{snapd}(1942) | |-{snapd}(1942) | |-{snapd}(1942) | |-{snapd}(1942)  不难发现，在一个真正的操作系统里，进程并不是“孤苦伶仃”地独自运行的，而是以进程组的方式，“有原则地”组织在一起。比如，这里有一个叫作 rsyslogd 的程序，它负责的是 Linux 操作系统里的日志处理。可以看到，rsyslogd 的主程序 main，和它要用到的内核日志模块 imklog 等，同属于 1632 进程组。这些进程相互协作，共同完成 rsyslogd 程序的职责。\n 注意：我在本篇中提到的“进程”，比如，rsyslogd 对应的 imklog，imuxsock 和 main，严格意义上来说，其实是 Linux 操作系统语境下的“线程”。这些线程，或者说，轻量级进程之间，可以共享文件、信号、数据内存、甚至部分代码，从而紧密协作共同完成一个程序的职责。所以同理，我提到的“进程组”，对应的也是 Linux 操作系统语境下的“线程组”。这种命名关系与实际情况的不一致，是 Linux 发展历史中的一个遗留问题。对这个话题感兴趣的同学，可以阅读这篇技术文章来了解一下。\n 而 Kubernetes 项目所做的，其实就是将“进程组”的概念映射到了容器技术中，并使其成为了这个云计算“操作系统”里的“一等公民”。\nKubernetes 项目之所以要这么做的原因，我在前面介绍 Kubernetes 和 Borg 的关系时曾经提到过：在 Borg 项目的开发和实践过程中，Google 公司的工程师们发现，他们部署的应用，往往都存在着类似于“进程和进程组”的关系。更具体地说，就是这些应用之间有着密切的协作关系，使得它们必须部署在同一台机器上。\n而如果事先没有“组”的概念，像这样的运维关系就会非常难以处理。\n我还是以前面的 rsyslogd 为例子。已知 rsyslogd 由三个进程组成：一个 imklog 模块，一个 imuxsock 模块，一个 rsyslogd 自己的 main 函数主进程。这三个进程一定要运行在同一台机器上，否则，它们之间基于 Socket 的通信和文件交换，都会出现问题。\n现在，我要把 rsyslogd 这个应用给容器化，由于受限于容器的“单进程模型”，这三个模块必须被分别制作成三个不同的容器。而在这三个容器运行的时候，它们设置的内存配额都是 1 GB。\n 再次强调一下：**容器的“单进程模型”，并不是指容器里只能运行“一个”进程，而是指容器没有管理多个进程的能力。**这是因为容器里 PID=1 的进程就是应用本身，其他的进程都是这个 PID=1 进程的子进程。可是，用户编写的应用，并不能够像正常操作系统里的 init 进程或者 systemd 那样拥有进程管理的功能。比如，你的应用是一个 Java Web 程序（PID=1），然后你执行 docker exec 在后台启动了一个 Nginx 进程（PID=3）。可是，当这个 Nginx 进程异常退出的时候，你该怎么知道呢？这个进程退出后的垃圾收集工作，又应该由谁去做呢？\n这里其实有一些开源的软件做了这些事情，例如具有管理子进程能力的dumb-init，具有服务管理能力的 runit。使用这些工具我们也能实现一个容器内运行多个进程的能力。\n 假设我们的 Kubernetes 集群上有两个节点：node-1 上有 3 GB 可用内存，node-2 有 2.5 GB 可用内存。\n这时，假设我要用 Docker Swarm 来运行这个 rsyslogd 程序。为了能够让这三个容器都运行在同一台机器上，我就必须在另外两个容器上设置一个 affinity=main（与 main 容器有亲密性）的约束，即：它们俩必须和 main 容器运行在同一台机器上。\n然后，我顺序执行：“docker run main”“docker run imklog”和“docker run imuxsock”，创建这三个容器。\n这样，这三个容器都会进入 Swarm 的待调度队列。然后，main 容器和 imklog 容器都先后出队并被调度到了 node-2 上（这个情况是完全有可能的）\n可是，当 imuxsock 容器出队开始被调度时，Swarm 就有点懵了：node-2 上的可用资源只有 0.5 GB 了，并不足以运行 imuxsock 容器；可是，根据 affinity=main 的约束，imuxsock 容器又只能运行在 node-2 上。\n这就是一个典型的成组调度（gang scheduling）没有被妥善处理的例子。\n在工业界和学术界，关于这个问题的讨论可谓旷日持久，也产生了很多可供选择的解决方案。\n比如，Mesos 中就有一个资源囤积（resource hoarding）的机制，*会在所有设置了 Affinity 约束的任务都达到时，才开始对它们统一进行调度。*而在 Google Omega 论文中，则提出了使用乐观调度处理冲突的方法，即：先不管这些冲突，而是通过精心设计的回滚机制在出现了冲突之后解决问题。\n但是，到了 Kubernetes 项目里，这样的问题就迎刃而解了：Pod 是 Kubernetes 里的原子调度单位。这就意味着，Kubernetes 项目的调度器，是统一按照 Pod 而非容器的资源需求进行计算的。\n所以，像 imklog、imuxsock 和 main 函数主进程这样的三个容器，正是一个典型的由三个容器组成的 Pod。Kubernetes 项目在调度时，自然就会去选择可用内存等于 3 GB 的 node-1 节点进行绑定，而根本不会考虑 node-2。\n像这样容器间的紧密协作，我们可以称为**“超亲密关系”**。这些具有“超亲密关系”容器的典型特征包括但不限于：互相之间会发生直接的文件交换、使用 localhost 或者 Socket 文件进行本地通信、会发生非常频繁的远程调用、需要共享某些 Linux Namespace（比如，一个容器要加入另一个容器的 Network Namespace）等等。\n这也就意味着，并不是所有有“关系”的容器都属于同一个 Pod。比如，PHP 应用容器和 MySQL 虽然会发生访问关系，但并没有必要、也不应该部署在同一台机器上，它们更适合做成两个 Pod。\n不过，相信此时你可能会有第二个疑问：\n对于初学者来说，一般都是先学会了用 Docker 这种单容器的工具，才会开始接触 Pod。\n而如果 Pod 的设计只是出于调度上的考虑，那么 Kubernetes 项目似乎完全没有必要非得把 Pod 作为“一等公民”吧？这不是故意增加用户的学习门槛吗？没错，如果只是处理“超亲密关系”这样的调度问题，有 Borg 和 Omega 论文珠玉在前，Kubernetes 项目肯定可以在调度器层面给它解决掉。\n不过，Pod 在 Kubernetes 项目里还有更重要的意义，那就是：容器设计模式。\n为了理解这一层含义，我就必须先给你介绍一下Pod 的实现原理。\n首先，关于 Pod 最重要的一个事实是：它只是一个逻辑概念。\n也就是说，Kubernetes 真正处理的，还是宿主机操作系统上 Linux 容器的 Namespace 和 Cgroups，而并不存在一个所谓的 Pod 的边界或者隔离环境。那么，Pod 又是怎么被“创建”出来的呢？答案是：Pod，其实是一组共享了某些资源的容器。\n具体的说：Pod 里的所有容器，共享的是同一个 Network Namespace，并且可以声明共享同一个 Volume。\n那这么来看的话，一个有 A、B 两个容器的 Pod，不就是等同于一个容器（容器 A）共享另外一个容器（容器 B）的网络和 Volume 的玩儿法么？\n这好像通过 docker run \u0026ndash;net \u0026ndash;volumes-from 这样的命令就能实现嘛，比如：\n$ docker run --net=B --volumes-from=B --name=A image-A ...  但是，你有没有考虑过，如果真这样做的话，容器 B 就必须比容器 A 先启动，这样一个 Pod 里的多个容器就不是对等关系，而是拓扑关系了。\n所以，在 Kubernetes 项目里，Pod 的实现需要使用一个中间容器，这个容器叫作 Infra 容器。在这个 Pod 中，Infra 容器永远都是第一个被创建的容器，而其他用户定义的容器，则通过 Join Network Namespace 的方式，与 Infra 容器关联在一起。这样的组织关系，可以用下面这样一个示意图来表达：\n如上图所示，这个 Pod 里有两个用户容器 A 和 B，还有一个 Infra 容器。很容易理解，在 Kubernetes 项目里，Infra 容器一定要占用极少的资源，所以它使用的是一个非常特殊的镜像，叫作：k8s.gcr.io/pause。这个镜像是一个用汇编语言编写的、永远处于“暂停”状态的容器，解压后的大小也只有 100~200 KB 左右。\n而在 Infra 容器“Hold 住”Network Namespace 后，用户容器就可以加入到 Infra 容器的 Network Namespace 当中了。所以，如果你查看这些容器在宿主机上的 Namespace 文件（这个 Namespace 文件的路径，我已经在前面的内容中介绍过），它们指向的值一定是完全一样的。\n这也就意味着，对于 Pod 里的容器 A 和容器 B 来说：\n 它们可以直接使用 localhost 进行通信； 它们看到的网络设备跟 Infra 容器看到的完全一样； 一个 Pod 只有一个 IP 地址，也就是这个 Pod 的 Network Namespace 对应的 IP 地址； 当然，其他的所有网络资源，都是一个 Pod 一份，并且被该 Pod 中的所有容器共享； Pod 的生命周期只跟 Infra 容器一致，而与容器 A 和 B 无关。  而对于同一个 Pod 里面的所有用户容器来说，它们的进出流量，也可以认为都是通过 Infra 容器完成的。这一点很重要，因为将来如果你要为 Kubernetes 开发一个网络插件时，应该重点考虑的是如何配置这个 Pod 的 Network Namespace，而不是每一个用户容器如何使用你的网络配置，这是没有意义的。\n这就意味着，如果你的网络插件需要在容器里安装某些包或者配置才能完成的话，是不可取的：Infra 容器镜像的 rootfs 里几乎什么都没有，没有你随意发挥的空间。当然，这同时也意味着你的网络插件完全不必关心用户容器的启动与否，而只需要关注如何配置 Pod，也就是 Infra 容器的 Network Namespace 即可。\n有了这个设计之后，共享 Volume 就简单多了：Kubernetes 项目只要把所有 Volume 的定义都设计在 Pod 层级即可。\n这样，一个 Volume 对应的宿主机目录对于 Pod 来说就只有一个，Pod 里的容器只要声明挂载这个 Volume，就一定可以共享这个 Volume 对应的宿主机目录。比如下面这个例子：\napiVersion: v1 kind: Pod metadata: name: two-containers spec: restartPolicy: Never volumes: - name: shared-data hostPath: path: /data containers: - name: nginx-container image: nginx volumeMounts: - name: shared-data mountPath: /usr/share/nginx/html - name: debian-container image: debian volumeMounts: - name: shared-data mountPath: /pod-data command: [\u0026quot;/bin/sh\u0026quot;] args: [\u0026quot;-c\u0026quot;, \u0026quot;echo Hello from the debian container \u0026gt; /pod-data/admin.html\u0026quot;]  在这个例子中，debian-container 和 nginx-container 都声明挂载了 shared-data 这个 Volume。而 shared-data 是 hostPath 类型。所以，它对应在宿主机上的目录就是：/data。而这个目录，其实就被同时绑定挂载进了上述两个容器当中。\n这就是为什么，nginx-container 可以从它的 /usr/share/nginx/html 目录中，读取到 debian-container 生成的 index.html 文件的原因。\n明白了 Pod 的实现原理后，我们再来讨论“容器设计模式”，就容易多了。\nPod 这种“超亲密关系”容器的设计思想，实际上就是希望，当用户想在一个容器里跑多个功能并不相关的应用时，应该优先考虑它们是不是更应该被描述成一个 Pod 里的多个容器。为了能够掌握这种思考方式，你就应该尽量尝试使用它来描述一些用单个容器难以解决的问题。\n第一个最典型的例子是：WAR 包与 Web 服务器。\n我们现在有一个 Java Web 应用的 WAR 包，它需要被放在 Tomcat 的 webapps 目录下运行起来。\n假如，你现在只能用 Docker 来做这件事情，那该如何处理这个组合关系呢？\n 一种方法是，把 WAR 包直接放在 Tomcat 镜像的 webapps 目录下，做成一个新的镜像运行起来。可是，这时候，如果你要更新 WAR 包的内容，或者要升级 Tomcat 镜像，就要重新制作一个新的发布镜像，非常麻烦。 另一种方法是，你压根儿不管 WAR 包，永远只发布一个 Tomcat 容器。不过，这个容器的 webapps 目录，就必须声明一个 hostPath 类型的 Volume，从而把宿主机上的 WAR 包挂载进 Tomcat 容器当中运行起来。不过，这样你就必须要解决一个问题，即：如何让每一台宿主机，都预先准备好这个存储有 WAR 包的目录呢？这样来看，你只能独立维护一套分布式存储系统了。  实际上，有了 Pod 之后，这样的问题就很容易解决了。我们可以把 WAR 包和 Tomcat 分别做成镜像，然后把它们作为一个 Pod 里的两个容器“组合”在一起。这个 Pod 的配置文件如下所示：\napiVersion: v1 kind: Pod metadata: name: javaweb-2 spec: initContainers: - image: geektime/sample:v2 name: war command: [\u0026quot;cp\u0026quot;, \u0026quot;/sample.war\u0026quot;, \u0026quot;/app\u0026quot;] volumeMounts: - mountPath: /app name: app-volume containers: - image: geektime/tomcat:7.0 name: tomcat command: [\u0026quot;sh\u0026quot;,\u0026quot;-c\u0026quot;,\u0026quot;/root/apache-tomcat-7.0.42-v2/bin/start.sh\u0026quot;] volumeMounts: - mountPath: /root/apache-tomcat-7.0.42-v2/webapps name: app-volume ports: - containerPort: 8080 hostPort: 8001 volumes: - name: app-volume emptyDir: {}  在这个 Pod 中，我们定义了两个容器，第一个容器使用的镜像是 geektime/sample:v2，这个镜像里只有一个 WAR 包（sample.war）放在根目录下。而第二个容器则使用的是一个标准的 Tomcat 镜像。\n不过，你可能已经注意到，WAR 包容器的类型不再是一个普通容器，而是一个 Init Container 类型的容器。\n在 Pod 中，所有 Init Container 定义的容器，都会比 spec.containers 定义的用户容器先启动。并且，Init Container 容器会按顺序逐一启动，而直到它们都启动并且退出了，用户容器才会启动。\n所以，这个 Init Container 类型的 WAR 包容器启动后，我执行了一句\u0026quot;cp /sample.war /app\u0026quot;，把应用的 WAR 包拷贝到 /app 目录下，然后退出。而后这个 /app 目录，就挂载了一个名叫 app-volume 的 Volume。接下来就很关键了。Tomcat 容器，同样声明了挂载 app-volume 到自己的 webapps 目录下。所以，等 Tomcat 容器启动时，它的 webapps 目录下就一定会存在 sample.war 文件：这个文件正是 WAR 包容器启动时拷贝到这个 Volume 里面的，而这个 Volume 是被这两个容器共享的。\n像这样，我们就用一种“组合”方式，解决了 WAR 包与 Tomcat 容器之间耦合关系的问题。\n实际上，这个所谓的“组合”操作，正是容器设计模式里最常用的一种模式，它的名字叫：sidecar。\n顾名思义，sidecar 指的就是我们可以在一个 Pod 中，启动一个辅助容器，来完成一些独立于主进程（主容器）之外的工作。\n比如，在我们的这个应用 Pod 中，Tomcat 容器是我们要使用的主容器，而 WAR 包容器的存在，只是为了给它提供一个 WAR 包而已。所以，我们用 Init Container 的方式优先运行 WAR 包容器，扮演了一个 sidecar 的角色。\n第二个例子，则是容器的日志收集。\n比如，我现在有一个应用，需要不断地把日志文件输出到容器的 /var/log 目录中。\n这时，我就可以把一个 Pod 里的 Volume 挂载到应用容器的 /var/log 目录上。然后，我在这个 Pod 里同时运行一个 sidecar 容器，它也声明挂载同一个 Volume 到自己的 /var/log 目录上。这样，接下来 sidecar 容器就只需要做一件事儿，那就是不断地从自己的 /var/log 目录里读取日志文件，转发到 MongoDB 或者 Elasticsearch 中存储起来。这样，一个最基本的日志收集工作就完成了。跟第一个例子一样，这个例子中的 sidecar 的主要工作也是使用共享的 Volume 来完成对文件的操作。\n但不要忘记，Pod 的另一个重要特性是，它的所有容器都共享同一个 Network Namespace。这就使得很多与 Pod 网络相关的配置和管理，也都可以交给 sidecar 完成，而完全无须干涉用户容器。这里最典型的例子莫过于 Istio 这个微服务治理项目了。\n总结\n在本篇文章中我重点分享了 Kubernetes 项目中 Pod 的实现原理。\nPod 是 Kubernetes 项目与其他单容器项目相比最大的不同，也是一位容器技术初学者需要面对的第一个与常规认知不一致的知识点。事实上，直到现在，仍有很多人把容器跟虚拟机相提并论，他们把容器当做性能更好的虚拟机，喜欢讨论如何把应用从虚拟机无缝地迁移到容器中。但实际上，无论是从具体的实现原理，还是从使用方法、特性、功能等方面，容器与虚拟机几乎没有任何相似的地方；也不存在一种普遍的方法，能够把虚拟机里的应用无缝迁移到容器中。因为，容器的性能优势，必然伴随着相应缺陷，即：它不能像虚拟机那样，完全模拟本地物理机环境中的部署方法。所以，这个“上云”工作的完成，最终还是要靠深入理解容器的本质，即：进程。\n实际上，一个运行在虚拟机里的应用，哪怕再简单，也是被管理在 systemd 或者 supervisord 之下的一组进程，而不是一个进程。这跟本地物理机上应用的运行方式其实是一样的。这也是为什么，从物理机到虚拟机之间的应用迁移，往往并不困难。\n可是对于容器来说，一个容器永远只能管理一个进程。更确切地说，一个容器，就是一个进程。这是容器技术的“天性”，不可能被修改。所以，将一个原本运行在虚拟机里的应用，“无缝迁移”到容器中的想法，实际上跟容器的本质是相悖的。\n这也是当初 Swarm 项目无法成长起来的重要原因之一：一旦到了真正的生产环境上，Swarm 这种单容器的工作方式，就难以描述真实世界里复杂的应用架构了。\n所以，你现在可以这么理解 Pod 的本质：\n Pod，实际上是在扮演传统基础设施里“虚拟机”的角色；而容器，则是这个虚拟机里运行的用户程序。\n 所以下一次，当你需要把一个运行在虚拟机里的应用迁移到 Docker 容器中时，一定要仔细分析到底有哪些进程（组件）运行在这个虚拟机里。\n然后，你就可以把整个虚拟机想象成为一个 Pod，把这些进程分别做成容器镜像，把有顺序关系的容器，定义为 Init Container。这才是更加合理的、松耦合的容器编排诀窍，也是从传统应用架构，到“微服务架构”最自然的过渡方式。\n 注意：Pod 这个概念，提供的是一种编排思想，而不是具体的技术方案。所以，如果愿意的话，你完全可以使用虚拟机来作为 Pod 的实现，然后把用户容器都运行在这个虚拟机里。比如，Mirantis 公司的virtlet 项目就在干这个事情。甚至，你可以去实现一个带有 Init 进程的容器项目，来模拟传统应用的运行方式。这些工作，在 Kubernetes 中都是非常轻松的，也是我们后面讲解 CRI 时会提到的内容。\n 相反的，如果强行把整个应用塞到一个容器里，甚至不惜使用 Docker In Docker 这种在生产环境中后患无穷的解决方案，恐怕最后往往会得不偿失。\n补充内容\n1、绑定挂载（bind mount）\n 允许用户将一个目录或者文件，挂载到一个指定的目录上，并且，之后在这个挂载点上的操作，只发生在被挂载的目录或者文件上，而原来挂载点的内容会被隐藏起来不受影响。\n 它的主要作用就是，允许你将一个目录或者文件，而不是整个设备，挂载到一个指定的目录上。并且，这时你在该挂载点上进行的任何操作，只是发生在被挂载的目录或者文件上，而原挂载点的内容则会被隐藏起来且不受影响。\n绑定挂载其实是一个inode替换的过程，在Linux操作系统中，inode可以理解为存放文件内容的对象，而dentry，也叫目录项，就是访问这个inode所使用的“指针”。 在下图中，mount \u0026ndash;bind /home /test ，会把/home挂载到/test上，实际上相当于吧/test的/dentry，指向修改为/home的inode，这样修改/test，实际上修改的是/home对应的inode。\n正如上图所示，mount \u0026ndash;bind /home /test，会将 /home 挂载到 /test 上。其实相当于将 /test 的 dentry，重定向到了 /home 的 inode。这样当我们修改 /test 目录时，实际修改的是 /home 目录的 inode。这也就是为何，一旦执行 umount 命令，/test 目录原先的内容就会恢复：因为修改真正发生在的，是 /home 目录里。\n所以，在一个正确的时机，进行一次绑定挂载，Docker 就可以成功地将一个宿主机上的目录或文件，不动声色地挂载到容器中。\n这样，进程在容器里对这个 /test 目录进行的所有操作，都实际发生在宿主机的对应目录（比如，/home，或者 /var/lib/docker/volumes/[VOLUME_ID]/_data）里，而不会影响容器镜像的内容。\n参考这篇文章docker 之挂载\n2、Pause container\nalmighty-pause-container\n","date":"2021年02月26日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter3/why-pod/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文转自张磊老师的《深入剖析Kubernetes》课程，第13课时：为什么我们需要 Pod?\u003c/p\u003e\n\u003c/blockquote\u003e","title":"为什么我们需要Pod"},{"contents":"1. yaml 文件 1.1 创建 $ kubectl create -f 我的配置文件  1.2 修改 更新 yaml 文件：\n$ kubectl replace -f nginx-deployment.yaml  声明式的表达方式，使用 apply 命令：\n$ kubectl apply -f nginx-deployment.yaml  2. Pod 2.1 查询 获取 pod 的信息\nkubectl get pods -n ${namespace}  **根据 pod 的标签过滤 **\nkubectl get pods -l app=nginx  注意的是，在命令行中，所有 key-value 格式的参数，都使用“=”而非“:”表示。\n获取 pod 的描述信息\n$ kubectl describe pod \u0026lt;pod-name\u0026gt;  2.2 登录 进入到 Pod 中：\n$ kubectl exec -it nginx-deployment-5c678cfb6d-lg9lw -- /bin/bash  2.3 删除 删除 Pod：\n$ kubectl delete -f nginx-deployment.yaml  2.4. 更新 通过修改yaml文件更新pod\n... spec: containers: - name: nginx image: nginx:1.8 #这里被从1.7.9修改为1.8 ports: - containerPort: 80  执行 kubectl replace 命令\n$ kubectl replace -f nginx-deployment.yaml  这里更推荐用 kubectl apply 命令，来统一进行 Kubernetes 对象的创建和更新操作\n# 修改nginx-deployment.yaml的内容 $ kubectl apply -f nginx-deployment.yaml  2.5 创建一个临时的Pod $ kubectl run -i --tty --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh  通过这条命令，我们启动了一个一次性的 Pod，因为–rm 意味着 Pod 退出后就会被删除掉。\n3. 日志 3.1. kube-apiserver 日志 PODNAME=$(kubectl -n kube-system get pod -l component=kube-apiserver -o jsonpath='{.items[0].metadata.name}') kubectl -n kube-system logs $PODNAME --tail 100  以上命令操作假设控制平面以 Kubernetes 静态 Pod 的形式来运行。如果 kube-apiserver 是用 systemd 管理的，则需要登录到 master 节点上，然后使用 journalctl -u kube-apiserver 查看其日志。\n3.2. kube-controller-manager 日志 PODNAME=$(kubectl -n kube-system get pod -l component=kube-controller-manager -o jsonpath='{.items[0].metadata.name}') kubectl -n kube-system logs $PODNAME --tail 100  以上命令操作假设控制平面以 Kubernetes 静态 Pod 的形式来运行。如果 kube-controller-manager 是用 systemd 管理的，则需要登录到 master 节点上，然后使用 journalctl -u kube-controller-manager 查看其日志。\n3.3. kube-scheduler 日志 PODNAME=$(kubectl -n kube-system get pod -l component=kube-scheduler -o jsonpath='{.items[0].metadata.name}') kubectl -n kube-system logs $PODNAME --tail 100  以上命令操作假设控制平面以 Kubernetes 静态 Pod 的形式来运行。如果 kube-scheduler 是用 systemd 管理的，则需要登录到 master 节点上，然后使用 journalctl -u kube-scheduler 查看其日志。\n3.4. kubelet 日志 journalctl -l -u kubelet  3.5. Pod 日志 查看指定Pod的日志\nkubectl logs \u0026lt;pod-name\u0026gt; -n \u0026lt;namespace\u0026gt; # 类似 tail -f 的日志 kubectl logs -f \u0026lt;pod-name\u0026gt; -n \u0026lt;namespace\u0026gt;  例子：\n# Return snapshot logs from pod nginx with only one container kubectl logs nginx # Return snapshot logs from pod nginx with multi containers kubectl logs nginx --all-containers=true # Return snapshot logs from all containers in pods defined by label app=nginx kubectl logs -lapp=nginx --all-containers=true # Return snapshot of previous terminated ruby container logs from pod web-1 kubectl logs -p -c ruby web-1 # Begin streaming the logs of the ruby container in pod web-1 kubectl logs -f -c ruby web-1 # Begin streaming the logs from all containers in pods defined by label app=nginx kubectl logs -f -lapp=nginx --all-containers=true # Display only the most recent 20 lines of output in pod nginx kubectl logs --tail=20 nginx # Show all logs from pod nginx written in the last hour kubectl logs --since=1h nginx # Show logs from a kubelet with an expired serving certificate kubectl logs --insecure-skip-tls-verify-backend nginx # Return snapshot logs from first container of a job named hello kubectl logs job/hello # Return snapshot logs from container nginx-1 of a deployment named nginx kubectl logs deployment/nginx -c nginx-1  4. 污点 一旦某个节点被加上了一个 Taint，即被“打上了污点”，那么所有 Pod 就都不能在这个节点上运行，因为 Kubernetes 的 Pod 都有“洁癖”。\n除非，有个别的 Pod 声明自己能“容忍”这个“污点”，即声明了 Toleration，它才可以在这个节点上运行。\n为节点打污点(Taint)的命令是：\n$ kubectl taint nodes node1 foo=bar:NoSchedule  这时，该 node1 节点上就会增加一个键值对格式的 Taint，即：foo=bar:NoSchedule。其中值里面的 NoSchedule，意味着这个 Taint 只会在调度新 Pod 时产生作用，而不会影响已经在 node1 上运行的 Pod，哪怕它们没有 Toleration。\n那么 Pod 又如何声明 Toleration 呢？\n我们只要在 Pod 的.yaml 文件中的 spec 部分，加入 tolerations 字段即可：\napiVersion: v1 kind: Pod ... spec: tolerations: - key: \u0026quot;foo\u0026quot; operator: \u0026quot;Equal\u0026quot; value: \u0026quot;bar\u0026quot; effect: \u0026quot;NoSchedule\u0026quot;  这个 Toleration 的含义是，这个 Pod 能“容忍”所有键值对为 foo=bar 的 Taint（ operator: “Equal”，“等于”操作）。\n通常，master 节点上会自带一个污点：\n$ kubectl describe node master Name: master Roles: master Taints: node-role.kubernetes.io/master:NoSchedule  可以看到，Master 节点默认被加上了node-role.kubernetes.io/master:NoSchedule这样一个“污点”，其中“键”是node-role.kubernetes.io/master，而没有提供“值”。\n此时，你就需要像下面这样用“Exists”操作符（operator: “Exists”，“存在”即可）来说明，该 Pod 能够容忍所有以 foo 为键的 Taint，才能让这个 Pod 运行在该 Master 节点上：\napiVersion: v1 kind: Pod ... spec: tolerations: - key: \u0026quot;foo\u0026quot; operator: \u0026quot;Exists\u0026quot; effect: \u0026quot;NoSchedule\u0026quot;  当然，如果你就是想要一个单节点的 Kubernetes，删除这个 Taint 才是正确的选择：\n$ kubectl taint nodes --all node-role.kubernetes.io/master-  如上所示，我们在“node-role.kubernetes.io/master”这个键后面加上了一个短横线“-”，这个格式就意味着移除所有以“node-role.kubernetes.io/master”为键的 Taint。\n5. ProjectedVolume 5.1 Secret 5.1.1. 创建 kubectl create secret generic user --from-file=./username.txt  5.1.2 查询 kubectl get secrets  5.2. ConfigMap 5.2.1. 创建 $ kubectl create configmap \u0026lt;configmap-na,e\u0026gt; --from-file=\u0026lt;file-name\u0026gt;  5.22. 查询 kubectl get configmaps \u0026lt;configmap-name\u0026gt; -o yaml  ","date":"2021年02月25日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter9/commands/","summary":"1. yaml 文件 1.1 创建 $ kubectl create -f 我的配置文件 1.2 修改 更新 yaml 文件： $ kubectl replace -f nginx-deployment.yaml 声明式的表达方式，使用 apply 命令： $ kubectl apply -f nginx-deployment.yaml 2. Pod 2.1 查询 获取 pod 的信息 kubectl get pods -n ${namespace} **根","title":"常用的k8s命令"},{"contents":"1. 准备工作 机器配置  8核CPU、8GB内存； 40GB磁盘 centos 7.9 内网互同 外网访问不受限制  组件信息    组件 版本     系统 Centos 7.9   Docker版本 18.09.9   k8s 版本 1.20.0   Pod 网段 10.32.0.0/    实践目标  在所有节点上安装 Docker 和 kubeadm； 部署 Kubernetes Master； 部署容器网络插件； 部署 Kubernetes Worker； 部署 Dashboard 可视化插件； 部署容器存储插件  基本配置 开始安装之前，我们还需要对系统做一些基本的配置。\n所有节点配置 hosts\n# cat /etc/hosts 10.186.4.100 master 10.186.4.167 node1 10.186.4.168 node2 10.186.4.169 node3  所有节点关闭防火墙、selinux、dnsmasq\nsystemctl disable --now firewalld #关闭dnsmasq(否则可能导致docker容器无法解析域名) systemctl disable --now dnsmasq systemctl disable --now NetworkManager # Set SELinux in permissive mode (effectively disabling it) sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config  关闭swap\nswapoff -a \u0026amp;\u0026amp; sysctl -w vm.swappiness=0 sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab  允许iptales查看网桥流量\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system  安装ntpdate\nrpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm yum install ntpdate -y  同步时间\nln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime echo 'Asia/Shanghai' \u0026gt;/etc/timezone ntpdate time2.aliyun.com  加入到 crontab\n*/5 * * * * ntpdate time2.aliyun.com  节点之间免密登录\nssh-keygen -t rsa for i in master node1 node2 node3;do ssh-copy-id -i .ssh/id_rsa.pub $i;done  开放端口\n网络插件weave的端口是6783。\n2. 安装 kubeadm 和 docker docker 安装  安装过程参考docker install\n yum源配置\nyum install -y yum-utils # 官方源 sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 阿里云源 yum-config-manager \\ --add-repo \\ https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo  如果是安装最新版本的 docker，可以直接安装:\n$ sudo yum install docker-ce docker-ce-cli containerd.io  指定版本安装的话，先查看源中可用的docker版本：\n$ yum list docker-ce --showduplicates | sort -r docker-ce.x86_64 3:18.09.1-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.0-3.el7 docker-ce-stable docker-ce.x86_64 18.06.1.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.0.ce-3.el7 docker-ce-stable  通过软件包名称安装特定版本，软件包名称是软件包名称（docker-ce）加上版本字符串（第二列），从第一个冒号（:)开始，直至第一个连字符，并用连字符（-）连接。例如docker-ce-18.09.9。\n$ sudo yum install -y docker-ce-\u0026lt;VERSION_STRING\u0026gt; docker-ce-cli-\u0026lt;VERSION_STRING\u0026gt; containerd.io  设置开机启动\n$ sudo systemctl start docker $ systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now docker  kubeadm  安装过程参考 Installing kubeadm\n 添加 yum 源，执行安装命令。\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF # 阿里云源 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF sed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes sudo systemctl enable --now kubelet  在上述安装 kubeadm 的过程中，kubeadm 和 kubelet、kubectl、kubernetes-cni 这几个二进制文件都会被自动安装好。\n安装的时候指定版本：\nsudo yum install -y kubelet-1.20.0-0 kubeadm-1.20.0-0 kubectl-1.20.0-0 --disableexcludes=kubernetes  kubectl 命令启用 shell 自动补齐功能\n# 1.安装bash-completion yum install bash-completion # 重新加载你的 Shell 并运行 type _init_completion type _init_completion # 2.启动 kubectl 自动补齐 echo 'source \u0026lt;(kubectl completion bash)' \u0026gt;\u0026gt;~/.bashrc source ~/.bashrc  开机自启动\nsystemctl daemon-reload \u0026amp;\u0026amp; systemctl enable --now kubelet  3. 部署kubernetes 的master节点 kubeadm 初始化的时候可以通过命令行参数或者配置文件的方式进行。我们可以通过 kubeadm config 命令来查看 kubeadm 的配置。\n初始化需要的镜像可以通过kubeadm config images list 来查看：\n# kubeadm config images list k8s.gcr.io/kube-apiserver:v1.20.4 k8s.gcr.io/kube-controller-manager:v1.20.4 k8s.gcr.io/kube-scheduler:v1.20.4 k8s.gcr.io/kube-proxy:v1.20.4 k8s.gcr.io/pause:3.2 k8s.gcr.io/etcd:3.4.13-0 k8s.gcr.io/coredns:1.7.0  为了减少初始化的时间，可以用 kubeadm config images pull 命令提前拉取镜像，获取镜像的时候，默认使用的 k8s.gcr.io 这个镜像源，国内是下载不了的。有两个解决办法：\n  使用国内的阿里云镜像源：\ncat \u0026gt;/etc/sysconfig/kubelet\u0026lt;\u0026lt;EOF KUBELET_EXTRA_ARGS=\u0026quot;--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.2\u0026quot; EOF # 或者通过命令行指定 image repo kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers    设置代理，docker pull 的代理配置参考Docker的三种网络代理配置\n  kubeadm 的初始化这里采用了配置文件的方式，kubeadm 对于低版本的配置文件是不兼容的，我们通过 kubeadm config migrate --new-config ${new-file} --old config ${old-file} 命令来转换。\n# kubeadm.yml apiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: bw4tlw.owb266appoos6afw ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 10.186.4.100 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock name: yxj-test taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: extraArgs: runtime-config: api/all=true timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: extraArgs: horizontal-pod-autoscaler-sync-period: 10s horizontal-pod-autoscaler-use-rest-clients: \u0026quot;true\u0026quot; node-monitor-grace-period: 10s dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: k8s.gcr.io kind: ClusterConfiguration kubernetesVersion: v1.20.3 imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers networking: dnsDomain: cluster.local podSubnet: 10.32.0.0/12 serviceSubnet: 10.96.0.0/12 scheduler: {}  这个配置中，给kube-controller-manager 设置了\nhorizontal-pod-autoscaler-use-rest-clients: \u0026quot;true\u0026quot;  这意味着，将来部署的 kube-controller-manager 能够使用自定义资源（Custom Metrics）进行自动水平扩展。\n由于我们这里使用的网络插件是wave，自定义Pod 的 cidr 网络为：\npodSubnet: 10.32.0.0/12  然后，执行一句指令：\n$ kubeadm init --config kubeadm.yaml [init] Using Kubernetes version: v1.20.0 [preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected \u0026quot;cgroupfs\u0026quot; as the Docker cgroup driver. The recommended driver is \u0026quot;systemd\u0026quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/ [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' [certs] Using certificateDir folder \u0026quot;/etc/kubernetes/pki\u0026quot; [certs] Generating \u0026quot;ca\u0026quot; certificate and key [certs] Generating \u0026quot;apiserver\u0026quot; certificate and key [certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local yxj-test] and IPs [10.96.0.1 10.186.4.100] [certs] Generating \u0026quot;apiserver-kubelet-client\u0026quot; certificate and key [certs] Generating \u0026quot;front-proxy-ca\u0026quot; certificate and key [certs] Generating \u0026quot;front-proxy-client\u0026quot; certificate and key [certs] Generating \u0026quot;etcd/ca\u0026quot; certificate and key [certs] Generating \u0026quot;etcd/server\u0026quot; certificate and key [certs] etcd/server serving cert is signed for DNS names [localhost yxj-test] and IPs [10.186.4.100 127.0.0.1 ::1] [certs] Generating \u0026quot;etcd/peer\u0026quot; certificate and key [certs] etcd/peer serving cert is signed for DNS names [localhost yxj-test] and IPs [10.186.4.100 127.0.0.1 ::1] [certs] Generating \u0026quot;etcd/healthcheck-client\u0026quot; certificate and key [certs] Generating \u0026quot;apiserver-etcd-client\u0026quot; certificate and key [certs] Generating \u0026quot;sa\u0026quot; key and public key [kubeconfig] Using kubeconfig folder \u0026quot;/etc/kubernetes\u0026quot; [kubeconfig] Writing \u0026quot;admin.conf\u0026quot; kubeconfig file [kubeconfig] Writing \u0026quot;kubelet.conf\u0026quot; kubeconfig file [kubeconfig] Writing \u0026quot;controller-manager.conf\u0026quot; kubeconfig file [kubeconfig] Writing \u0026quot;scheduler.conf\u0026quot; kubeconfig file [kubelet-start] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot; [kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot; [kubelet-start] Starting the kubelet [control-plane] Using manifest folder \u0026quot;/etc/kubernetes/manifests\u0026quot; [control-plane] Creating static Pod manifest for \u0026quot;kube-apiserver\u0026quot; [control-plane] Creating static Pod manifest for \u0026quot;kube-controller-manager\u0026quot; [control-plane] Creating static Pod manifest for \u0026quot;kube-scheduler\u0026quot; [etcd] Creating static Pod manifest for local etcd in \u0026quot;/etc/kubernetes/manifests\u0026quot; [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026quot;/etc/kubernetes/manifests\u0026quot;. This can take up to 4m0s [apiclient] All control plane components are healthy after 15.004566 seconds [upload-config] Storing the configuration used in ConfigMap \u0026quot;kubeadm-config\u0026quot; in the \u0026quot;kube-system\u0026quot; Namespace [kubelet] Creating a ConfigMap \u0026quot;kubelet-config-1.20\u0026quot; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node yxj-test as control-plane by adding the labels \u0026quot;node-role.kubernetes.io/master=''\u0026quot; and \u0026quot;node-role.kubernetes.io/control-plane='' (deprecated)\u0026quot; [mark-control-plane] Marking the node yxj-test as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: bw4tlw.owb266appoos6afw [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \u0026quot;cluster-info\u0026quot; ConfigMap in the \u0026quot;kube-public\u0026quot; namespace [kubelet-finalize] Updating \u0026quot;/etc/kubernetes/kubelet.conf\u0026quot; to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026quot;kubectl apply -f [podnetwork].yaml\u0026quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.186.4.100:6443 --token bw4tlw.owb266appoos6afw \\ --discovery-token-ca-cert-hash sha256:67c5b1f31669be60c07280f52b9aab867af54d2d4bb679490216028f858c7fb6  就可以完成 Kubernetes Master 的部署了，这个过程只需要几分钟。部署完成后，kubeadm 会生成一行指令\nkubeadm join 10.186.4.100:6443 --token bw4tlw.owb266appoos6afw \\ --discovery-token-ca-cert-hash sha256:67c5b1f31669be60c07280f52b9aab867af54d2d4bb679490216028f858c7fb6  这个 kubeadm join 命令，就是用来给这个 Master 节点添加更多工作节点（Worker）的命令。我们在后面部署 Worker 节点的时候马上会用到它，所以找一个地方把这条命令记录下来。\n此外，kubeadm 还会提示我们第一次使用 Kubernetes 集群所需要的配置命令：\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config  而需要这些配置命令的原因是：Kubernetes 集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的 Kubernetes 集群的安全配置文件，保存到当前用户的.kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群。如果不这么做的话，我们每次都需要通过 export KUBECONFIG 环境变量告诉 kubectl 这个安全配置文件的位置。现在，我们就可以使用 kubectl get 命令来查看当前唯一一个节点的状态了：\n# kubectl get nodes NAME STATUS ROLES AGE VERSION yxj-test Ready control-plane,master 3h14m v1.20.0  默认情况下，出于安全原因，不会在master节点上调度Pod。如果希望能够在master节点上调度Pod，就可以运行如下的命令：\n# kubectl taint nodes --all node-role.kubernetes.io/master- node/yxj-test untainted taint \u0026quot;node-role.kubernetes.io/master\u0026quot; not found taint \u0026quot;node-role.kubernetes.io/master\u0026quot; not found taint \u0026quot;node-role.kubernetes.io/master\u0026quot; not found  这样就删除主节点上的 node-role.kubernetes.io/master  污点，调度器就能将Pod调度到 master 节点上。\n修改组件参数\n如果我们需要修改 k8s 组件的参数，那么可以在 /etc/kubernetes/manifests/ 这个目录下编辑组件的yaml文件，保存后会重启对应的组件。\n4. 部署网络插件 以weave为例子：\nkubectl apply -f \u0026quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\u0026quot;  部署完成后，我们可以通过 kubectl get 重新检查 Pod 的状态：\n# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-74ff55c5b-f95sj 1/1 Running 0 131m coredns-74ff55c5b-zvjdz 1/1 Running 0 131m etcd-yxj-test 1/1 Running 0 131m kube-apiserver-yxj-test 1/1 Running 0 131m kube-controller-manager-yxj-test 1/1 Running 0 131m kube-proxy-nm4tt 1/1 Running 0 131m kube-scheduler-yxj-test 1/1 Running 0 131m weave-net-pl4qp 2/2 Running 1 126m  可以看到，所有的系统 Pod 都成功启动了，而刚刚部署的 Weave 网络插件则在 kube-system 下面新建了一个名叫 weave-net-pl4qp 的 Pod，一般来说，这些 Pod 就是容器网络插件在每个节点上的控制组件。\nKubernetes 支持容器网络插件，使用的是一个名叫 CNI 的通用接口，它也是当前容器网络的事实标准，市面上的所有容器网络开源项目都可以通过 CNI 接入 Kubernetes，比如 Flannel、Calico、Canal、Romana 等等，它们的部署方式也都是类似的“一键部署”。\n weave 插件部署报错：weave Inconsistent bridge state detected. Please do \u0026lsquo;weave reset\u0026rsquo; and try again\n解决：安装weave\nsudo curl -L git.io/weave -o /usr/local/bin/weave sudo chmod a+x /usr/local/bin/weave   5. 部署 k8s 的 worker 节点 Kubernetes 的 Worker 节点跟 Master 节点几乎是相同的，它们运行着的都是一个 kubelet 组件。唯一的区别在于，在 kubeadm init 的过程中，kubelet 启动后，Master 节点上还会自动运行 kube-apiserver、kube-scheduler、kube-controller-manger 这三个系统 Pod。\n所以，相比之下，部署 Worker 节点反而是最简单的，只需要两步即可完成。\n  第一步，在所有 Worker 节点上执行“安装 kubeadm 和 Docker”一节的所有步骤。\n  第二步，执行部署 Master 节点时生成的 kubeadm join 指令：\nkubeadm join 10.186.4.100:6443 --token bw4tlw.owb266appoos6afw \\ --discovery-token-ca-cert-hash sha256:36f6f60943015fadcc0fc9824611af4d594b7c4b43ab264c2113342fbd1e3994    通过 Taint/Toleration 调整 Master 执行 Pod 的策略\n我在前面提到过，默认情况下 Master 节点是不允许运行用户 Pod 的。而 Kubernetes 做到这一点，依靠的是 Kubernetes 的 Taint/Toleration 机制。\n它的原理非常简单：一旦某个节点被加上了一个 Taint，即被“打上了污点”，那么所有 Pod 就都不能在这个节点上运行，因为 Kubernetes 的 Pod 都有“洁癖”。除非，有个别的 Pod 声明自己能“容忍”这个“污点”，即声明了 Toleration，它才可以在这个节点上运行。\n其中，为节点打上“污点”（Taint）的命令是：\n$ kubectl taint nodes node1 foo=bar:NoSchedule  这时，该 node1 节点上就会增加一个键值对格式的 Taint，即：foo=bar:NoSchedule。其中值里面的 NoSchedule，意味着这个 Taint 只会在调度新 Pod 时产生作用，而不会影响已经在 node1 上运行的 Pod，哪怕它们没有 Toleration。\n那么 Pod 又如何声明 Toleration 呢？\n我们只要在 Pod 的.yaml 文件中的 spec 部分，加入 tolerations 字段即可：\napiVersion: v1 kind: Pod ... spec: tolerations: - key: \u0026quot;foo\u0026quot; operator: \u0026quot;Equal\u0026quot; value: \u0026quot;bar\u0026quot; effect: \u0026quot;NoSchedule\u0026quot;  这个 Toleration 的含义是，这个 Pod 能“容忍”所有键值对为 foo=bar 的 Taint（ operator: “Equal”，“等于”操作）。\n现在回到我们已经搭建的集群上来。这时，如果你通过 kubectl describe 检查一下 Master 节点的 Taint 字段，就会有所发现了：\n$ kubectl describe node master Name: master Roles: master Taints: node-role.kubernetes.io/master:NoSchedule  可以看到，Master 节点默认被加上了node-role.kubernetes.io/master:NoSchedule这样一个“污点”，其中“键”是node-role.kubernetes.io/master，而没有提供“值”。\n此时，你就需要像下面这样用“Exists”操作符（operator: “Exists”，“存在”即可）来说明，该 Pod 能够容忍所有以 foo 为键的 Taint，才能让这个 Pod 运行在该 Master 节点上：\napiVersion: v1 kind: Pod ... spec: tolerations: - key: \u0026quot;foo\u0026quot; operator: \u0026quot;Exists\u0026quot; effect: \u0026quot;NoSchedule\u0026quot;  当然，如果你就是想要一个单节点的 Kubernetes，删除这个 Taint 才是正确的选择：\n$ kubectl taint nodes --all node-role.kubernetes.io/master-  如上所示，我们在“node-role.kubernetes.io/master”这个键后面加上了一个短横线“-”，这个格式就意味着移除所有以“node-role.kubernetes.io/master”为键的 Taint。到了这一步，一个基本完整的 Kubernetes 集群就部署完毕了。是不是很简单呢？有了 kubeadm 这样的原生管理工具，Kubernetes 的部署已经被大大简化。更重要的是，像证书、授权、各个组件的配置等部署中最麻烦的操作，kubeadm 都已经帮你完成了。接下来，我们再在这个 Kubernetes 集群上安装一些其他的辅助插件，比如 Dashboard 和存储插件。\n6. 部署 Dashboard 可视化插件 在 Kubernetes 社区中，有一个很受欢迎的 Dashboard 项目，它可以给用户提供一个可视化的 Web 界面来查看当前集群的各种信息。毫不意外，它的部署也相当简单：\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.2.0/aio/deploy/recommended.yaml  这里通过暴露NodePort的方式来提供服务，需要修改默认的yaml文件：\n... kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: type: NodePort ports: - port: 443 targetPort: 8443 nodePort: 30000 selector: k8s-app: kubernetes-dashboard ...  部署完成之后，我们就可以查看 Dashboard 对应的 Pod 的状态了：\n# kubectl get pods -n kubernetes-dashboard NAME READY STATUS RESTARTS AGE dashboard-metrics-scraper-79c5968bdc-wwhns 1/1 Running 0 12m kubernetes-dashboard-9f9799597-pkc76 1/1 Running 0 12m  需要注意的是，由于 Dashboard 是一个 Web Server，很多人经常会在自己的公有云上无意地暴露 Dashboard 的端口，从而造成安全隐患。所以，1.7 版本之后的 Dashboard 项目部署完成后，默认只能通过 Proxy 的方式在本地访问。具体的操作，你可以查看 Dashboard 项目的官方文档。\n创建dashboard管理员 [root@fztelecom3-34 dashboard]# vim dashboard-admin.yaml apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: dashboard-admin namespace: kubernetes-dashboard [root@fztelecom3-34 dashboard]# kubectl create -f ./dashboard-admin.yaml  6.2. 为用户分配权限 [root@fztelecom3-34 dashboard]# vim dashboard-admin-bind-cluster-role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: dashboard-admin-bind-cluster-role labels: k8s-app: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: dashboard-admin namespace: kubernetes-dashboard [root@fztelecom3-34 dashboard]# kubectl create -f ./dashboard-admin-bind-cluster-role.yaml  6.3. 查看登录的token [root@fztelecom3-34 dashboard]# kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep dashboard-admin | awk '{print $1}')  打开nodeip:port访问\n7. 部署容器存储插件 很多时候我们需要用数据卷（Volume）把外面宿主机上的目录或者文件挂载进容器的 Mount Namespace 中，从而达到容器和宿主机共享这些目录或者文件的目的。容器里的应用，也就可以在这些数据卷中新建和写入文件。\n可是，如果你在某一台机器上启动的一个容器，显然无法看到其他机器上的容器在它们的数据卷里写入的文件。这是容器最典型的特征之一：无状态。\n而容器的持久化存储，就是用来保存容器存储状态的重要手段：存储插件会在容器里挂载一个基于网络或者其他机制的远程数据卷，使得在容器里创建的文件，实际上是保存在远程存储服务器上，或者以分布式的方式保存在多个节点上，而与当前宿主机没有任何绑定关系。这样，无论你在其他哪个宿主机上启动新的容器，都可以请求挂载指定的持久化存储卷，从而访问到数据卷里保存的内容。这就是“持久化”的含义。\n由于 Kubernetes 本身的松耦合设计，绝大多数存储项目，比如 Ceph、GlusterFS、NFS 等，都可以为 Kubernetes 提供持久化存储能力。在这次的部署实战中，我会选择部署一个很重要的 Kubernetes 存储插件项目：Rook。\nRook 项目是一个基于 Ceph 的 Kubernetes 存储插件（它后期也在加入对更多存储实现的支持）。不过，不同于对 Ceph 的简单封装，Rook 在自己的实现中加入了水平扩展、迁移、灾难备份、监控等大量的企业级功能，使得这个项目变成了一个完整的、生产级别可用的容器存储插件。\n得益于容器化技术，用几条指令，Rook 就可以把复杂的 Ceph 存储后端部署起来：\nhttps://rook.io/docs/rook/v1.5/ceph-quickstart.html#ceph-storage-quickstart\ngit clone --single-branch --branch v1.5.8 https://github.com/rook/rook.git cd rook/cluster/examples/kubernetes/ceph kubectl create -f crds.yaml -f common.yaml -f operator.yaml kubectl create -f cluster.yaml # 所有的镜像，需要通过外网来拉取 ceph/ceph:v15.2.9 k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1 k8s.gcr.io/sig-storage/csi-attacher:v3.0.0 k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.0 k8s.gcr.io/sig-storage/csi-resizer:v1.0.0 k8s.gcr.io/sig-storage/csi-provisioner:v2.0.0 quay.io/cephcsi/cephcsi:v3.2.0 ceph/ceph:v15.2.9  在部署完成后，你就可以看到 Rook 项目会将自己的 Pod 放置在由它自己管理的两个 Namespace 当中：\n$ kubectl get pods -n rook-ceph-system NAME READY STATUS RESTARTS AGE rook-ceph-agent-7cv62 1/1 Running 0 15s rook-ceph-operator-78d498c68c-7fj72 1/1 Running 0 44s rook-discover-2ctcv 1/1 Running 0 15s $ kubectl get pods -n rook-ceph NAME READY STATUS RESTARTS AGE rook-ceph-mon0-kxnzh 1/1 Running 0 13s rook-ceph-mon1-7dn2t 1/1 Running 0 2s  这样，一个基于 Rook 持久化存储集群就以容器的方式运行起来了，而接下来在 Kubernetes 项目上创建的所有 Pod 就能够通过 Persistent Volume（PV）和 Persistent Volume Claim（PVC）的方式，在容器里挂载由 Ceph 提供的数据卷了。\n这时候，你可能会有个疑问：为什么我要选择 Rook 项目呢？其实，是因为这个项目很有前途\n如果你去研究一下 Rook 项目的实现，就会发现它巧妙地依赖了 Kubernetes 提供的编排能力，合理的使用了很多诸如 Operator、CRD 等重要的扩展特性（这些特性我都会在后面的文章中逐一讲解到）。这使得 Rook 项目，成为了目前社区中基于 Kubernetes API 构建的最完善也最成熟的容器存储插件。我相信，这样的发展路线，很快就会得到整个社区的推崇。\n 备注：其实，在很多时候，大家说的所谓“云原生”，就是“Kubernetes 原生”的意思。而像 Rook、Istio 这样的项目，正是贯彻这个思路的典范。在我们后面讲解了声明式 API 之后，相信你对这些项目的设计思想会有更深刻的体会。\n 8. 卸载集群 8.1 Master kubeadm reset\n停止docker\nsudo systemctl stop docker kubelet  删除相关配置文件：\nrm -rf ~/.kube/  Node\n[root@test-2 ~]# kubeadm reset [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted. [reset] Are you sure you want to proceed? [y/N]: y [preflight] Running pre-flight checks W0305 11:09:18.394192 10467 removeetcdmember.go:79] [reset] No kubeadm config, using etcd pod spec to get data directory [reset] No etcd config found. Assuming external etcd [reset] Please, manually reset etcd to prevent further issues [reset] Stopping the kubelet service [reset] Unmounting mounted directories in \u0026quot;/var/lib/kubelet\u0026quot; [reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki] [reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf] [reset] Deleting contents of stateful directories: [/var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni] The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually by using the \u0026quot;iptables\u0026quot; command. If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar) to reset your system's IPVS tables. The reset process does not clean your kubeconfig files and you must remove them manually. Please, check the contents of the $HOME/.kube/config file.  停止相关的服务\nsudo systemctl stop docker \u0026amp;\u0026amp; yum remove docker-ce docker-ce-cli containerd.io -y  删除相关配置文件：\n# 清除网络插件的配置 rm -rf /etc/cni/net.d/ # 清除iptables规则或者 IPVS 表 # 查看规则以number的方式，一条一条的出来，然后我们根据号码来删除哪一条规则 iptables -L INPUT --line-numbers # 删除第七条规则 iptables -D INPUT 7 # 删除所有规则 sudo iptables -F \u0026amp;\u0026amp; sudo iptables -X \u0026amp;\u0026amp; sudo iptables -F -t nat \u0026amp;\u0026amp; sudo iptables -X -t nat # 如果开启了ipvs，通过下面的命令清除 ipvsadm --clear # 清除网桥 ip link del weave ip link del docker0 ip link del vethwe-bridge ip link del vxlan-6784 # 删除kubeconfig文件 rm -rf ~/.kube/  参考文档： [1] kubeadm\n[2] Kubernetes实战指南（三十四）： 高可用安装K8s集群1.20.x\n[3] kubeadm安装最新高可用K8S集群v1.20.2\n[4] k8s 1.20.0 在centos7 使用 kubeadm 安装\n[5] Creating a cluster with kubeadm\n","date":"2021年02月20日","permalink":"https://ahamoment.cn/docs/k8s-doc/chapter2/kubeadm/","summary":"1. 准备工作 机器配置 8核CPU、8GB内存； 40GB磁盘 centos 7.9 内网互同 外网访问不受限制 组件信息 组件 版本 系统 Centos 7.9 Docker版本 18.09.9 k8s 版本 1.20.0 Pod 网段 10.32.0.0/","title":"使用kubeadm 安装k8s集群"},{"contents":"bazel 简介 安装 centos7：https://docs.bazel.build/versions/4.0.0/install-redhat.html\nStep1：\n从 Fedora COPR 下载 .repo 文件并复制到 /etc/yum.repos.d/ 目录。\nStep2:\n运行 yum install bazel3 命令安装。\n教程 java / c++\n常用命令 命令文档：https://docs.bazel.build/versions/master/command-line-reference.html#fetch-options\n查询依赖关系：\nbazel query --notool_deps --noimplicit_deps \u0026quot;deps(//:ProjectRunner)\u0026quot; --output graph  可视化网站：\nhttp://www.webgraphviz.com/\n编译命令：\nbazel build //:ProjectRunner  清除命令：\nbazel clean  bazel clean在 outputPath和action_cache目录上执行rm -rf。它还会删除工作空间符号链接。 --expunge选项将清除整个outputBase。\n输出目录 https://docs.bazel.build/versions/master/output_directories.html\n 必须从包含WORKSPACE文件的目录（“工作区目录”）或其子目录中调用Bazel。如果不是，它将报告错误。 outputRoot目录在Linux上默认为〜/.cache/bazel，在macOS上默认为/ private/var/tmp，在Windows上默认为％HOME％（如果设置），否则默认为％USERPROFILE％，否则调用SHGetKnownFolderPath()的结果设置了FOLDERID_Profile标志。如果设置了环境变量$ TEST_TMPDIR（如测试bazel本身），则该值将覆盖默认值。 我们将Bazel用户的构建状态保留在outputRoot/_bazel_$USER下。这称为 outputUserRoot 目录。 在outputUserRoot目录下，我们创建一个installBase目录，其名称为 install加上Bazel安装清单的MD5哈希。 在outputUserRoot目录下，我们还创建了一个outputBase目录，该目录的名称是工作区目录的路径名的MD5哈希。因此，例如，如果Bazel在工作空间目录/ home/user/src/my-project中（或在与该目录符号链接的目录中）运行，则我们将创建一个输出基本目录，名为：/home/user/.cache/bazel/ _bazel_user/7ffd56a6e4cb724ea575aba15733d113。 用户可以使用Bazel的--output_base启动选项来覆盖默认的输出基本目录。例如，bazel --output_base = /tmp/bazel/output build x/y:z。 用户还可以使用Bazel的--output_user_root启动选项来覆盖默认的安装基础目录和输出基础目录。例如：bazel --output_user_root=/tmp/bazel build x/y:z。  我们在工作区目录中放置了符号链接“bazel-”, “bazel-out”, “bazel-testlogs”, and “bazel-bin” 。这些符号链接指向输出目录内特定于目标的目录内的某些目录。这些符号链接只是为了方便用户，因为Bazel本身并不使用它们。另外，我们仅在工作空间目录可写时才这样做。\n#机器上所有Bazel输出的根目录：outputRoot /home/user/.cache/bazel/ #给定用户的顶级目录取决于用户名：outputUserRoot _bazel_$USER/ #Bazel安装清单的哈希：installBase install/ fba9a2c87ee9589d72889caf082f1029/ #包含从的数据部分解压缩的二进制文件和脚本首次运行时的bazel可执行文件（例如帮助程序脚本和主Java文件BazelServer_deploy.jar） _embedded_binaries/ #客户端工作区目录的哈希(/home/some-user/src/my-project):outputBase 7ffd56a6e4cb724ea575aba15733d113/  缓存 外部依赖缓存 使用 bazel fetch //repo:...，可以从具有存储库功能的远程源（例如http_archive()或maven_jar())下载外部存储库和工件。有时，网络访问不可用，或者重新下载已下载的存储库是浪费的。\n使用中央缓存，Bazel可以在访问网络之前检查它是否包含请求的存储库。允许多个工作空间共享同一存储库高速缓存，并且缓存也是可移植的。\n相关联的issue：\nMake maven_jar and friends smarter by re-using previously fetched artifacts across different projects＃1752\n缓存选项\n--experimental_repository_cache\n--repository_cache\n  参考文档：\nhttps://bazel.build/designs/2016/09/30/repository-cache.html\nhttps://github.com/bazelbuild/bazel/issues/1752\n ","date":"2021年02月10日","permalink":"https://ahamoment.cn/posts/tool/tool-bazel/","summary":"bazel 简介 安装 centos7：https://docs.bazel.build/versions/4.0.0/install-redhat.ht","title":"bazel 外部存储库缓存"},{"contents":"这两天在看一个开源的小说管理平台的代码，发现它的项目使用了MyBatis Generator来生成代码操作数据库，花了两个早上的时间研究了一下，确实是很不错的工具，能在我们的开发过程中省掉不少的工作。\nmybatis generator 的使用大致分成几步：\n 创建配置文件 保存配置文件到本地 执行 MyBatis Generator  我们以一个例子来说明如何使用 MyBaits Generator，项目的结构如下所示：\n├── pom.xml ├── README.md ├── src │ ├── main │ │ ├── java │ │ │ └── com │ │ │ └── xueqiang │ │ │ └── footmark │ │ │ ├── controller │ │ │ ├── FootmarkApplication.java │ │ │ ├── model │ │ │ ├── service │ │ │ └── utils │ │ └── resources │ │ ├── application.properties │ │ ├── log4j.properties │ │ ├── mybatis │ │ │ └── generator-configuration.xml │ └── test  1. 配置文件 在 resources/mybatis/ 目录下创建配置文件 generator-configuration.xml，该文件的具体内容如下所示：\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt; \u0026lt;!DOCTYPE generatorConfiguration PUBLIC \u0026quot;-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN\u0026quot; \u0026quot;http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd\u0026quot;\u0026gt; \u0026lt;generatorConfiguration\u0026gt; \u0026lt;!-- 全局配置文件，下面可以通过占位符的形式读取文件中的值 --\u0026gt; \u0026lt;!-- \u0026lt;properties resource=\u0026quot;db.properties\u0026quot;/\u0026gt;--\u0026gt; \u0026lt;!-- 用来指定数据源驱动包（jar/zip）的绝对路径 --\u0026gt; \u0026lt;classPathEntry location=\u0026quot;D:\\Users\\xueqiang.chen\\.m2\\repository\\mysql\\mysql-connector-java\\8.0.11\\mysql-connector-java-8.0.11.jar\u0026quot;/\u0026gt; \u0026lt;!-- 用于运行时的解析模式和具体的代码生成行为，对应org.mybatis.generator.config.Context类 --\u0026gt; \u0026lt;!-- id:Context示例的唯一ID，用于输出错误信息时候作为唯一标记 targetRuntime:用于执行代码生成模式。可选值： MyBatis3DynamicSql：动态SQL MyBatis3Kotlin：基于Kotlin生成 MyBatis3：提供基本的基于动态SQL的CRUD方法和XXXByExample方法，会生成XML映射文件 MyBatis3Simple：提供基本的基于动态SQL的CRUD方法，会生成XML映射文件 MyBatis3DynamicSqlV1：已经过时，不推荐使用 defaultModeType:控制Domain类的生成行为。执行引擎为MyBatis3DynamicSql或者MyBatis3Kotlin时忽略此配置，可选值： conditional：默认值，类似hierarchical，但是只有一个主键的时候会合并所有属性生成在同一个类。 flat：所有内容全部生成在一个对象中。 hierarchical：键生成一个XXKey对象，Blob等单独生成一个对象，其他简单属性在一个对象中 --\u0026gt; \u0026lt;!-- context的内容需要按照顺序排列,否则会报错： property-\u0026gt;plugin-\u0026gt;commentGenerator-\u0026gt;connectionFactory|jdbcConnection-\u0026gt;javaTypeResolver-\u0026gt;javaModelGenerator-\u0026gt;sqlMapGenerator-\u0026gt;javaClientGenerator-\u0026gt;table --\u0026gt; \u0026lt;context id=\u0026quot;default\u0026quot; targetRuntime=\u0026quot;MyBatis3DynamicSql\u0026quot;\u0026gt; \u0026lt;property name=\u0026quot;javaFileEncoding\u0026quot; value=\u0026quot;UTF-8\u0026quot;/\u0026gt; \u0026lt;!-- \u0026lt;plugin type=\u0026quot;org.mybatis.generator.plugins.SerializablePlugin\u0026quot;/\u0026gt;--\u0026gt; \u0026lt;!--用于控制生成的实体的注释内容 suppressAllComments:是否生成注释 suppressDate:是否在注释中添加生成的时间戳 dateFormat:配合suppressDate使用，指定输出时间戳的格式 addRemarkComments:是否输出表和列的Comment信息--\u0026gt; \u0026lt;commentGenerator\u0026gt; \u0026lt;property name=\u0026quot;suppressDate\u0026quot; value=\u0026quot;true\u0026quot;/\u0026gt; \u0026lt;property name=\u0026quot;suppressAllComments\u0026quot; value=\u0026quot;true\u0026quot;/\u0026gt; \u0026lt;/commentGenerator\u0026gt; \u0026lt;!--用于指定数据源的连接信息，对应的类为org.mybatis.generator.config.JDBCConnectionConfiguration driverClass:数据源驱动的全类名 connectionURL:JDBC的连接URL userId:用户名 password:密码--\u0026gt; \u0026lt;jdbcConnection driverClass=\u0026quot;com.mysql.jdbc.Driver\u0026quot; connectionURL=\u0026quot;jdbc:mysql://127.0.0.1:3306/footmark\u0026quot; userId=\u0026quot;root\u0026quot; password=\u0026quot;mysql\u0026quot;\u0026gt; \u0026lt;/jdbcConnection\u0026gt; \u0026lt;!--用于解析和计算数据库列类型和Java类型的映射关系，该标签只包含一个type属性，用于指定org.mybatis.generator.api.JavaTypeResolver接口的实现类 forceBigDecimals:是否强制把所有的数字类型强制使用java.math.BigDecimal类型表示 useJSR310Types:是否支持JSR310，主要是JSR310的新日期类型 数据库JDBC类型 Java类型 DATE\tjava.time.LocalDate TIME\tjava.time.LocalTime TIMESTAMP\tjava.time.LocalDateTime TIME_WITH_TIMEZONE\tjava.time.OffsetTime TIMESTAMP_WITH_TIMEZONE\tjava.time.OffsetDateTime--\u0026gt; \u0026lt;javaTypeResolver\u0026gt; \u0026lt;!-- 不强制把所有的数字类型转化为BigDecimal --\u0026gt; \u0026lt;property name=\u0026quot;forceBigDecimals\u0026quot; value=\u0026quot;false\u0026quot; /\u0026gt; \u0026lt;/javaTypeResolver\u0026gt; \u0026lt;!--主要用于控制实体（Model）类的代码生成行为 targetPackage:生成的实体类的包名 targetProject:生成的实体类文件相对于项目（根目录）的位置 property: constructorBased:是否生成一个带有所有字段属性的构造函数 enableSubPackages:是否允许通过Schema生成子包 exampleTargetPackage:生成的伴随实体类的Example类的包名 exampleTargetProject:生成的伴随实体类的Example类文件相对于项目（根目录）的位置 immutable:是否不可变 rootClass:为生成的实体类添加父类 trimStrings:Setter方法是否对字符串类型进行一次trim操作--\u0026gt; \u0026lt;javaModelGenerator targetPackage=\u0026quot;com.xueqiang.footmark.model.entity\u0026quot; targetProject=\u0026quot;src/main/java\u0026quot;\u0026gt; \u0026lt;property name=\u0026quot;enableSubPackages\u0026quot; value=\u0026quot;true\u0026quot; /\u0026gt; \u0026lt;property name=\u0026quot;trimStrings\u0026quot; value=\u0026quot;true\u0026quot; /\u0026gt; \u0026lt;/javaModelGenerator\u0026gt; \u0026lt;!--主要用于控制XML映射文件的代码生成行为 targetPackage:生成的XML映射文件的包名 targetProject:生成的XML映射文件相对于项目（根目录）的位置,例如:src/main/resources property: enableSubPackages:是否允许通过Schema生成子包--\u0026gt; \u0026lt;sqlMapGenerator targetPackage=\u0026quot;mapper\u0026quot; targetProject=\u0026quot;src/main/resources\u0026quot;\u0026gt; \u0026lt;property name=\u0026quot;enableSubPackages\u0026quot; value=\u0026quot;true\u0026quot; /\u0026gt; \u0026lt;/sqlMapGenerator\u0026gt; \u0026lt;!--主要用于控制Mapper接口的代码生成行为 type:Mapper接口生成策略.(\u0026lt;context\u0026gt;标签的targetRuntime属性为MyBatis3DynamicSql或者MyBatis3Kotlin时此属性配置忽略) ANNOTATEDMAPPER:Mapper接口生成的时候依赖于注解和SqlProviders（也就是纯注解实现），不会生成XML映射文件。 XMLMAPPER:Mapper接口生成接口方法，对应的实现代码生成在XML映射文件中（也就是纯映射文件实现） MIXEDMAPPER:Mapper接口生成的时候复杂的方法实现生成在XML映射文件中，而简单的实现通过注解和SqlProviders实现（也就是注解和映射文件混合实现）。 targetPackage:生成的Mapper接口的包名 targetProject:生成的Mapper接口文件相对于项目（根目录）的位置--\u0026gt; \u0026lt;!--property属性： enableSubPackages:是否允许通过Schema生成子包 useLegacyBuilder:是否通过SQL Builder生成动态SQL rootInterface:为生成的Mapper接口添加父接口--\u0026gt; \u0026lt;javaClientGenerator type=\u0026quot;ANNOTATEDMAPPER\u0026quot; targetPackage=\u0026quot;com.xueqiang.footmark.model.mapper\u0026quot; targetProject=\u0026quot;src/main/java\u0026quot;\u0026gt; \u0026lt;property name=\u0026quot;enableSubPackages\u0026quot; value=\u0026quot;true\u0026quot; /\u0026gt; \u0026lt;/javaClientGenerator\u0026gt; \u0026lt;!--enableInsert 动态SQL提供类SqlProvider的类名称 enableSelectByPrimaryKey 是否允许生成selectByPrimaryKey方法 enableSelectByExample 是否允许生成selectByExample方法 enableUpdateByPrimaryKey 是否允许生成updateByPrimaryKey方法 enableDeleteByPrimaryKey 是否允许生成deleteByPrimaryKey方法 enableDeleteByExample 是否允许生成deleteByExample方法 enableCountByExample 是否允许生成countByExample方法 enableUpdateByExample 是否允许生成updateByExample方法 selectByPrimaryKeyQueryId value指定对应的主键列提供列表查询功能 selectByExampleQueryId value指定对应的查询ID提供列表查询功能--\u0026gt; \u0026lt;table tableName=\u0026quot;t_order\u0026quot; enableCountByExample=\u0026quot;false\u0026quot; enableDeleteByExample=\u0026quot;false\u0026quot; enableSelectByExample=\u0026quot;true\u0026quot; enableUpdateByExample=\u0026quot;false\u0026quot; domainObjectName=\u0026quot;Order\u0026quot; mapperName=\u0026quot;OrderMapper\u0026quot;\u0026gt; \u0026lt;generatedKey column=\u0026quot;id\u0026quot; sqlStatement=\u0026quot;MySql\u0026quot;/\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/context\u0026gt; \u0026lt;/generatorConfiguration\u0026gt;  上面的这个配置文件主要是按t_order这个数据库表的属性生成整个 xml 配置文件的元素意义我们可以参考官方文档的介绍。\n","date":"2021年01月26日","permalink":"https://ahamoment.cn/posts/java/framework-mybatis-generator/","summary":"","title":"MyBatis Generator 使用与原理(上)"},{"contents":"1. Docker 问题 1.1. docker 后端存储驱动 devicemapper、overlay 几种的区别？ 刚开始拿到这道题我有点蒙，因为我只知道目前我们用的是vg-pool devicemapper 来存储镜像和容器，后来面试官问我镜像分层的技术知道吗？我说知道，就是**联合文件系统，多层文件系统联合组成一个统一的文件系统视角，当需要修改文件时采用写时复制（CopyW）的技术从上往下查找，找到之后复制到可写的容器层，进行修改并保存至容器层，**说完之后面试官再问我，那每次修改文件都需要从上往下查找，层数又那么多，性能是否比较差，现在才反应回来，原先面试官想考察我aufs、overlay 或者是 devicemapper 等几种存储驱动的区别。\nAUFS\nAUFS （Another UnionFS）是一种 Union FS，是文件级的存储驱动，AUFS 简单理解就是将多层的文件系统联合挂载成统一的文件系统，这种文件系统可以一层一层地叠加修改文件，只有最上层是可写层，底下所有层都是只读层，对应到 Docker，最上层就是 container 层，底层就是 image 层，结构如下图所示：\nOverlay\nOverlay 也是一种 Union FS，和 AUFS 多层相比，Overlay 只有两层：一个 upper 文件系统和一个 lower 文件系统，分别代表 Docker 的容器层（upper）和镜像层（lower）。当需要修改一个文件时，使用 CopyW 将文件从只读的 lower 层复制到可写层 upper，结果也保存在 upper 层，结构如下图所示：\nDevicemapper\nDevice mapper，提供的是一种从逻辑设备到物理设备的映射框架机制，前面讲的 AUFS 和 OverlayFS 都是文件级存储，而 Device mapper 是块级存储，所有的操作都是直接对块进行操作，而不是文件。Device mapper 驱动会先在块设备上创建一个资源池，然后在资源池上创建一个带有文件系统的基本设备，所有镜像都是这个基本设备的快照，而容器则是镜像的快照。所以在容器里看到文件系统是资源池上基本设备的文件系统的快照。当要写入一个新文件时，在容器的镜像内为其分配新的块并写入数据，这个叫用时分配。当要修改已有文件时，再使用CoW为容器快照分配块空间，将要修改的数据复制到在容器快照中新的块里再进行修改。Devicemapper 驱动默认会创建一个100G 的文件包含镜像和容器。每一个容器被限制在 10G 大小的卷内，可以自己配置调整。结构如下图所示：\n详细内容请参考：\n  Docker 五种存储驱动\n  深入了解 Docker 存储驱动\n  1.2. 容器隔离不彻底，Memory 和 CPU 隔离不彻底，怎么处理解决这个问题？ 由于 /proc 文件系统是以只读的方式挂载到容器内部，所以在容器内看到的都是宿主机的信息，包括 CPU 和 Memory，docker 是以 cgroups 来进行资源限制的，而 jdk1.9 以下版本目前无法自动识别容器的资源配额，1.9以上版本会自动识别和正常读取 cgroups 中为容器限制的资源大小。\nMemory 隔离不彻底\n Docker 通过 cgroups 完成对内存的限制，而 /proc 文件目录是以只读的形式挂载到容器中，由于默认情况下，Java 压根就看不到 cgroups 限制的内容的大小，而默认使用 /proc/meminfo 中的信息作为内存信息进行启动，默认情况下，JVM 初始堆大小为内存总量的 1/4，这种情况会导致，如果容器分配的内存小于 JVM 的内存， JVM 进程会被 linux killer 杀死。\n那么目前有几种解决方式：\n（1）升级 JDK 版本到1.9以上，让 JVM 能自动识别 cgroups 对容器的资源限制，从而自动调整 JVM 的参数并启动 JVM 进程。\n（2）对于较低版本的JDK，一定要设置 JVM 初始堆大小，并且JVM 的最大堆内存不能超过容器的最大内存值，正常理论值应该是：容器 limit-memory = JVM 最大堆内存 + 750MB。\n（3）使用 lxcfs ，这是一种用户态文件系统，用来支持LXC 容器，lxcfs 通过用户态文件系统，在容器中提供下列 procfs 的文件，启动时，把宿主机对应的目录 /var/lib/lxcfu/proc/meminfo 文件挂载到 Docker 容器的 /proc/meminfo 位置后，容器中进程（JVM）读取相应文件内容时，lxcfs 的 fuse 将会从容器对应的 cgroups 中读取正确的内存限制，从而获得正确的资源约束设定。\nCPU 隔离不彻底\n JVM GC （垃圾回收）对于 java 程序执行性能有一定的影响，默认的 JVM 使用如下公式： ParallelGCThreads = ( ncpu \u0026lt;= 8 ) ? ncpu：3 + （ncpu * 5）/ 8 来计算并行 GC 的线程数，但是在容器里面，ncpu 获取的就是所在宿主机的 cpu 个数，这会导致 JVM 启动过多的 GC 线程，直接的结果就是 GC 的性能下降，java 服务的感受就是：延时增加， TPS 吞度量下降，针对这种问题，也有以下几种解决方案：\n（1）显示传递 JVM 启动参数：“-XX: ParallelGCThreads\u0026quot; 告诉 JVM 应该启动多少个并行 GC 线程，缺点是需要业务感知，而且需要为不同配置的容器传递不同的 JVM 参数。\n（2）在容器内使用 Hack 过的 glibc ，使 JVM 通过 sysconf 系统调用能正确获取容器内 CPU 资源核数，优点是业务无感知，并且能自动适配不同配置的容器，缺点是有一定的维护成本。具体参考：容器内获取 CPU 核数问题\n1.3. 介绍一下容器实现的基础: Namespace and Cgroups 主要用到了Linux的两种技术：Namespace 和 CGroup。Namespace 做隔离，Cgroups 做限制。\nNamespace 技术实际上修改了应用进程看待整个计算机“视图”，即它的“视线”被操作系统做了限制，只能“看到”某些指定的内容。在创建进程的时候，Linux 系统提供了Mount、UTS、IPC、Network和User这些Namespace，用来对各种不同的进程上下文进行隔离操作。所以，Docker 容器实际上是在创建容器进程时，指定了这个进程所需要启用的一组 Namespace 参数。这样，容器就只能“看”到当前 Namespace 所限定的资源、文件、设备、状态，或者配置。而对于宿主机以及其他不相关的程序，它就完全看不到了。所以说，容器，其实是一种特殊的进程而已。\nLinux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。\n一个正在运行的 Docker 容器，其实就是一个启用了多个 Linux Namespace 的应用进程，而这个进程能够使用的资源量，则受 Cgroups 配置的限制。\n详情请参考：https://coolshell.cn/articles/17049.html\n1.4. docker load 加载一个镜像， docker images 查看不到，是哪些原因？ 1.4 有没有遇到容器 OOM 的问题？怎么处理的？ OOM 可能的原因：\n  容器隔离不彻底。默认情况下，JVM 初始堆大小为内存总量的 1/4，例如这台宿主机的内存为32G，那么初始堆的大小为8G，这种情况会导致，如果容器分配的内存小于 JVM 的内存， JVM 进程会被 linux killer 杀死。\n   处理方法：\n 升级jdk版本或者设置初始堆大小和堆内存最大值，即-Xms 和 -Xmx。  2. Kubernetes 问题 2.1. k8s 的架构体系了解吗？简单描述一下 这道题主要考察 k8s 体系，涉及的范围其实太广泛，可以从本身 k8s 组件、存储、网络、监控等方面阐述，当时我主要将 k8s 的每个组件功能都大概说了一下。\nMaster节点\nMaster节点主要有四个组件，分别是：api-server、controller-manager、kube-scheduler 和 etcd。\n api-server\n负责API服务。kube-apiserver 作为 k8s 集群的核心，负责整个集群功能模块的交互和通信，集群内的各个功能模块如 kubelet、controller、scheduler 等都通过 api-server 提供的接口将信息存入到 etcd 中，当需要这些信息时，又通过 api-server 提供的 restful 接口，如get、watch 接口来获取，从而实现整个 k8s 集群功能模块的数据交互。\ncontroller-manager\n负责容器编排。controller-manager 作为 k8s 集群的管理控制中心，负责集群内 Node、Namespace、Service、Token、Replication 等资源对象的管理，使集群内的资源对象维持在预期的工作状态。\n每一个 controller 通过 api-server 提供的 restful 接口实时监控集群内每个资源对象的状态，当发生故障，导致资源对象的工作状态发生变化，就进行干预，尝试将资源对象从当前状态恢复为预期的工作状态，常见的 controller 有 Namespace Controller、Node Controller、Service Controller、ServiceAccount Controller、Token Controller、ResourceQuote Controller、Replication Controller等。\nkube-scheduler\nkube-scheduler 简单理解为通过特定的调度算法和策略为待调度的 Pod 列表中的每个 Pod 选择一个最合适的节点进行调度，调度主要分为两个阶段，预选阶段和优选阶段，其中预选阶段是遍历所有的 node 节点，根据策略和限制筛选出候选节点，优选阶段是在第一步的基础上，通过相应的策略为每一个候选节点进行打分，分数最高者胜出，随后目标节点的 kubelet 进程通过 api-server 提供的接口监控到 kube-scheduler 产生的 pod 绑定事件，从 etcd 中获取 Pod 的清单，然后下载镜像，启动容器。\n预选阶段的策略有：\n(1) MatchNodeSelector：判断节点的 label 是否满足 Pod 的 nodeSelector 属性值。\n(2) PodFitResource：判断节点的资源是否满足 Pod 的需求，批判的标准是：当前节点已运行的所有 Pod 的 request值 + 待调度的 Pod 的 request 值是否超过节点的资源容量。\n(3) PodFitHostName：判断节点的主机名称是否满足 Pod 的 nodeName 属性值。\n(4) PodFitHostPort：判断 Pod 的端口所映射的节点端口是否被节点其他 Pod 所占用。\n(5) CheckNodeMemoryPressure：判断 Pod 是否可以调度到内存有压力的节点，这取决于 Pod 的 Qos 配置，如果是 BestEffort（尽量满足，优先级最低），则不允许调度。\n(6) CheckNodeDiskPressure：如果当前节点磁盘有压力，则不允许调度。\n优选阶段的策略有：\n(1) SelectorSpreadPriority：尽量减少节点上同属一个 SVC/RC/RS 的 Pod 副本数，为了更好的实现容灾，对于同属一个 SVC/RC/RS 的 Pod 实例，应尽量调度到不同的 node 节点。\n(2) LeastRequestPriority：优先调度到请求资源较少的节点，节点的优先级由节点的空闲资源与节点总容量的比值决定的，即（节点总容量 - 已经运行的 Pod 所需资源）/ 节点总容量，CPU 和 Memory 具有相同的权重，最终的值由这两部分组成。\n(3) BalancedResourceAllocation：该策略不能单独使用，必须和 LeaseRequestPriority 策略一起结合使用，尽量调度到 CPU 和 Memory 使用均衡的节点上。\nETCD\n强一致性的键值对存储，k8s 集群中的所有资源对象都存储在 etcd 中。\nNode节点\n 在 Kubernetes 项目中，kubelet 主要负责同容器运行时（比如 Docker 项目）打交道。而这个交互所依赖的，是一个称作 CRI（Container Runtime Interface）的远程调用接口，这个接口定义了容器运行时的各项核心操作。而具体的容器运行时，比如 Docker 项目，则一般通过 OCI 这个容器运行时规范同底层的 Linux 操作系统进行交互。此外，kubelet 还通过 gRPC 协议同一个叫作 Device Plugin 的插件进行交互。这个插件，是 Kubernetes 项目用来管理 GPU 等宿主机物理设备的主要组件，也是基于 Kubernetes 项目进行机器学习训练、高性能作业支持等工作必须关注的功能。而 kubelet 的另一个重要功能，则是调用网络插件和存储插件为容器配置网络和持久化存储。这两个插件与 kubelet 进行交互的接口，分别是 CNI（Container Networking Interface）和 CSI（Container Storage Interface）。\nNode节点主要有三个组件：分别是 kubelet、kube-proxy 和 容器运行时 docker 或者 rkt。\nkubelet\n在 k8s 集群中，每个 node 节点都会运行一个 kubelet 进程，该进程用来处理 Master 节点下达到该节点的任务，同时，通过 api-server 提供的接口定期向 Master 节点报告自身的资源使用情况，并通过 cadvisor 组件监控节点和容器的使用情况。\nkube-proxy\nkube-proxy 就是一个智能的软件负载均衡器，将 service 的请求转发到后端具体的 Pod 实例上，并提供负载均衡和会话保持机制，目前有三种工作模式，分别是：用户模式（userspace）、iptables 模式和 IPVS 模式。\n容器运行时——docker\n负责管理 node 节点上的所有容器和容器 IP 的分配。\n2.2. k8s 创建一个pod的详细流程，涉及的组件怎么通信的？ k8s 创建一个 Pod 的详细流程如下：\n(1) 客户端提交创建请求，可以通过 api-server 提供的 restful 接口，或者是通过 kubectl 命令行工具，支持的数据类型包括 JSON 和 YAML。\n(2) api-server 处理用户请求，将 pod 信息存储至 etcd 中。\n(3) kube-scheduler 通过 api-server 提供的接口监控到未绑定的 pod，尝试为 pod 分配 node 节点，主要分为两个阶段，预选阶段和优选阶段，其中预选阶段是遍历所有的 node 节点，根据策略筛选出候选节点，而优选阶段是在第一步的基础上，为每一个候选节点进行打分，分数最高者胜出。\n(4) 选择分数最高的节点，进行 pod binding 操作，并将结果存储至 etcd 中。\n(5) 随后目标节点的 kubelet 进程通过 api-server 提供的接口监测到 kube-scheduler 产生的 pod 绑定事件，然后从 etcd 获取 pod 清单，下载镜像并启动容器。\n 整个事件流可以参考下图：\n参考文章：kubectl 创建 Pod 背后到底发生了什么？\n2.3. k8s 中服务级别，怎样设置服务的级别才是最高的 这道题主要考察 k8s Qos 类别。在 k8s 中，Qos 主要有三种类别，分别是 BestEffort、Burstable 和 Guaranteed，三种类别区别如下：\nBestEffort\n什么都不设置（CPU or Memory），佛系申请资源。\nBurstable\nPod 中的容器至少一个设置了CPU 或者 Memory 的请求\nGuaranteed\nPod 中的所有容器必须设置 CPU 和 Memory，并且 request 和 limit 值相等。\n详情可以参考这篇博客：K8s Qos\n2.4. 有状态的容器如何上云？ 2.5. 解释一下CRD和Operator？有没有自己开发过CRD或者Operator？ 2.6. 什么是 CNI? 平时 K8S 集群用的是哪个网络插件？ 2.7. 为什么 Pod 中关于资源有 request 和 limit 两个字段？有想过这么设计的原因吗？ 2.8. Pod被调度到一个节点上的具体过程？ 2.9. 一个请求到 Pod 接收响应，中间经历了哪些过程？ 3. 参考资料 [1] docker \u0026amp; kubernetes 面试\n","date":"2020年12月28日","permalink":"https://ahamoment.cn/posts/interview/interview-docker-k8s/","summary":"1. Docker 问题 1.1. docker 后端存储驱动 devicemapper、overlay 几种的区别？ 刚开始拿到这道题我有点蒙，因为我只知道目前我们用的是vg-poo","title":"Docker 和 Kubernetes 面试题"},{"contents":" 面试题目录\n 1. 数据结构  排序算法 链表和数组的区别？跳表的实现 二分查找的时间复杂度和优点？ 哈希表，解决冲突的方式 二叉树和B+树 二个亿的无序整数，如何找到中间值  常见的编程题：链表反转、动态规划、二叉树遍历\n2. 操作系统   进程与线程\n  进程间的通信方式\n  进程调度\n  死锁的条件与解除\n  僵尸进程是什么\n  用户态与内核态\n  3. Linux  select 与 epoll 的区别 fork的底层原理 查看文件中的100-200行，其中有error相关的附近日志 top分析 查看端口占用 查看打开的文件 tcpdump 抓包 awk，grep的使用  4. 计算机网络  七层协议与tcp四层协议？五层协议  4.1. TCP  tcp与udp的区别 tcp的三次握手和四次挥手 syn包丢了最多会重传多少次 time_wait 的作用与危害，tcp_reuse 的机制 TCP 如何确保可靠传输 TCP 拥塞控制和流量控制 TCP 的negla算法，如何解决粘包和半包(丢包？)问题 TCP的链接保活计时器 基于UDP设计一个可靠的传输协议(参考quic)  4.2. HTTP  一次url访问会经过什么过程 GET 和 POST 的区别，PUT 和 POST 的区别 http 常见错误码 http与https，https的原理 session与cookie  5. 数据库 5.1. MySQL  常见的存储引擎有哪些，他们有什么区别 常见的索引实现方式，B+树索引，哈希索引他们分别适用什么场景 Innodb 引擎的索引存储方式，聚簇索引与非聚簇索引 什么情况下索引不命中？MySQL 如何优化联合索引不满足最左规则的查询(索引下推) 一级索引与二级索引的查询方式，如何优化(覆盖索引) 如何优化慢查询(explain 分析) 事务的隔离级别 innodb 引擎是如何解决RR级别下的幻读问题(mvcc,next-key) MySQL为什么RC级别下会出现不可重复度的问题，而RR不会（read View） 乐观锁与悲观锁 mysql的锁机制（对索引加锁）（共享锁，排他锁，意向锁，gap锁） mysql如何保证事务的正确性（redolog，undo log） mysql是怎么保证数据持久，不丢失的（binlog） mysql主从复制的方式（同步复制，半同步复制，异步复制），如何解决主从延迟（多线程执行，组提交） MySQL 查询的IN和EXIST有什么区别？ MySQL 死锁是如何形成的？MySQL是如何解决死锁问题的？  5.2. Redis  常见的数据类型(String, List, Hash, Set, Sort Set) 字符串类型是怎么实现(sds结构) hash 是怎么实现（渐进性hash） sort set 是怎么实现的（跳表） redis 实现分布式锁，实现可重入锁（lua,setnxex），如何解决宿主机宕机后，主从切换可能会导致同时有两个线程获取锁（红锁） redis实现乐观锁（Multi，watch，Exec） redis 持久化机制 redis 主从复制（全量复制，增量复制，runid，复制缓冲区，复制偏移量） sentinel是如何做到高可用的（主观下线，客观下线，raft选主，主从切换） redis cluster是如何做到高可用的  6. Java 6.1. 基础 6.2. 多线程  什么是线程安全问题？ 实现多线程的几种方式？ 线程池的关闭方式有哪些？  7. 常用框架与第三方组件 7.1. Spring  Spring AOP的实现原理 spring IOC 的好处 spring 用了什么设计模式 Transaction 什么时候会生效？事务的实现原理是什么？ spring mvc的controller参数是如何映射的？ 多例注入的实现原理及其生命周期 Spring 中的 Controller 注解是单例的还是多例的？会不会有并发问题？  7.2. SpringBoot  常用的SpringBoot的注解有哪些？  7.3 Mybatis  Mybatis 的一级缓存和二级缓存 Mybatis 插件的原理，用到了什么设计模式 Mybatis 是怎么找到指定的mapper的  7.4. 本地缓存  guava，caffeine，ohc  8. 分布式  CAP理论 BASE理论  8.1. 缓存  穿透，击穿，雪崩是什么，如何避免（随机过期时间，异步刷新，单线程回源）  8.2. 一致性协议  raft协议原理 Paxos 算法 如何解决脑裂问题 多主一致性协议 gossip 原理  8.3. 负载均衡  常见的负载均衡算法（轮询，加权轮询，哈希，一致性哈希） 一致性哈希  8.4 分布式锁  使用zk怎么实现分布式锁 使用redis怎么实现分布式锁  8.5. 分布式事务  2pc，3pc提交  8.6. 分布式会话  分布式 session 如何设计（集成redis）  8.7 分布式ID 8.8 Zookeeper 9. 开发相关  常见的设计模式有哪些？ 单例模式的实现方式？  10. 微服务 11. 系统设计 11.1. 高并发  消息队列 读写分离 分库分表 负载均衡  11.2. 高可用  限流 降级 熔断 排队  12. 消息中间件  Kafka是如何保证消息的有效性的？ 使用 Kafka 的过程中有没有遇到过什么问题?如何解决？  13. 容器云   ","date":"2020年12月22日","permalink":"https://ahamoment.cn/posts/interview/interview-questions/","summary":"\u003cblockquote\u003e\n\u003cp\u003e面试题目录\u003c/p\u003e\n\u003c/blockquote\u003e","title":"面试题目录"},{"contents":" 本文收集有关分布式面试题的内容\n 1. 分布式理论   CAP 理论和 BASE 理论\n任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。\n  一致性协议\nraft 协议、paxos 算法\n  如何解决脑裂问题\n  gossip 原理\n  2. 分布式锁 3. 分布式事务 4. 分布式会话 5. 分布式id 6. 负载均衡 7. zookeeper vs etcd 存储数据时，zookeeper 使用树形结构，其中的每个节点称作 ZNode，访问一个 ZNode 时，需要提供从 root 开始的绝对路径。\nhttps://zhuanlan.zhihu.com/p/96690890\nhttps://imesha.me/apache-curator-vs-etcd3-9c1362600b26\n","date":"2020年12月18日","permalink":"https://ahamoment.cn/posts/interview/interview-distributed-system/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文收集有关分布式面试题的内容\u003c/p\u003e\n\u003c/blockquote\u003e","title":"分布式面试题"},{"contents":" 本文包含spring, spring boot, spring mvc 以及 mybatis的面试题\n 1. Spring 2. SpringBoot 2.1. SpringBoot 的常用注解有哪些？ @SpringBootApplication: 标识springboot项目的启动类。\n@Configuration: 用于定义配置类，指出该类是 Bean 配置的信息源，相当于传统的xml配置文件，一般加在主类上。如果有些第三方库需要用到xml文件，建议仍然通过@Configuration类作为项目的配置主类——可以使用@ImportResource注解加载xml配置文件。\n@ComponentScan: 组件扫描。让spring Boot扫描到Configuration类并把它加入到程序上下文。默认就会装配标识了@Controller，@Service，@Repository，@Component注解的类到spring容器中。\n@EnableAutoConfiguration: 允许 Spring Boot 自动配置注解，开启这个注解之后，Spring Boot 就能根据当前类路径下的包或者类来配置 Spring Bean。\n@RestController: 用于标注控制层组件，表示这是个控制器bean，并且是将函数的返回值直接填入HTTP响应体中，是REST风格的控制器；它是@Controller和@ResponseBody的合集。\n@ResponseBody: 表示该方法的返回结果直接写入HTTP response body中。一般在异步获取数据时使用，在使用@RequestMapping后，返回值通常解析为跳转路径，加上@responsebody后返回结果不会被解析为跳转路径，而是直接写入HTTP response body中。比如异步获取json数据，加上@Responsebody后，会直接返回json数据。\n@RequestMapping: RequestMapping是一个用来处理请求地址映射的注解；提供路由信息，负责URL到Controller中的具体函数的映射，可用于类或方法上。用于类上，表示类中的所有响应请求的方法都是以该地址作为父路径。\n@AutoWired: 把配置好的Bean拿来用，完成属性、方法的组装，它可以对类成员变量、方法及构造函数进行标注，完成自动装配的工作。\n@Qualifier: 当有多个同一类型的Bean时，可以用@Qualifier(\u0026ldquo;name\u0026rdquo;)来指定。与@Autowired配合使用。\n@Bean: 放在方法上面，而不是类上面，意思是该方法产生的Bean，交给 Spring 来管理。\n@Component: 泛指组件，当组件不好归类的时候，我们可以使用这个注解来标注。\n@Service: 一般作用于 service 层的组件。\n@Repository: 用于标注数据访问组件，即DAO组件。\n2.2. SpringBoot 异常处理 通过@ControllerAdvice和@ExceptionHandler来处理全局异常。\n详细内容参考文章：《SpringBoot异常处理》\n2.3. SpringBoot 中如何实现过滤器和拦截器？ （1）过滤器\nspring boot 中自定义过滤器只需要实现 Filter 接口，重写里面的init() ,doFilter(),destroy()方法即可。\npublic interface Filter { //初始化过滤器后执行的操作 default void init(FilterConfig filterConfig) throws ServletException { } // 对请求进行过滤 void doFilter(ServletRequest var1, ServletResponse var2, FilterChain var3) throws IOException, ServletException; // 销毁过滤器后执行的操作，主要用户对某些资源的回收 default void destroy() { } }  在配置中注册自定义的过滤器。\n@Configuration public class MyFilterConfig { @Autowired MyFilter myFilter; @Bean public FilterRegistrationBean\u0026lt;MyFilter\u0026gt; thirdFilter() { FilterRegistrationBean\u0026lt;MyFilter\u0026gt; filterRegistrationBean = new FilterRegistrationBean\u0026lt;\u0026gt;(); filterRegistrationBean.setFilter(myFilter); filterRegistrationBean.setUrlPatterns(new ArrayList\u0026lt;\u0026gt;(Arrays.asList(\u0026quot;/api/*\u0026quot;))); return filterRegistrationBean; } }  如果有多个过滤器，可以在配置中设置一下顺序，filterRegistrationBean.setOrder(xx);\n详情可以参考文章：《SpringBoot 实现过滤器》\n（2）拦截器\n如果你需要自定义 Interceptor 的话必须实现 org.springframework.web.servlet.HandlerInterceptor接口或继承 org.springframework.web.servlet.handler.HandlerInterceptorAdapter类，并且需要重写下面下面3个方法：\npublic boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex)  注意： preHandle方法返回 true或 false。如果返回 true，则意味着请求将继续到达 Controller 被处理。\n3. MyBatis","date":"2020年12月17日","permalink":"https://ahamoment.cn/posts/interview/interview-spring/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文包含spring, spring boot, spring mvc 以及 mybatis的面试题\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Spring系列面试题"},{"contents":" 本文收集Java基础知识点相关的面试题目\n 1. Java 基础 1.1. 语法 1.1.1 Java 泛型了解么？什么是类型擦除？介绍一下常用的通配符？ Java 泛型（generics）是 JDK 5 中引入的一个新特性, 泛型提供了编译时类型安全检测机制，该机制允许开发者在编译时检测到非法的类型。泛型的本质是参数化类型，也就是说所操作的数据类型被指定为一个参数。\nJava 的泛型是伪泛型，这是因为 Java 在编译期间，所有的泛型信息都会被擦掉，这也就是通常所说类型擦除 。 更多关于类型擦除的问题，可以查看这篇文章：《Java 泛型类型擦除以及类型擦除带来的问题》 。\nList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); list.add(12); //这里直接添加会报错 list.add(\u0026quot;a\u0026quot;); Class\u0026lt;? extends List\u0026gt; clazz = list.getClass(); Method add = clazz.getDeclaredMethod(\u0026quot;add\u0026quot;, Object.class); //但是通过反射添加，是可以的 add.invoke(list, \u0026quot;kl\u0026quot;); System.out.println(list)  泛型一般有三种使用方式:泛型类、泛型接口、泛型方法。\n1.泛型类：\n//此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型 //在实例化泛型类时，必须指定T的具体类型 public class Generic\u0026lt;T\u0026gt;{ private T key; public Generic(T key) { this.key = key; } public T getKey(){ return key; } }  如何实例化泛型类：\nGeneric\u0026lt;Integer\u0026gt; genericInteger = new Generic\u0026lt;Integer\u0026gt;(123456);  2.泛型接口 ：\npublic interface Generator\u0026lt;T\u0026gt; { public T method(); }  实现泛型接口，不指定类型：\nclass GeneratorImpl\u0026lt;T\u0026gt; implements Generator\u0026lt;T\u0026gt;{ @Override public T method() { return null; } }  实现泛型接口，指定类型：\nclass GeneratorImpl\u0026lt;T\u0026gt; implements Generator\u0026lt;String\u0026gt;{ @Override public String method() { return \u0026quot;hello\u0026quot;; } }  3.泛型方法 ：\npublic static \u0026lt; E \u0026gt; void printArray( E[] inputArray ) { for ( E element : inputArray ){ System.out.printf( \u0026quot;%s \u0026quot;, element ); } System.out.println(); }  使用：\n// 创建不同类型数组： Integer, Double 和 Character Integer[] intArray = { 1, 2, 3 }; String[] stringArray = { \u0026quot;Hello\u0026quot;, \u0026quot;World\u0026quot; }; printArray( intArray ); printArray( stringArray );  常用的通配符为： T，E，K，V，？\n ？ 表示不确定的 java 类型 T (type) 表示具体的一个 java 类型 K V (key value) 分别代表 java 键值中的 Key Value E (element) 代表 Element  更多关于 Java 泛型中的通配符可以查看这篇文章：《聊一聊-JAVA 泛型中的通配符 T，E，K，V，？》\n1.1.2. == 和 equals 的区别 == : 它的作用是判断两个对象的地址是不是相等。即判断两个对象是不是同一个对象。(基本数据类型==比较的是值，引用数据类型==比较的是内存地址)\n 因为 Java 只有值传递，所以，对于 == 来说，不管是比较基本数据类型，还是引用数据类型的变量，其本质比较的都是值，只是引用类型变量存的值是对象的地址。\n equals() : 它的作用也是判断两个对象是否相等，它不能用于比较基本数据类型的变量。equals()方法存在于Object类中，而Object类是所有类的直接或间接父类。\nObject类equals()方法：\npublic boolean equals(Object obj) { return (this == obj); }  equals() 方法存在两种使用情况：\n 情况 1：类没有覆盖 equals()方法。则通过equals()比较该类的两个对象时，等价于通过“==”比较这两个对象。使用的默认是 Object类equals()方法。 情况 2：类覆盖了 equals()方法，是对对象内容的比较。一般，我们都覆盖 equals()方法来两个对象的内容相等；若它们的内容相等，则返回 true(即，认为这两个对象相等)。  举个例子：\npublic class test1 { public static void main(String[] args) { String a = new String(\u0026quot;ab\u0026quot;); // a 为一个引用 String b = new String(\u0026quot;ab\u0026quot;); // b为另一个引用,对象的内容一样 String aa = \u0026quot;ab\u0026quot;; // 放在常量池中 String bb = \u0026quot;ab\u0026quot;; // 从常量池中查找 if (aa == bb) // true System.out.println(\u0026quot;aa==bb\u0026quot;); if (a == b) // false，非同一对象 System.out.println(\u0026quot;a==b\u0026quot;); if (a.equals(b)) // true System.out.println(\u0026quot;aEQb\u0026quot;); if (42 == 42.0) { // true System.out.println(\u0026quot;true\u0026quot;); } } }  说明：\n String 中的 equals 方法是被重写过的，因为 Object 的 equals 方法是比较的对象的内存地址，而 String 的 equals 方法比较的是对象的值。 当创建 String 类型的对象时，虚拟机会在常量池中查找有没有已经存在的值和要创建的值相同的对象，如果有就把它赋给当前引用。如果没有就在常量池中重新创建一个 String 对象。  String类equals()方法：\npublic boolean equals(Object anObject) { if (this == anObject) { return true; } if (anObject instanceof String) { String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) { char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- != 0) { if (v1[i] != v2[i]) return false; i++; } return true; } } return false; }  1.1.3. hashCode() 与 equals() 面试官可能会问你：“你重写过 hashcode 和 equals么，为什么重写 equals 时必须重写 hashCode 方法？”\n1)hashCode()介绍:\nhashCode() 的作用是获取哈希码，也称为散列码；它实际上是返回一个 int 整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode()定义在 JDK 的 Object 类中，这就意味着 Java 中的任何类都包含有 hashCode() 函数。另外需要注意的是： Object 的 hashcode 方法是本地方法，也就是用 c 语言或 c++ 实现的，该方法通常用来将对象的 内存地址 转换为整数之后返回。\npublic native int hashCode();  散列表存储的是键值对(key-value)，它的特点是：能根据“键”快速的检索出对应的“值”。这其中就利用到了散列码！（可以快速找到所需要的对象）\n2)为什么要有 hashCode？\n我们以HashSet 如何检查重复”为例子来说明为什么要有 hashCode？\n当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashcode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashcode 值作比较，如果没有相符的 hashcode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用 equals() 方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。\n3) 为什么重写 equals 时必须重写 hashCode 方法？\n如果两个对象相等，则 hashcode 一定也是相同的。两个对象相等，对两个对象分别调用 equals 方法都返回 true。但是，两个对象有相同的 hashcode 值，它们也不一定是相等的 。因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖。\n hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）\n 4)为什么两个对象有相同的 hashcode 值，它们也不一定是相等的？\n在这里解释一位小伙伴的问题。以下内容摘自《Head Fisrt Java》。\n因为 hashCode() 所使用的杂凑算法也许刚好会让多个对象传回相同的杂凑值。越糟糕的杂凑算法越容易碰撞，但这也与数据值域分布的特性有关（所谓碰撞也就是指的是不同的对象得到相同的 hashCode。\n我们刚刚也提到了 HashSet,如果 HashSet 在对比的时候，同样的 hashcode 有多个对象，它会使用 equals() 来判断是否真的相同。也就是说 hashcode 只是用来缩小查找成本。\n更多关于 hashcode() 和 equals() 的内容可以查看：Java hashCode() 和 equals()的若干问题解答\n1.2. 数据类型 1.2.1 String StringBuffer 和 StringBuilder 的区别是什么? String 为什么是不可变的? 简单的来说：String 类中使用 final 关键字修饰字符数组来保存字符串，private final char value[]，所以String 对象是不可变的。\n 在 Java 9 之后，String 类的实现改用 byte 数组存储字符串 private final byte[] value;\n 而 StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串char[] value 但是没有用 final 关键字修饰，所以这两种对象都是可变的。\n线程安全性\nString 中的对象是不可变的，也就可以理解为常量，线程安全。\nStringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。\nStringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。\n性能\n每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。\n对于三者使用的总结：\n 操作少量的数据: 适用 String 单线程操作字符串缓冲区下操作大量数据: 适用 StringBuilder 多线程操作字符串缓冲区下操作大量数据: 适用 StringBuffer  1.3. 方法 1.3.1 重载和重写的区别 重载就是同样的一个方法能够根据输入数据的不同，做出不同的处理。\n重写就是当子类继承自父类的相同方法，输入数据一样，但要做出有别于父类的响应时，你就要覆盖父类方法。\n   区别点 重载方法 重写方法     发生范围 同一个类 子类   参数列表 必须修改 一定不能修改   返回类型 可修改 子类方法返回值类型应比父类方法返回值类型更小或相等   异常 可修改 子类方法声明抛出的异常类应比父类方法声明抛出的异常类更小或相等；   访问修饰符 可修改 一定不能做更严格的限制（可以降低限制）   发生阶段 编译期 运行期    1.4 面向对象 1.4.1. 在 Java 中定义一个不做事且没有参数的构造方法的作用 Java 程序在执行子类的构造方法之前，如果没有用 super()来调用父类特定的构造方法，则会调用父类中“没有参数的构造方法”。因此，如果父类中只定义了有参数的构造方法，而在子类的构造方法中又没有用 super()来调用父类中特定的构造方法，则编译时将发生错误，因为 Java 程序在父类中找不到没有参数的构造方法可供执行。解决办法是在父类里加上一个不做事且没有参数的构造方法。\n1.4.2. 面对对象的三大特性 封装、继承和多态。\n封装是指把一个对象的状态信息（也就是属性）隐藏在对象内部，不允许外部对象直接访问对象的内部信息。\n继承：不同类型的对象，相互之间经常有一定数量的共同点。\n多态，顾名思义，表示一个对象具有多种的状态。具体表现为父类的引用指向子类的实例。\n1.4. 反射机制 JAVA 反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为 java 语言的反射机制。\n反射机制优缺点\n 优点： 运行期类型的判断，动态加载类，提高代码灵活度。 缺点： 1,性能瓶颈：反射相当于一系列解释操作，通知 JVM 要做的事情，性能比直接的 java 代码要慢很多。2,安全问题，让我们可以动态操作改变类的属性同时也增加了类的安全隐患。  反射的应用场景\n反射是框架设计的灵魂。\n在我们平时的项目开发过程中，基本上很少会直接使用到反射机制，但这不能说明反射机制没有用，实际上有很多设计、开发都与反射机制有关，例如模块化的开发，通过反射去调用对应的字节码；动态代理设计模式也采用了反射机制，还有我们日常使用的 Spring／Hibernate 等框架也大量使用到了反射机制。\n举例：\n 我们在使用 JDBC 连接数据库时使用 Class.forName()通过反射加载数据库的驱动程序； Spring 框架的 IOC（动态加载管理 Bean）创建对象以及 AOP（动态代理）功能都和反射有联系； 动态配置实例的属性；  1.5. 文件与IO流 1.5.1. BIO,NIO,AIO 有什么区别?  BIO (Blocking I/O): 同步阻塞 I/O 模式，数据的读取写入必须阻塞在一个线程内等待其完成。在活动连接数不是特别高（小于单机 1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。 NIO (Non-blocking/New I/O): NIO 是一种同步非阻塞的 I/O 模型，在 Java 1.4 中引入了 NIO 框架，对应 java.nio 包，提供了 Channel , Selector，Buffer 等抽象。NIO 中的 N 可以理解为 Non-blocking，不单纯是 New。它支持面向缓冲的，基于通道的 I/O 操作方法。 NIO 提供了与传统 BIO 模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞 I/O 来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发 AIO (Asynchronous I/O): AIO 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的 IO 模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。AIO 是异步 IO 的缩写，虽然 NIO 在网络操作中，提供了非阻塞的方法，但是 NIO 的 IO 行为还是同步的。对于 NIO 来说，我们的业务线程是在 IO 操作准备好时，得到通知，接着就由这个线程自行进行 IO 操作，IO 操作本身是同步的。查阅网上相关资料，我发现就目前来说 AIO 的应用还不是很广泛，Netty 之前也尝试使用过 AIO，不过又放弃了。  2. 容器 3. 参考 [1] JavaGuide\n","date":"2020年12月16日","permalink":"https://ahamoment.cn/posts/interview/interview-java/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文收集Java基础知识点相关的面试题目\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Java 基础面试题"},{"contents":" 本文主要收集面试过程中遇到的计算机网络的面试题。\n 1. OSI 与 TCP/IP 各层的结构与功能，都有哪些协议？ 学习计算机网络时我们一般采用折中的办法，也就是中和 OSI 和 TCP/IP 的优点，采用一种只有五层协议的体系结构，这样既简洁又能将概念阐述清楚。\n结合互联网的情况，自上而下地，非常简要的介绍一下各层的作用。\n1.1. 应用层 应用层(application-layer）的任务是通过应用进程间的交互来完成特定网络应用。应用层协议定义的是应用进程（进程：主机中正在运行的程序）间的通信和交互的规则。对于不同的网络应用需要不同的应用层协议。在互联网中应用层协议很多，如域名系统DNS，支持万维网应用的 HTTP协议，支持电子邮件的 SMTP协议等等。我们把应用层交互的数据单元称为报文。\n1.2. 运输层 运输层(transport layer)的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务。应用进程利用该服务传送应用层报文。“通用的”是指并不针对某一个特定的网络应用，而是多种应用可以使用同一个运输层服务。由于一台主机可同时运行多个线程，因此运输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面运输层的服务，分用和复用相反，是运输层把收到的信息分别交付上面应用层中的相应进程。\n运输层主要使用以下两种协议:\n 传输控制协议 TCP（Transmission Control Protocol）\u0026ndash;提供面向连接的，可靠的数据传输服务。 用户数据协议 UDP（User Datagram Protocol）\u0026ndash;提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性）。  TCP 与 UDP 的对比见问题三。\n1.3. 网络层 在 计算机网络中进行通信的两个计算机之间可能会经过很多个数据链路，也可能还要经过很多通信子网。网络层的任务就是选择合适的网间路由和交换结点， 确保数据及时传送。 在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组和包进行传送。在 TCP/IP 体系结构中，由于网络层使用 IP 协议，因此分组也叫 IP 数据报 ，简称 数据报。\n这里要注意：不要把运输层的“用户数据报 UDP ”和网络层的“ IP 数据报”弄混。另外，无论是哪一层的数据单元，都可笼统地用“分组”来表示。\n这里强调指出，网络层中的“网络”二字已经不是我们通常谈到的具体网络，而是指计算机网络体系结构模型中第三层的名称.\n互联网是由大量的异构（heterogeneous）网络通过路由器（router）相互连接起来的。互联网使用的网络层协议是无连接的网际协议（Internet Protocol）和许多路由选择协议，因此互联网的网络层也叫做网际层或IP层。\n1.4. 数据链路层 数据链路层(data link layer)通常简称为链路层。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层的协议。 在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息（如同步信息，地址信息，差错控制等）。\n在接收数据时，控制信息使接收端能够知道一个帧从哪个比特开始和到哪个比特结束。这样，数据链路层在收到一个帧后，就可从中提出数据部分，上交给网络层。 控制信息还使接收端能够检测到所收到的帧中有无差错。如果发现差错，数据链路层就简单地丢弃这个出了差错的帧，以避免继续在网络中传送下去白白浪费网络资源。如果需要改正数据在链路层传输时出现差错（这就是说，数据链路层不仅要检错，而且还要纠错），那么就要采用可靠性传输协议来纠正出现的差错。这种方法会使链路层的协议复杂些。\n1.5. 物理层 在物理层上所传送的数据单位是比特。\n物理层(physical layer)的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异， 使其上面的数据链路层不必考虑网络的具体传输介质是什么。“透明传送比特流”表示经实际电路传送后的比特流没有发生变化，对传送的比特流来说，这个电路好像是看不见的。\n在互联网使用的各种协中最重要和最著名的就是 TCP/IP 两个协议。现在人们经常提到的TCP/IP并不一定单指TCP和IP这两个具体的协议，而往往表示互联网所使用的整个TCP/IP协议族。\n2. TCP 三次握手和四次挥手   Q1: 什么是三次握手和四次挥手？ Q2: 为什么要三次握手？如果是两次握手会发生什么？ Q4: 为什么需要四次挥手？   这部分的内容网上有很多文章都讲的很不错，可以直接借鉴。\n  两张动图-彻底明白TCP的三次握手与四次挥手 面试官，不要再问我三次握手和四次挥手   上面的两篇文章都没有讲清楚为什么两次握手会有问题，我从谢希仁的《计算机网络》 一书中找到了比较完整的答案。\n 为什么客户端还要发送一次确认呢？这主要是为了防止已失效的连接请求报文突然又传送到了服务端，因而产生错误。\n所谓的“已失效的连接请求报文段”是这样产生的。考虑一种正常情况。客户端发出连接请求，但因为连接请求报文丢失而未收到确认。于是客户端再重传了一次连接请求。后来收到了确认，建立了连接。数据传输完毕后，就释放了连接。客户端共发送了两个连接请求报文段，其中第一个丢失，第二个到达了服务端。没有“已失效的连接请求报文段”。\n现在假设一种异常情况，即客户端发送出的第一个连接请求报文段并没有丢失，而是在某些网络结点长时间滞留了，以至延误到连接释放后的某个时间才到达服务端。本来这是一个早已失效的报文段。但是服务端收到此失效的连接报文段后，就误以为是客户端又发送一次新的请求。于是就向客户端发出确认报文段，同意建立连接。假定不采用三次握手，那么只要客户端发出确认，新的连接就建立了。\n由于现在客户端并没有发送建立连接的请求，因此不会理睬服务端的确认，也不会向服务端发送数据。但服务端却认为新的运输连接已经建立了，并一直等待客户端发送数据。服务端的许多资源就这样白白浪费了。\n采用三次握手可以防止上述现象。例如在刚才的情况下，客户端不会向服务端确认发送确认。服务端由于收不到确认，就知道客户端没有建立连接的要求。\n 3. TCP 协议如何保证可靠传输  应用数据被分割成 TCP 认为最适合发送的数据块。 TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 校验和： TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 TCP 的接收端会丢弃重复的数据。 流量控制： TCP 连接的每一方都有固定大小的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 （TCP 利用滑动窗口实现流量控制） 拥塞控制： 当网络拥塞时，减少数据的发送。 ARQ协议： 也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 超时重传： 当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。  3.1. 流量控制 （1）什么是流量控制？流量控制的目的？\n如果发送者发送数据过快，接收者来不及接收，那么就会有分组丢失。为了避免分组丢失，控制发送者的发送速度，使得接收者来得及接收，这就是流量控制。流量控制根本目的是防止分组丢失，它是构成TCP可靠性的一方面。\n（2）如何实现流量控制？\n由滑动窗口协议（连续ARQ协议）实现。滑动窗口协议既保证了分组无差错、有序接收，也实现了流量控制。主要的方式就是接收方返回的 ACK 中会包含自己的接收窗口的大小，并且利用大小来控制发送方的数据发送。\n（3）流量控制引发的死锁？怎么避免死锁的发生？\n当发送者收到了一个窗口为0的应答，发送者便停止发送，等待接收者的下一个应答。但是如果这个窗口不为0的应答在传输过程丢失，发送者一直等待下去，而接收者以为发送者已经收到该应答，等待接收新数据，这样双方就相互等待，从而产生死锁。 为了避免流量控制引发的死锁，TCP使用了持续计时器。每当发送者收到一个零窗口的应答后就启动该计时器。时间一到便主动发送报文询问接收者的窗口大小。若接收者仍然返回零窗口，则重置该计时器继续等待；若窗口不为0，则表示应答报文丢失了，此时重置发送窗口后开始发送，这样就避免了死锁的产生。\n3.2 . 拥塞控制  https://zhuanlan.zhihu.com/p/37379780\n 4. TCP 的粘包和拆包 参考: TCP 的粘包和拆包\n拆包和粘包是在socket编程中经常出现的情况，在socket通讯过程中，如果通讯的一端一次性连续发送多条数据包，tcp协议会将多个数据包打包成一个tcp报文发送出去，这就是所谓的粘包。而如果通讯的一端发送的数据包超过一次tcp报文所能传输的最大值时，就会将一个数据包拆成多个最大tcp长度的tcp报文分开传输，这就叫做拆包。\n总结出现粘包的原因：\n 要发送的数据小于TCP发送缓冲区的大小，TCP将多次写入缓冲区的数据一次发送出去； 接收数据端的应用层没有及时读取接收缓冲区中的数据； 数据发送过快，数据包堆积导致缓冲区积压多个数据后才一次性发送出去(如果客户端每发送一条数据就睡眠一段时间就不会发生粘包)；  解决方案\n对于粘包的情况，要对粘在一起的包进行拆包。对于拆包的情况，要对被拆开的包进行粘包，即将一个被拆开的完整应用包再组合成一个完整包。比较通用的做法就是每次发送一个应用数据包前在前面加上四个字节的包长度值，指明这个应用包的真实长度。如下图就是应用数据包格式。\n实现解决拆包粘包问题，有两种实现方式：\n 一种方式是引入netty库，netty封装了多种拆包粘包的方式，只需要对接口熟悉并调用即可，减少自己处理数据协议的繁琐流程； 自己写协议封装和解析流程，相当于实现了netty库拆粘包的简易版本.  5. 在浏览器中输入url地址到显示主页的过程 大致上分为以下几个过程：\n DNS解析 TCP连接 发送HTTP请求 服务器处理请求并返回HTTP报文 浏览器解析渲染页面 连接结束  可以参考下面这篇文章: 前端经典面试题: 从输入URL到页面加载发生了什么？\nhttps://github.com/Snailclimb/JavaGuide/blob/master/docs/network/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C.md\n","date":"2020年12月10日","permalink":"https://ahamoment.cn/posts/interview/interview-network/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文主要收集面试过程中遇到的计算机网络的面试题。\u003c/p\u003e\n\u003c/blockquote\u003e","title":"计算机网络面试题"},{"contents":"1. 什么是MySQL？ MySQL 是一种关系型数据库，在Java企业级开发中非常常用，因为 MySQL 是开源免费的，并且方便扩展。阿里巴巴数据库系统也大量用到了 MySQL，因此它的稳定性是有保障的。MySQL是开放源代码的，因此任何人都可以在 GPL(General Public License) 的许可下下载并根据个性化的需要对其进行修改。MySQL的默认端口号是3306。\n2. 存储引擎 2.1. MyISAM 和 InnoDB 的区别 MyISAM是MySQL的默认数据库引擎（5.5版之前）。虽然性能极佳，而且提供了大量的特性，包括全文索引、压缩、空间函数等，但MyISAM不支持事务和行级锁，而且最大的缺陷就是崩溃后无法安全恢复。不过，5.5版本之后，MySQL引入了InnoDB（事务性数据库引擎），MySQL 5.5版本后默认的存储引擎为InnoDB。\n大多数时候我们使用的都是 InnoDB 存储引擎，但是在某些情况下使用 MyISAM 也是合适的比如读密集的情况下。（如果你不介意 MyISAM 崩溃恢复问题的话）。\n两者的对比：\n 是否支持行级锁 : MyISAM 只有表级锁(table-level locking)，而InnoDB 支持行级锁(row-level locking)和表级锁，默认为行级锁。 是否支持事务和崩溃后的安全恢复： MyISAM 强调的是性能，每次查询具有原子性,其执行速度比InnoDB类型更快，但是不提供事务支持。但是InnoDB 提供事务支持，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 是否支持外键： MyISAM不支持，而InnoDB支持。 是否支持MVCC ：仅 InnoDB 支持。应对高并发事务, MVCC比单纯的加锁更高效;MVCC只在 READ COMMITTED 和 REPEATABLE READ 两个隔离级别下工作;MVCC可以使用 乐观(optimistic)锁 和 悲观(pessimistic)锁来实现;各数据库中MVCC实现并不统一。推荐阅读：MySQL-InnoDB-MVCC多版本并发控制  3. 索引 3.1. 什么是索引 索引是一种用于快速查询和检索数据的数据结构。常见的索引结构有: B树， B+树和Hash。\n索引的作用就相当于目录的作用。打个比方: 我们在查字典的时候，如果没有目录，那我们就只能一页一页的去找我们需要查的那个字，速度很慢。如果有目录了，我们只需要先去目录里查找字的位置，然后直接翻到那一页就行了。索引是一种用于快速查询和检索数据的数据结构。常见的索引结构有: B树， B+树和Hash。\n3.2. 为什么用索引？ 索引的优点\n可以大大加快检索的速度，这也是创建索引的最主要的原因。毕竟大部分系统的读请求总是大于写请求的。 另外，通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。\n索引的缺点\n 创建索引和维护索引需要耗费许多时间：当对表中的数据进行增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低SQL执行效率。 占用物理存储空间 ：索引需要使用物理文件存储，也会耗费一定空间。  3.3. 索引的原理 InnoDB 的索引使用的是 B+ 树。\n为什么要用B+树作为索引？而不是B树？\n MySQL索引原理及慢查询优化\n 3.4 索引类型 3.4.1. 主键索引与二级索引 （1）主键索引\n数据表的主键列使用的就是主键索引。一张数据表有只能有一个主键，并且主键不能为null，不能重复。在mysql的InnoDB的表中，当没有显示的指定表的主键时，InnoDB会自动先检查表中是否有唯一索引的字段，如果有，则选择该字段为默认的主键，否则InnoDB将会自动创建一个6Byte的自增主键。\n（2）二级索引\n又称为辅助索引，是因为二级索引的叶子节点存储的数据是主键。也就是说，通过二级索引，可以定位主键的位置。唯一索引，普通索引，前缀索引等索引属于二级索引。\n 唯一索引(Unique Key) ：唯一索引也是一种约束。**唯一索引的属性列不能出现重复的数据，但是允许数据为NULL，一张表允许创建多个唯一索引。**建立唯一索引的目的大部分时候都是为了该属性列的数据的唯一性，而不是为了查询效率。 普通索引(Index) ：普通索引的唯一作用就是为了快速查询数据，一张表允许创建多个普通索引，并允许数据重复和NULL。 前缀索引(Prefix) ：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小， 因为只取前几个字符。 全文索引(Full Text) ：全文索引主要是为了检索大文本数据中的关键字的信息，是目前搜索引擎数据库使用的一种技术。Mysql5.6之前只有MYISAM引擎支持全文索引，5.6之后InnoDB也支持了全文索引。  3.4.2. 聚集索引与非聚集索引 （1）聚集索引\n聚集索引即索引结构和数据一起存放的索引。主键索引属于聚集索引。\n聚集索引的优点:\n聚集索引的查询速度非常的快，因为整个B+树本身就是一颗多叉平衡树，叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。\n聚集索引的缺点:\n 依赖于有序的数据 ：因为B+树是多路平衡树，如果索引的数据不是有序的，那么就需要在插入时排序，如果数据是整型还好，否则类似于字符串或UUID这种又长又难比较的数据，插入或查找的速度肯定比较慢。 更新代价大 ： 如果对索引列的数据被修改时，那么对应的索引也将会被修改， 而且况聚集索引的叶子节点还存放着数据，修改代价肯定是较大的， 所以对于主键索引来说，主键一般都是不可被修改的。  （2）非聚集索引\n非聚集索引即索引结构和数据分开存放的索引。\n非聚集索引的叶子节点并不一定存放数据的指针， 因为二级索引的叶子节点就存放的是主键，根据主键再回表查数据。\n非聚集索引的优点:\n更新代价比聚集索引要小 。非聚集索引的更新代价就没有聚集索引那么大了，非聚集索引的叶子节点是不存放数据的\n非聚集索引的缺点:\n 跟聚集索引一样，非聚集索引也依赖于有序的数据 可能会二次查询(回表) :这应该是非聚集索引最大的缺点了。 当查到索引对应的指针或主键后，可能还需要根据指针或主键再到数据文件或表中查询。  （3）非聚集索引一定回表查询吗(覆盖索引)?\n非聚集索引不一定回表查询。\n 试想一种情况，用户准备使用SQL查询用户名，而用户名字段正好建立了索引。\n  SELECT name FROM table WHERE username='guang19';   那么这个索引的key本身就是name，查到对应的name直接返回就行了，无需回表查询。\n 主键索引本身的key就是主键，查到返回就行了。这种情况就称之为覆盖索引了。\n3.5. 索引创建原则 （1）单列索引\n单列索引即由一列属性组成的索引。\n（2）联合索引(多列索引)\n联合索引既由多列属性组成索引。\n（3）最左前缀原则\n假设创建的联合索引由三个字段组成:\nALTER TABLE table ADD INDEX index_name (num,name,age)  那么当查询的条件有为:num / (num AND name) / (num AND name AND age)时，索引才生效。所以在创建联合索引时，尽量把查询最频繁的那个字段作为最左(第一个)字段。查询的时候也尽量以这个字段为第一条件。\n4. 事务 事务是逻辑上的一组操作，要么都执行，要么都不执行。\n事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。\n4.1. 事务的四大特性  原子性（Atomicity）： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性（Consistency）： 执行事务后，数据库从一个正确的状态变化到另一个正确的状态； 隔离性（Isolation）： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性（Durability）： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。  4.2. 并发事务带来了哪些问题 在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。\n 脏读（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据（A 写 B 读）。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。 丢失修改（Lost to modify）: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改（A写 B 写）。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。 不可重复读（Unrepeatableread）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读（A 读 B 写 A 读）。 幻读（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。（A 读 B 写 A 读）  不可重复读和幻读区别：\n不可重复读的重点是修改比如多次读取一条记录发现其中某些列的值被修改，幻读的重点在于新增或者删除比如多次读取一条记录发现记录增多或减少了。\n4.3. 事务的隔离级别  READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。     隔离级别 脏读 不可重复读 幻影读     READ-UNCOMMITTED √ √ √   READ-COMMITTED × √ √   REPEATABLE-READ × × √   SERIALIZABLE × × ×    MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）。我们可以通过SELECT @@tx_isolation;命令来查看，MySQL 8.0 该命令改为SELECT @@transaction_isolation;\nmysql\u0026gt; SELECT @@tx_isolation; +-----------------+ | @@tx_isolation | +-----------------+ | REPEATABLE-READ | +-----------------+  这里需要注意的是：与 SQL 标准不同的地方在于 InnoDB 存储引擎在 REPEATABLE-READ（可重读） 事务隔离级别下使用的是Next-Key Lock 锁算法，因此可以避免幻读的产生，这与其他数据库系统(如 SQL Server) 是不同的。所以说InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读） 已经可以完全保证事务的隔离性要求，即达到了 SQL标准的 SERIALIZABLE(可串行化) 隔离级别。因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是 READ-COMMITTED(读取提交内容) ，但是你要知道的是InnoDB 存储引擎默认使用 REPEAaTABLE-READ（可重读） 并不会有任何性能损失。\nInnoDB 存储引擎在 分布式事务 的情况下一般会用到 SERIALIZABLE(可串行化) 隔离级别。N,MF. ÔV ","date":"2020年12月09日","permalink":"https://ahamoment.cn/posts/interview/interview-mysql/","summary":"1. 什么是MySQL？ MySQL 是一种关系型数据库，在Java企业级开发中非常常用，因为 MySQL 是开源免费的，并且方便扩展。阿里巴巴数据库系统也大量用到了 M","title":"MySQL 面试题"},{"contents":"1. 撤销提交 撤销提交属于误操作的范畴，Git 误操作的类型主要有以下两个方面：\n commit - 分支提交错误 reset - 误删代码  1.1 分支提交错误 有时我们会遇到这种情况：我们从develop 分支新建一个名为feat/home 分支去做A功能，然后由于一些其他原因A 功能需要延后，然后我们再从develop分支新建一个分支去做B功能或者C功能，在多分支多功能开发时，就容易出现做B功能时，忘记切换分支，一直等做完了提交了push之后才发现 push 错了远端的分支，并且 push 的改动与该分支需要开发的功能并没有交集，因此我们需要将已经提交错的分支内容回滚并提交push 到正确的远端分支。\n此时有两种情况:\n场景一：已经commit，但是未push到远端 使用git reset命令，可以在提交层面在私有分支舍弃一些没有提交的修改：\n# 回退到上一个版本 git reset --hard HEAD^  git reset 命令主要有三个选项： \u0026ndash;soft、\u0026ndash;mixed 、\u0026ndash;hard，默认参数为 \u0026ndash;mixed。\ngit reset \u0026ndash;soft 提交：\n--soft 这个版本的命令有“最小”影响，只改变一个符号引用的状态使其指向一个新提交，不会改变其索引和工作目录， 具体体现如下：\n# 模拟一份提交历史 git add 1.js \u0026amp;\u0026amp; git commit -m \u0026quot;update part 1\u0026quot; git add 2.js \u0026amp;\u0026amp; git commit -m \u0026quot;update part 2\u0026quot; git add 3.js \u0026amp;\u0026amp; git commit -m \u0026quot;update part 3\u0026quot; git add 4.js \u0026amp;\u0026amp; git commit -m \u0026quot;update part 4\u0026quot; git log --oneline --graph -4 --decorate  # 用 --soft 参数尝试回退一个版本 git reset --soft HEAD~1  当我们执行 --soft 命令后，可以看到控制台无任何输出，此时再次查看当前提交历史：\ngit log --oneline --graph -4 --decorate  如下图，可以看到版本库已经回退了一个版本：\n执行 git status，可以看到SHA1为54b1941 的commit 上的更改回到了缓存区：\n因此我们可以认为 \u0026ndash;soft 操作是软重置，只撤销了git commit操作，保留了 git add 操作。\ngit reset \u0026ndash;hard 提交:\n此时接上面的流程，我们这次执行 --hard 操作，尝试回退两个版本：\ngit reset --hard HEAD~2  如下图，可以看到版本库回退了两个版本，并且将本地版本库的头指针全部重置到了指定版本，暂存区也会被重置，工作区的代码也回退到了这一版本：\n执行git status 可以看到 我们的 SHA1 为 54b1941的 commit 上做的修改都“丢失”了，新的文件也被删除了。\n因此可以知道，git commit --hard 是具有破坏性，是很危险的操作，它很容易导致数据丢失，如果我们真的进行了该操作想要找回丢失的数据，那么此时可以使用git reflog 回到未来，找到丢失的commit。这个命令的具体使用会在文章后面介绍。\ngit reset \u0026ndash;mixed 提交：\n我们重新造一系列 commit 历史：\ngit add 1.js \u0026amp;\u0026amp; git commit -m \u0026quot;update 1.js\u0026quot; git add 2.js \u0026amp;\u0026amp; git commit -m \u0026quot;update 2.js\u0026quot; git add 3.js \u0026amp;\u0026amp; git commit -m \u0026quot;update 3.js\u0026quot; git add 4.js \u0026amp;\u0026amp; git commit -m \u0026quot;update 4.js\u0026quot; git add 5.js \u0026amp;\u0026amp; git commit -m \u0026quot;update 5.js\u0026quot; git log --oneline --graph -4 --decorate  可以看到当前的 commit 历史如下：\n此时执行\u0026ndash;mixed 操作，尝试回退两个版本：\n# 等价于 git reset HEAD~2 git reset --mixed HEAD~2  提交历史此时改变为下图所示：\n此时执行 git status ，命令行输出如下：\nHEAD、索引被更改，工作目录未被更改\nSourceTree 工具上的直观显示如下：\n可以看出，该命令加上 \u0026ndash;mixed 参数会保留提交的源码改动，只是将索引信息回退到了某一个版本，如果还需要继续提交，再次执行 git add 和 git commit\n介绍完git reset，那么我们来说一下如何用该命令解决提交分支错误的问题:\n第一种方法：\n适用于多个分支一起开发的时候将A分支的改动错误的提交到B的场景：\n# 将该分支的本不应该提交的commit撤销 git reset HEAD^ # 按需选择想要回到哪个版本 # 回到HEAD git reset --soft HEAD # 回到HEAD的前一个版本 git reset --soft HEAD^ # 回到HEAD的前10个版本 git reset --soft HEAD~5 # 利用id回到指定版本 git reset --soft a06ef2f # 将撤销的代码暂存起来 git stash # 切换到正确的分支 git checkout feat/xxx # 重新应用缓存 git stash pop # 在正确的分支进行提交操作 git add . \u0026amp;\u0026amp; git commit -m \u0026quot;update xxxx\u0026quot;  第二种方法：\n适用于在不小心在 master 分支上提交了代码，而实际想要在 feature 分支上提交代码的场景：\n# 新检出一个新分支，但是仍在master 分支上，并不会切换到新分支 git branch feat/update # 恢复master本身提交的状态 git reset --hard origin/master # 提交错的代码已经在新检出的分支上面了，可以继续进行开发或者push git checkout feat/update  第三种方法：\n 适用于想要对特定的某一个或几个commit 进行“嫁接”，使其复制一份到正确的 feature 分支的场景； 在功能性迭代开发中发现一个bug，并提交了一个commit 进行修复，但是发现该bug也存在线上的发布版本上，必须要尽快对线上进行修复，此时可以使用git cherry-pick 将bug 修复的commit 嫁接到 fix 分支上进行代码修复，并及时发布，解决线上bug。  # 先切换到正确的分支 git checkout feat/update # 取出提交错误的或bug fix的 commit 引入到feat/update 分支中 git cherry-pick a06ef2f # 回到错误的分支 git checkout feat/feedback # 将 a06ef2f 的改动从当前分支销毁 git reset --head a06ef2f  上面演示的是“嫁接” 一个commit，如果想要嫁接多个 commit 可以这样做：\n# 将三个commit 合并过来 git cherry-pick b9dabf9 e2c739d dad9e51  如果想加个一个应用范围内的 commit，可以这样做：\ngit cherry-pick 422db47..e2c739d  需要注意的是无论是对单个 commit 进行 git cherry-pick ，还是批量处理，注意一定要根据时间线，依照 commit 的先后顺序来处理。\n如果你只想把改动转移到目标分支，但是并不想提交，可以这样做：\n# --no-commit 参数会使嫁接过来的改动不会提交，只会放在暂存区 git cherry-pick b9dabf9 --no-commit  第四种方法：\n适用于当多个文件被缓存时，发现其中一个文件是其他分支的功能性改动，想直接取消该文件的缓存：\n# 编辑了 1.js 2.js 3.js # 缓存所有改动的文件 git add . # 发现 3.js 不应该出现在此时提交的功能上，要取消它的缓存 git reset 3.js # 此时3.js 被取消了缓存，我们继续提交1.js 2.js git commit -m \u0026quot;Update 1.js 2.js\u0026quot; # 将3.js 暂存起来 git stash # 切换到提交 3.js 改动的分支 git checkout feat/update # 重新应用缓存起来的 stash（3.js） # pop 参数会将缓存栈的第一个stash删除，并将对应修改应用到当前分支目录下 git stash pop # 继续提交 git add \u0026amp;\u0026amp; git commit -m \u0026quot;update 3.js\u0026quot;  场景2：Commit之后已经 push 到远端 场景：假设我们在 feat/feedback 分支上发现最后一次 commit 的功能是feat/update 分支的改动，此时想要取消这次commit（update 2.js）\n下图是feat/feedback 的提交历史：\n此时我们需要借助 git revert 命令来撤销我们的操作。\n解决方式：\n# 撤销最近的一次提交 git revert HEAD --no-edit  接着我们使用 sourceTree 查看撤销之后的提交历史：\n我们看到想要撤销的 SHA1 为 db6bb3 的 commit（Update 2.js）记录还在，并且多了一个SHA1 为 6e1d7ee 新的 commit（Revert \u0026ldquo;Update 2.js\u0026rdquo;）。因此可以看出，git revert 是对给定的 commit 提交进行逆过程，该命令会引入一个新的提交来抵消给定提交的影响。 和 git cherry-pick 一样，revert命令不修改版本库的现存历史记录，相反它只会在记录添加新的提交。\n接下来我们已经解决了错误分支的提交，但是还要把这次提交放到正确的分支上，依然可以使用 git cherry pick 去操作：\n# 将revert commit push到远端 git push origin feat/feedback # 切换到正确的分支 git checkout feat/update 将目标commit 嫁接到当前分支 git cherry pick db6bb3f  git revert 后面可以加不同的参数达到不同的撤销效果，常用的如下：\n--edit ：该参数为git revert 的默认参数，它会自动创建提交日志提醒，此时会弹出编辑器会话，可以在里面修改提交消息，然后再提交。\ngit revert 6ac5152 --edit  --no-edit ：表示不编辑 commit 信息，revert 的 commit 会直接自动变回 \u0026lsquo;Revert + 想要撤销的commit 的message\u0026rsquo; 的格式。上面例子中使用的就是这种方式。\n--no-commit：该命令会使撤销的 commit 里面的改动放到暂存区，不进行提交，用户可以自行再次提交。这种参数并且适用于将多个 commit 结果还原到索引中，集体放置在缓冲区，进行用户自定义的操作。\ngit revert 13b7faf --no-commit  场景3：改动不仅已经 push 到远端，并且已经合到主仓库 当我们把本不属于该分支的代码或者不需要提交的改动提交到主仓库，并合并到了develop 仓库之后，这是想要撤销合到主仓库的改动，解决方式如下：\n1. 当以pull request 的方式进行的合并\n在团队的 github flow 流程中，若我们把问题分支的 pull request 请求通过并合并到develop 之后，我们可以看到在 open a pull request 页面有如下提示：\n这时我们可以直接点击 【Revert】按钮进行撤回。该撤回操作会提示你需要创建一个 revert pull request，格式默认为： revert-${问题pr号}-${问题分支}\n最后我们将revert 产生的 pull request 合并到 develop 分支。\n因此对于团队协作中，推荐的工作流程是如上图所示在一个新分支中恢复错误的提交。在这里有人会问，为什么不直接在 develop 分支进行 git revert 操作，岂不是更方便，何必麻麻烦烦的去多建一个分支出来？\n这么做的原因是：在拥有大量开发人员的团队中， develop、master 分支为保护分支，为了安全不允许或建议去直接修改。\n通过这次操作我们可以了解到：revert 分支的操作实际上是合并进develop 分支的逆操作，它会新产生一个新的分支，将 feat/feedback 的改动还原。\n在团队协作流程中，通常我们会使用 Github 的【Merge pull request】 绿色按钮进行合并pull request 的操作，因为这样会更简单直观，建议始终使用该绿色按钮进行操作。\n2. 当用命令行执行合并时\n上面展示了通过界面按钮去操作如何撤销已经合并develop 分支的改动，那么在个人项目中用命令行操作是怎么样的呢？\n# 添加三个文件 echo 1 \u0026gt; 1.html echo 2 \u0026gt; 2.html echo 3 \u0026gt; 3.html # 以为提交的是1.html 2.html，将改动推到了远端分支 git add . \u0026amp;\u0026amp; git commit -m \u0026quot;Add 1.html 2.html\u0026quot; git push origin feat/update # 将feat/update的改动创建一个“合并提交”合入develop 分支，生成的 Merge commit 的SHA1 为 f439c6f git checkout develop git merge feat/update --no-ff # 如果存在冲突，先解决冲突，然后继续请求合并 git add . \u0026amp;\u0026amp; git merge --continue # 将develop 合并的最后结果提交到远端 git push origin develop # 合并之后发现不应该将3.html 不应该放入功能迭代中。需要撤销本次合并 # 做任何操作前，先保证本地的develop 代码是最新状态 git pull --rebase origin develop # 从develop分支新建一个 revert 分支 git checkout -b revert-feat/update # 用 -m 参数指定父编号（从1开始），因为它是“合并提交” git revert -m 1 f439c6f # push revert 的改动 git push origin revert-feat/update # 切换回 develop 分支，将 revert-feat/update 分支进行合并 git checkout develop git merge revert-feat/update --no-ff git push origin develop  图为新建revert 分支：\n图为git revert 弹出编辑器编辑 revert commit message 过程：\n图为执行完git revert 之后的 commit 历史记录：\n接下来我们想将 3.html 的改动撤销的操作就变成了上面场景 2 的操作流程了。\n场景4：revert 错误，需要再次补救 当我们的代码合到主仓库，并且成功发布到生产环境，此时发现线上有集中报错，必须马上将线上代码回滚到最新版本。这是我们需要进行revert 操作。revert 的代码发布到生产之后，发现错误仍旧存在，最后排查到是某个外部服务依赖出现问题，本次revert 的改动无关，并且外部服务已经恢复。此时需要将 revert 的改动再次发布上生产环境。\n我们可以再用一次git revert，revert 掉我们之前的 revert commit：\ngit revert HEAD --no-edit  这样 revert 撤销的改动又回来了，此时会发现提交历史上又会出现一个新的revert commit。\n1.2. 误删代码 介绍上面提交错误 commit 的时候，我们提到了git reset --hard。 如果我们真的使用了git reset --hard 之后，发现某些修改还有必要的，这时候就需要借助时光机 git reflog “回来未来”了。\ngit reflog 是非常好用的“后悔药”，它几乎可以恢复我们 commit 过的改动，即使这条 commit 记录已经被我们 reset 掉了。\n具体演示如下：\n如上图，在当前提交历史中，我们认为最新的两个commit 已经没有用了，想直接reset 到 SHA1 为 c48a245 这个 commit：\n# 回到 c48a245 commit git reset --hard c48a245  此时提交历史变为现在这样：\n此时可以看到SHA1 为c48a245 的 commit 时间线之后的改动都已经被撤销了。 这时候我们突然想到：commit 信息为 “Add 1.html 2.html” 的提交里面的改动很重要，需要被找回，但是我们使用 git log 查看过去提交历史，已经找不到这条被我们 reset 掉的历史记录了。这时候进行如下操作：\ngit reflog  我们如愿以偿的看到了曾经提交过的这个想要找回的commit（commit: Add 1.html 2.html），它的 SHA1 为 cf2e245。\n接下来怎么做取决于你具体想要达到什么目的：\n 想要回到cf2e245 这个特定的commit：  git reset --hard cf2e245   想要暂存 cf2e245 中的改动，并且不想马上提交：  git reset --soft cf2e245   想要把cf2e245 嫁接到某个分支目录下：  git checkout feat/xxx git cherry-pick cf2e245   想要找回 cf2e245 某个文件的改动，暂存起来：  git checkout cf2e245 1.html  对于 git reflog 需要注意的是： 它不是万能的。Git 会定期清理那些你已经不再用到的“对象”，如果你想找到几个月以前的提交，可能会指望不上它。\n2. filemode 的变化 执行git diff filename ,出现 old mode 100644 new mode 100755 的提示，如下图：\n但是发现文件内容并没有发生改变\n产生这个问题的原因就是：filemode的变化，文件chmod后其文件某些位是改变了的，如果严格的比较原文件和chmod后的文件，两者是有区别的，但是源代码通常只关心文本内容，因此chmod产生的变化应该忽略，所以设置一下：\n切到源码的根目录下，\ngit config --add core.filemode false  这样你的所有的git库都会忽略filemode变更了。\n参考  Git 误操作救命篇一： 如何将改动撤销\ngit diff old mode 100644 new mode 100755\n ","date":"2020年11月24日","permalink":"https://ahamoment.cn/posts/tool/tool-git-common-operations/","summary":"1. 撤销提交 撤销提交属于误操作的范畴，Git 误操作的类型主要有以下两个方面： commit - 分支提交错误 reset - 误删代码 1.1 分支提交错误 有时我们会遇到这种情况：","title":"Git 常用命令汇总"},{"contents":"1. 为什么要用线程池？  池化技术相比大家已经屡见不鲜了，线程池、数据库连接池、Http 连接池等等都是对这个思想的应用。池化技术的思想主要是为了减少每次获取资源的消耗，提高对资源的利用率。\n 线程池提供了一种限制和管理资源（包括执行一个任务）。 每个线程池还维护一些基本统计信息，例如已完成任务的数量。\n这里借用《Java 并发编程的艺术》提到的来说一下使用线程池的好处：\n 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。  2. 自定义一个简单的线程池 一个线程池应该具备以下要素：\n 任务队列：用于缓存提交的任务。 任务线程管理功能：一个线程池必须能够很好地管理和控制线程数量，可通过如下三个参数来实现，比如创建线程池时初始的线程数量init；线程池自动扩充时最大的线程数量max；在线程池空闲时需要释放线程但是也要维护一定数量的活跃数量或者核心数量core。有了这三个参数，就能够很好地控制线程池中的线程数量，将其维护在一个合理的范围之内，三者之间的关系是init＜=core＜=max。 任务拒绝策略：如果线程数量已达到上限且任务队列已满，则需要有相应的拒绝策略来通知任务提交者。 线程工厂：主要用于个性化定制线程，比如将线程设置为守护线程以及设置线程名称等。 QueueSize：任务队列主要存放提交的Runnable，但是为了防止内存溢出，需要有limit数量对其进行控制。 Keepedalive时间：该时间主要决定线程各个重要参数自动维护的时间间隔。  2.1 线程池实现类图 上图为线程池实现类图，下面看具体的代码。\n2.2 ThreadPool 先定义一个线程池接口，定义常用的方法。\npublic interface ThreadPool { //提交任务到线程池 void execute(Runnable runnable); // 关闭线程池 void shutdown(); // 获取线程池的初始化大小 int getInitSize(); // 获取线程池的最大线程数 int getMaxSize(); // 获取线程池的核心线程数 int getCoreSize(); // 获取线程池中用于缓存任务队列的大小 int getQueueSize(); // 获取线程池中活跃线程的数量 int getActiveCount(); // 查看线程池是否已经被shutdown boolean isShutdown(); }  2.3 RunnableQueue 我们需要一个任务队列，用来存放提交的任务，该队列是一个BlockedQueue，并且有limit的限制。\npublic interface RunnableQueue { // 当有新任务进来时首先会offer到队列中 void offer(Runnable runnable); // 工作线程通过take方法获取Runnable Runnable take() throws InterruptedException; // 获取任务队列中任务的数量 int size(); }  2.4 ThreadFactory ThreadFactory提供了创建线程的接口，以便于个性化地定制Thread，比如Thread应该被加到哪个Group中，优先级、线程名字以及是否为守护线程等。\n@FunctionalInterface public interface ThreadFactory { Thread createThread(Runnable runnable); }  2.5 拒绝策略（DenyPolicy） DenyPolicy主要用于当Queue中的runnable达到了limit上限时，决定采用何种策略通知提交者。该接口中定义了三种默认的实现。\n DiscardDenyPolicy：直接将任务丢弃。 AbortDenyPolicy：向任务提交者抛出异常。 RunnerDenyPolicy：使用提交者所在的线程执行任务。  public interface DenyPolicy { void reject(Runnable runnable, ThreadPool threadPool); /** * 该拒绝策略会直接将任务丢弃 */ class DiscardDenyPolicy implements DenyPolicy { @Override public void reject(Runnable runnable, ThreadPool threadPool) { //do nothing System.out.println(\u0026quot;task will be discard\u0026quot;); } } /** * 该拒绝策略会向任务提交者抛出异常 */ class AbortDenyPolicy implements DenyPolicy { @Override public void reject(Runnable runnable, ThreadPool threadPool) { throw new RunnableDenyException(\u0026quot;The Runnable \u0026quot; + runnable + \u0026quot; will be abort.\u0026quot;); } } /** * 该拒绝策略会使任务在提交者所在的线程中执行任务 */ class RunnerDenyPolicy implements DenyPolicy { @Override public void reject(Runnable runnable, ThreadPool threadPool) { if (!threadPool.isShutdown()) { runnable.run(); } } } }  这里还定义了一个 RunnableDenyException ，主要用于通知任务提交者，任务队列已经无法再接收新的任务。\npublic class RunnableDenyException extends RuntimeException{ public RunnableDenyException(String message) { super(message); } }  2.6 InternalTask InternalTask是Runnable的一个实现，是实际任务存储的数据结构。主要用于线程池内部，该类会使用到RunnableQueue，然后不断地从queue中取出某个runnable，并运行runnable的run方法。\npublic class InternalTask implements Runnable{ private final RunnableQueue runnableQueue; private volatile boolean running = true; public InternalTask(RunnableQueue runnableQueue) { this.runnableQueue = runnableQueue; } @Override public void run() { // 如果当前任务为running并且没有被中断，则其将不断地从queue中获取runnable，然后执行run方法 // 这是提交到线程池的任务最终运行的地方 while (running \u0026amp;\u0026amp; !Thread.currentThread().isInterrupted()) { try { Runnable task = runnableQueue.take(); task.run(); } catch (InterruptedException e) { running = false; break; } } } // 停止当前任务，主要会在线程池的shutdown方法中使用 public void stop() { this.running = false; } }  代码还对该类增加了一个开关方法stop，主要用于停止当前线程，一般在线程池销毁和线程数量维护的时候会使用到。\n2.7 线程池详细实现 在LinkedRunnableQueue中有几个重要的属性，第一个是limit，也就是Runnable队列的上限；当提交的Runnable数量达到limit上限时，则会调用DenyPolicy的reject方法；runnableList是一个双向循环列表，用于存放Runnable任务\npublic class LinkedRunnableQueue implements RunnableQueue{ // 任务队列的最大容量，在构造时传入 private final int limit; // 若任务队列已满，则执行拒绝策略 private final DenyPolicy denyPolicy; // 存放任务的队列 private final LinkedList\u0026lt;Runnable\u0026gt; runnableList = new LinkedList\u0026lt;\u0026gt;(); private final ThreadPool threadPool; public LinkedRunnableQueue(int limit, DenyPolicy denyPolicy, ThreadPool threadPool) { this.limit = limit; this.denyPolicy = denyPolicy; this.threadPool = threadPool; } @Override public void offer(Runnable runnable) { synchronized (runnableList) { if (runnableList.size() \u0026gt;= limit) { // 无法容纳新的任务，执行拒绝策略 denyPolicy.reject(runnable, threadPool); } else { // 将任务加入队尾，并且唤醒阻塞中的线程 runnableList.addLast(runnable); runnableList.notifyAll(); } } } /** * take方法也是同步方法，线程不断从队列中获取Runnable任务，当队列为空的时候工作线程会陷入阻塞， * 有可能在阻塞的过程中被中断，为了传递中断信号需要在catch语句块中将异常抛出以通知上游（InternalTask） * @return 任务 * @throws InterruptedException 中断异常，通知上游(InternalTask) */ @Override public Runnable take() throws InterruptedException { synchronized (runnableList) { while (runnableList.isEmpty()) { try { // 如果任务队列中没有可执行任务，则当前线程挂起，进入runnableList关联的monitor waitset中等待唤醒 runnableList.wait(); } catch (InterruptedException e) { // 被中断时需要将异常抛出 throw e; } } // 从任务队列头排除一个任务 return runnableList.removeFirst(); } } @Override public int size() { return runnableList.size(); } }  根据前面的讲解，线程池需要有数量控制属性、创建线程工厂、任务队列策略等功能，线程池初始化代码如下：\npublic class BasicThreadPool extends Thread implements ThreadPool{ // 初始化线程数量 private final int initSize; // 线程池最大数量 private final int maxSize; // 线程池核心线程数量 private final int coreSize; // 当前活跃的线程数量 private int activeCount; // 创建线程所需的工厂 private final ThreadFactory threadFactory; // 任务队列 private final RunnableQueue runnableQueue; // 线程池是否已经被shutdown private volatile boolean isShutdown = false; // 工作线程队列 private final Queue\u0026lt;ThreadTask\u0026gt; threadQueue = new ArrayDeque\u0026lt;\u0026gt;(); private final static DenyPolicy DEFAULT_DENY_POLICY = new DenyPolicy.DiscardDenyPolicy(); private final static ThreadFactory DEFAULT_THREAD_FACTORY = new DefaultThreadFactory(); private final long keepAliveTime; private final TimeUnit timeUnit; // 构造时需要传递的参数：初始的线程数量，最大的线程数量，核心线程数量，任务队列的最大数量 public BasicThreadPool(int initSize, int maxSize, int coreSize, int queueSize) { this(initSize, maxSize, coreSize, DEFAULT_THREAD_FACTORY, queueSize, DEFAULT_DENY_POLICY, 10 ,TimeUnit.SECONDS); } // 构造线程池时需要传入的参数，该构造函数需要的参数比较多 public BasicThreadPool(int initSize, int maxSize, int coreSize, ThreadFactory threadFactory, int queueSize, DenyPolicy denyPolicy, long keepAliveTime, TimeUnit timeUnit) { this.initSize = initSize; this.maxSize = maxSize; this.coreSize = coreSize; this.threadFactory = threadFactory; this.runnableQueue = new LinkedRunnableQueue(queueSize, denyPolicy, this); this.keepAliveTime = keepAliveTime; this.timeUnit = timeUnit; this.init(); } // 初始化时，先创建 initSize 个线程 private void init() { start(); for (int i = 0; i \u0026lt; initSize; i++) { newThread(); } } private void newThread() { //创建任务线程，并且启动 InternalTask internalTask = new InternalTask(runnableQueue); Thread thread = this.threadFactory.createThread(internalTask); ThreadTask threadTask = new ThreadTask(thread, internalTask); threadQueue.offer(threadTask); this.activeCount++; thread.start(); } private void removeThread() { // 从线程池中移除某个线程 ThreadTask threadTask = threadQueue.remove(); threadTask.internalTask.stop(); this.activeCount--; } @Override public void execute(Runnable runnable) { if (this.isShutdown) { throw new IllegalStateException(\u0026quot;The thread pool is destroy\u0026quot;); } // 提交任务只是简单地往任务队列中插入Runnable this.runnableQueue.offer(runnable); } @Override public void run() { // run 方法继承自Thread，主要用于维护线程数量，比如扩容、回收工作 while (!isShutdown \u0026amp;\u0026amp; !isInterrupted()) { try { timeUnit.sleep(keepAliveTime); } catch (InterruptedException e) { isShutdown = true; break; } synchronized (this) { if (isShutdown) { break; } //当前队列中有尚未处理，并且activeCount\u0026lt;coreSize则继续扩容 if (runnableQueue.size() \u0026gt; 0 \u0026amp;\u0026amp; activeCount \u0026lt; coreSize) { for (int i = initSize; i \u0026lt; coreSize; i++) { newThread(); } // continue 的目的在于不想让线程的扩容直接达到maxsize continue; } // 当前队列中有任务尚未处理，并且activeCount\u0026lt;maxSize则继续扩容 if (runnableQueue.size() \u0026gt; 0 \u0026amp;\u0026amp; activeCount \u0026lt; maxSize) { for (int i = coreSize; i \u0026lt; maxSize; ++i) { newThread(); } } // 如果任务队列中没有任务，则需要回收，回收至coreSize即可 if (runnableQueue.size() == 0 \u0026amp;\u0026amp; activeCount \u0026gt; coreSize) { for (int i = coreSize; i \u0026lt; activeCount; i++) { removeThread(); } } } } } //ThreadTask 只是InternalTask和Thread的一个组合 private static class ThreadTask { Thread thread; InternalTask internalTask; public ThreadTask(Thread thread, InternalTask internalTask) { this.thread = thread; this.internalTask = internalTask; } } /** * 销毁线程池主要为了是停止BasicThreadPool线程，停止线程池中的活动线程并且将isShutdown开关变量更改为true。 */ @Override public void shutdown() { synchronized (this) { if (isShutdown) { return; } isShutdown = true; threadQueue.forEach(threadTask -\u0026gt; { threadTask.internalTask.stop(); threadTask.thread.interrupt(); }); this.interrupt(); } } @Override public int getInitSize() { if (isShutdown) { throw new IllegalStateException(\u0026quot;The thread pool is destroy\u0026quot;); } return this.initSize; } @Override public int getMaxSize() { if (isShutdown) { throw new IllegalStateException(\u0026quot;The thread pool is destroy\u0026quot;); } return this.maxSize; } @Override public int getCoreSize() { if (isShutdown) { throw new IllegalStateException(\u0026quot;The thread pool is destroy\u0026quot;); } return this.coreSize; } @Override public int getQueueSize() { if (isShutdown) { throw new IllegalStateException(\u0026quot;The thread pool is destroy\u0026quot;); } return runnableQueue.size(); } @Override public int getActiveCount() { synchronized (this) { return this.activeCount; } } @Override public boolean isShutdown() { return this.isShutdown; } private static class DefaultThreadFactory implements ThreadFactory { private static final AtomicInteger GROUP_COUNTER = new AtomicInteger(1); private static final ThreadGroup group = new ThreadGroup(\u0026quot;MyThreadPool-\u0026quot; + GROUP_COUNTER.getAndIncrement()); private static final AtomicInteger COUNTER = new AtomicInteger(0); @Override public Thread createThread(Runnable runnable) { return new Thread(group, runnable, \u0026quot;thread-pool-\u0026quot; + COUNTER.getAndIncrement()); } } }  自动维护线程的代码块是同步代码块，主要是为了阻止在线程维护过程中线程池销毁引起的数据不一致问题。\n任务队列中若存在积压任务，并且当前活动线程少于核心线程数，则新建 coreSize-initSize数量的线程，并且将其加入到活动线程队列中，为了防止马上进行maxSize-coreSize数量的扩充，建议使用continue终止本次循环。\n任务队列中有积压任务，并且当前活动线程少于最大线程数，则新建maxSize-coreSize数量的线程，并且将其加入到活动队列中。\n当前线程池不够繁忙时，则需要回收部分线程，回收到coreSize数量即可，回收时调用removeThread()方法，在该方法中需要考虑的一点是，如果被回收的线程恰巧从Runnable任务取出了某个任务，则会继续保持该线程的运行，直到完成了任务的运行为止，详见InternalTask的run方法。\n3. 线程池的应用 写一个简单的程序分别测试线程池的任务提交、线程池线程数量的动态扩展，以及线程池的销毁功能。\npublic class ThreadPoolTest { public static void main(String[] args) throws InterruptedException { //定义线程池，初始化线程数为2，核心线程数为4，最大线程数位6，任务队列最多允许1000个任务 final ThreadPool threadPool = new BasicThreadPool(2, 6, 4, 1000); // 定义20个任务并且提交给线程池 for (int i = 0; i \u0026lt; 20; i++) { threadPool.execute(()-\u0026gt; { try { TimeUnit.SECONDS.sleep(10); System.out.println(Thread.currentThread().getName() + \u0026quot; is running and done.\u0026quot;); } catch (InterruptedException e) { e.printStackTrace(); } }); } for (; ;) { //不断输出线程池的信息 System.out.println(\u0026quot;getActiveCount: \u0026quot; + threadPool.getActiveCount()); System.out.println(\u0026quot;getQueueSize: \u0026quot; + threadPool.getQueueSize()); System.out.println(\u0026quot;getCoreSize: \u0026quot; + threadPool.getCoreSize()); System.out.println(\u0026quot;getMaxSize: \u0026quot; + threadPool.getMaxSize()); System.out.println(\u0026quot;================================================\u0026quot;); TimeUnit.SECONDS.sleep(5); } } }  上述测试代码中，定义了一个Basic线程池，其中初始化线程数量为2，核心线程数量为4，最大线程数量为6，最大任务队列数量为1000，同时提交了20个任务到线程池中，然后在main线程中不断地输出线程池中的线程数量信息监控变化，运行上述代码，截取的部分输出信息如下：\ngetActiveCount: 2 getQueueSize: 18 getCoreSize: 4 getMaxSize: 6 ================================================ getActiveCount: 2 getQueueSize: 18 getCoreSize: 4 getMaxSize: 6 ================================================ thread-pool-1 is running and done. thread-pool-0 is running and done. getActiveCount: 4 getQueueSize: 14 getCoreSize: 4 getMaxSize: 6 ================================================ getActiveCount: 4 getQueueSize: 14 getCoreSize: 4 getMaxSize: 6 ================================================ thread-pool-2 is running and done. thread-pool-3 is running and done. thread-pool-0 is running and done. thread-pool-1 is running and done. getActiveCount: 6 getQueueSize: 8 getCoreSize: 4 getMaxSize: 6 ================================================ getActiveCount: 6 getQueueSize: 8 getCoreSize: 4 getMaxSize: 6 ================================================ thread-pool-4 is running and done. thread-pool-5 is running and done. thread-pool-3 is running and done. thread-pool-2 is running and done. thread-pool-0 is running and done. thread-pool-1 is running and done. getActiveCount: 6 getQueueSize: 2 getCoreSize: 4 getMaxSize: 6 ================================================ getActiveCount: 6 getQueueSize: 2 getCoreSize: 4 getMaxSize: 6 ================================================ thread-pool-3 is running and done. thread-pool-2 is running and done. thread-pool-5 is running and done. thread-pool-4 is running and done. thread-pool-1 is running and done. thread-pool-0 is running and done. getActiveCount: 6 getQueueSize: 0 getCoreSize: 4 getMaxSize: 6 ================================================ getActiveCount: 6 getQueueSize: 0 getCoreSize: 4 getMaxSize: 6 ================================================ thread-pool-3 is running and done. thread-pool-2 is running and done. getActiveCount: 5 getQueueSize: 0 getCoreSize: 4 getMaxSize: 6 ================================================ getActiveCount: 5 getQueueSize: 0 getCoreSize: 4 getMaxSize: 6 ================================================ getActiveCount: 4 getQueueSize: 0 getCoreSize: 4 getMaxSize: 6 ================================================  通过上述输出信息可以看出，线程池中线程的动态扩展状况以及任务执行情况，在输出的最后会发现active count停留在了core size的位置，这也符合我们的设计，最后为了确定线程池中的活跃线程数量\n================================================ getActiveCount: 4 getQueueSize: 0 getCoreSize: 4 getMaxSize: 6 ================================================ getActiveCount: 4 getQueueSize: 0 getCoreSize: 4 getMaxSize: 6  4. 参考  【1】《Java 高并发编程详解》-汪文君\n ","date":"2020年11月12日","permalink":"https://ahamoment.cn/posts/java/java-multithread-thread-pool/","summary":"1. 为什么要用线程池？ 池化技术相比大家已经屡见不鲜了，线程池、数据库连接池、Http 连接池等等都是对这个思想的应用。池化技术的思想主要是为了减","title":"Java 多线程 - 自定义线程池"},{"contents":"线程的生命周期大体可以分为如下6个主要的阶段：\n NEW RUNNABLE WAITING TIMED_WAITING BLOCKED TERMINATED  从 JDK 的源代码中也能看到关于线程状态的描述：\n// Thread.State public enum State { /** * Thread state for a thread which has not yet started. */ NEW, /** * Thread state for a runnable thread. A thread in the runnable * state is executing in the Java virtual machine but it may * be waiting for other resources from the operating system * such as processor. */ RUNNABLE, /** * Thread state for a thread blocked waiting for a monitor lock. * A thread in the blocked state is waiting for a monitor lock * to enter a synchronized block/method or * reenter a synchronized block/method after calling * {@link Object#wait() Object.wait}. */ BLOCKED, /** * Thread state for a waiting thread. * A thread is in the waiting state due to calling one of the * following methods: * \u0026lt;ul\u0026gt; * \u0026lt;li\u0026gt;{@link Object#wait() Object.wait} with no timeout\u0026lt;/li\u0026gt; * \u0026lt;li\u0026gt;{@link #join() Thread.join} with no timeout\u0026lt;/li\u0026gt; * \u0026lt;li\u0026gt;{@link LockSupport#park() LockSupport.park}\u0026lt;/li\u0026gt; * \u0026lt;/ul\u0026gt; * * \u0026lt;p\u0026gt;A thread in the waiting state is waiting for another thread to * perform a particular action. * * For example, a thread that has called \u0026lt;tt\u0026gt;Object.wait()\u0026lt;/tt\u0026gt; * on an object is waiting for another thread to call * \u0026lt;tt\u0026gt;Object.notify()\u0026lt;/tt\u0026gt; or \u0026lt;tt\u0026gt;Object.notifyAll()\u0026lt;/tt\u0026gt; on * that object. A thread that has called \u0026lt;tt\u0026gt;Thread.join()\u0026lt;/tt\u0026gt; * is waiting for a specified thread to terminate. */ WAITING, /** * Thread state for a waiting thread with a specified waiting time. * A thread is in the timed waiting state due to calling one of * the following methods with a specified positive waiting time: * \u0026lt;ul\u0026gt; * \u0026lt;li\u0026gt;{@link #sleep Thread.sleep}\u0026lt;/li\u0026gt; * \u0026lt;li\u0026gt;{@link Object#wait(long) Object.wait} with timeout\u0026lt;/li\u0026gt; * \u0026lt;li\u0026gt;{@link #join(long) Thread.join} with timeout\u0026lt;/li\u0026gt; * \u0026lt;li\u0026gt;{@link LockSupport#parkNanos LockSupport.parkNanos}\u0026lt;/li\u0026gt; * \u0026lt;li\u0026gt;{@link LockSupport#parkUntil LockSupport.parkUntil}\u0026lt;/li\u0026gt; * \u0026lt;/ul\u0026gt; */ TIMED_WAITING, /** * Thread state for a terminated thread. * The thread has completed execution. */ TERMINATED; }  线程的NEW状态 当我们用关键字new创建一个Thread对象时，此时它并不处于执行状态，因为没有调用start方法启动该线程，那么线程的状态为NEW状态，准确地说，它只是Thread对象的状态，因为在没有start之前，该线程根本不存在，与你用关键字new创建一个普通的Java对象没什么区别。\nNEW状态通过start方法进入RUNNABLE状态。\n线程的RUNNABLE状态 操作系统隐藏 Java 虚拟机（JVM）中的 READY 和 RUNNING 状态，它只能看到 RUNNABLE 状态，所以 Java 系统一般将这两个状态统称为 RUNNABLE（运行中） 状态 。\n线程调用 start() 方法后开始运行，那么此时才是真正地在JVM进程中创建了一个线程，线程一经启动就可以立即得到执行吗？答案是否定的，线程的运行与否和进程一样都要听令于CPU的调度，线程这时候处于 READY（可运行） 状态，也就是说它具备执行的资格，但是并没有真正地执行起来而是在等待CPU的调度。可运行状态的线程获得了 CPU 时间片（timeslice）后就处于 RUNNING（运行） 状态。\n由于存在RUNNING状态，所以不会直接进入BLOCKED状态和TERMINATED状态，即使是在线程的执行逻辑中调用wait、sleep或者其他block的IO操作等，也必须先获得CPU的调度执行权才可以，严格来讲，RUNNABLE的线程只能意外终止或者进入RUNNING状态。\n在该RUNNING状态中，线程的状态可以发生如下的状态转换。\n 直接进入TERMINATED状态，比如调用JDK已经不推荐使用的stop方法或者判断某个逻辑标识。 进入WAITING状态，比如调用了sleep，或者wait方法而加入了waitSet中。 进行某个阻塞的IO操作，比如因网络数据的读写而进入了BLOCKED状态。 获取某个锁资源，从而加入到该锁的阻塞队列中而进入了BLOCKED状态。 由于CPU的调度器轮询使该线程放弃执行，进入READY 状态。 线程主动调用yield方法，放弃CPU执行权，进入READY状态。  线程的BLOCKED状态 线程在BLOCKED状态中可以切换至如下几个状态。\n 直接进入TERMINATED状态，比如调用JDK已经不推荐使用的stop方法或者意外死亡（JVM Crash）。 线程阻塞的操作结束，比如读取了想要的数据字节进入到RUNNABLE状态。 线程完成了指定时间的休眠，进入到了RUNNABLE状态。 线程获取到了某个锁资源，进入RUNNABLE状态。 线程在阻塞过程中被打断，比如其他线程调用了interrupt方法，进入RUNNABLE状态。  线程的 WAITING 状态 线程进入WAITING状态，可能是调用了wait/join/park方法使线程进入等待状态，处在WAITING 状态的线程被其他线程调用 notify/notifyAll 唤醒之后，就会重新进入 RUNNABLE 状态。\n线程的 TIMED_WAITING 状态 TIMED_WAITING 就是超时等待的意思，跟 WAITING 状态不同的是，TIMED_WAITING 会等待指定的超时时间后自动退出。\n线程的TERMINATED状态 TERMINATED是一个线程的最终状态，在该状态中线程将不会切换到其他任何状态，线程进入TERMINATED状态，意味着该线程的整个生命周期都结束了，下列这些情况将会使线程进入TERMINATED状态。\n 线程运行正常结束，结束生命周期。 线程运行出错意外结束。 JVM Crash，导致所有的线程都结束。  参考  【1】《Java 高并发编程详解》-汪文君\n ","date":"2020年11月11日","permalink":"https://ahamoment.cn/posts/java/java-multithread-thread-lifecycle/","summary":"线程的生命周期大体可以分为如下6个主要的阶段： NEW RUNNABLE WAITING TIMED_WAITING BLOCKED TERMINATED 从 JDK 的源代码中也能看到关于线程状态的描述： // Thread.State public enum State { /** * Thread state for a thread which has not yet started. */ NEW,","title":"Java 多线程 - 线程生命周期"},{"contents":" JavaGuide\n 1. 请简要描述线程与进程的关系，区别及优缺点？ 从 JVM 角度说进程和线程之间的关系\n一个进程中可以有多个线程，多个线程共享进程的堆和**方法区 (JDK1.8 之后的元空间)*资源，但是每个线程有自己的*程序计数器、虚拟机栈 和 本地方法栈。\n总结： 线程是进程划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。线程执行开销小，但不利于资源的管理和保护；而进程正相反。\n下面是该知识点的扩展内容！\n下面来思考这样一个问题：为什么程序计数器、虚拟机栈和本地方法栈是线程私有的呢？为什么堆和方法区是线程共享的呢？\n1.1. 程序计数器为什么是私有的? 程序计数器主要有下面两个作用：\n 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。  需要注意的是，如果执行的是 native 方法，那么程序计数器记录的是 undefined 地址，只有执行的是 Java 代码时程序计数器记录的才是下一条指令的地址。\n所以，程序计数器私有主要是为了线程切换后能恢复到正确的执行位置。\n1.2. 虚拟机栈和本地方法栈为什么是私有的?  虚拟机栈： 每个 Java 方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息。从方法调用直至执行完成的过程，就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。 本地方法栈： 和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。  所以，为了保证线程中的局部变量不被别的线程访问到，虚拟机栈和本地方法栈是线程私有的。\n1.3. 一句话简单了解堆和方法区 堆和方法区是所有线程共享的资源，其中堆是进程中最大的一块内存，主要用于存放新创建的对象 (几乎所有对象都在这里分配内存)，方法区主要用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。\n2. 说说并发与并行的区别?  并发： 同一时间段，多个任务都在执行 (单位时间内不一定同时执行)； 并行： 单位时间内，多个任务同时执行。  3. 说说线程的生命周期  参考文章：Java 多线程 - 线程生命周期\n 4. 什么是上下文切换? 多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。\n概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。\n上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。\nLinux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。\n5. 什么是线程死锁？如何避免死锁？  参考文章：https://chenxq.xyz/post/java-multithread-dead-lock/\n 6. 说说 sleep() 方法和 wait() 方法区别和共同点? 共同点：\n wait 和 sleep 方法都可以使线程进入阻塞状态 wait 和 sleep 方法均是可中断方法，被中断后都会收到中断异常。  区别：\n wait是Object的方法，而sleep是Thread特有的方法。   sleep 方法没有释放锁，而 wait 方法释放了锁 。 wait方法的执行必须在同步方法中进行，而sleep则不需要。 sleep方法短暂休眠之后会主动退出阻塞，而wait方法（没有指定wait时间）则需要被其他线程中断后才能退出阻塞。  7. 为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？ 这是另一个非常经典的 java 多线程面试问题，而且在面试中会经常被问到。很简单，但是很多人都会答不上来！\nnew 一个 Thread，线程进入了新建状态;调用 start() 方法，会启动一个线程并使线程进入了就绪状态，当分配到时间片后就可以开始运行了。 start() 会执行线程的相应准备工作，然后自动执行 run() 方法的内容，这是真正的多线程工作。 而直接执行 run() 方法，会把 run 方法当成一个 main 线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。\n总结： 调用 start 方法方可启动线程并使线程进入就绪状态，而 run 方法只是 thread 的一个普通方法调用，还是在主线程里执行。\n","date":"2020年11月11日","permalink":"https://ahamoment.cn/posts/java/java-multithread-interview-questions/","summary":"","title":"Java 多线程面试题总结"},{"contents":"synchronized关键字提供了一种互斥机制，也就是说在同一时刻，只能有一个线程访问同步资源，很多资料、书籍将synchronized（mutex）称为锁，其实这种说法是不严谨的，准确地讲应该是某线程获取了与mutex关联的monitor锁（当然写程序的时候知道它想要表达的语义即可）。\n使用JDK命令javap对Mutex class进行反汇编，输出了大量的JVM指令，在这些指令中，你将发现monitor enter和monitor exit是成对出现的（有些时候会出现一个monitor enter多个monitor exit，但是每一个monitor exit之前必有对应的monitor enter，这是肯定的）.\n Monitorenter 每个对象都与一个monitor相关联，一个monitor的lock的锁只能被一个线程在同一时间获得，在一个线程尝试获得与对象关联monitor的所有权时会发生如下的几件事情。  如果monitor的计数器为0，则意味着该monitor的lock还没有被获得，某个线程获得之后将立即对该计数器加一，从此该线程就是这个monitor的所有者了。 如果一个已经拥有该monitor所有权的线程重入，则会导致monitor计数器再次累加。 如果monitor已经被其他线程所拥有，则其他线程尝试获取该monitor的所有权时，会被陷入阻塞状态直到monitor计数器变为0，才能再次尝试获取对monitor的所有权。   Monitorexit 释放对monitor的所有权，想要释放对某个对象关联的monitor的所有权的前提是，你曾经获得了所有权。释放monitor所有权的过程比较简单，就是将monitor的计数器减一，如果计数器的结果为0，那就意味着该线程不再拥有对该monitor的所有权，通俗地讲就是解锁。与此同时被该monitor block的线程将再次尝试获得对该monitor的所有权。  使用synchronized方法需要注意几个问题：\n 与monitor关联的对象不能为空 synchronized作用域太大 由于synchronized关键字存在排他性，也就是说所有的线程必须串行地经过synchronized保护的共享区域，如果synchronized作用域越大，则代表着其效率越低，甚至还会丧失并发的优势。 不同的monitor企图锁相同的方法 多个锁的交叉导致死锁  ","date":"2020年10月25日","permalink":"https://ahamoment.cn/posts/java/java-multithread-synchronized2/","summary":"synchronized关键字提供了一种互斥机制，也就是说在同一时刻，只能有一个线程访问同步资源，很多资料、书籍将synchronized（","title":"Java 多线程 - 深入理解synchronized关键字"},{"contents":"线程interrupt，是一个非常重要的API，也是经常使用的方法，在本文中我们将Thread深入源码对其进行详细的剖析。\n首先来看一下与线程中断相关的几个API：\npublic void interrupt() public static boolean interrupted() public boolean isInterrupted()  interrupt 如下方法的调用会使得当前线程进入阻塞状态，而调用当前线程的interrupt方法，就可以打断阻塞。\n Object的wait方法。 Object的wait（long）方法。 Object的wait（long，int）方法。 Thread的sleep（long）方法。 Thread的sleep（long，int）方法。 Thread的join方法。 Thread的join（long）方法。 Thread的join（long，int）方法。 InterruptibleChannel的io操作。 Selector的wakeup方法。  上述若干方法都会使得当前线程进入阻塞状态，若另外的一个线程调用被阻塞线程的interrupt方法，则会打断这种阻塞，因此这种方法有时会被称为可中断方法，记住，打断一个线程并不等于该线程的生命周期结束，仅仅是打断了当前线程的阻塞状态。\n一旦线程在阻塞的情况下被打断，都会抛出一个称为InterruptedException的异常，这个异常就像一个signal（信号）一样通知当前线程被打断了，下面我们来看一个例子：\nimport java.util.concurrent.TimeUnit; public class ThreadInterrupt { public static void main(String[] args) throws InterruptedException { Thread thread = new Thread(() -\u0026gt; { try { TimeUnit.MINUTES.sleep(1); } catch( InterruptedException e) { System.out.println(\u0026quot;Oh, i am be interrupted.\u0026quot;); } }); thread.start(); //short block and make sure thread is started. TimeUnit.MILLISECONDS.sleep(2); thread.interrupt(); } }  上面的代码创建了一个线程，并且企图休眠1分钟的时长，不过很可惜，大约在2毫秒之后就被主线程调用interrupt方法打断，程序的执行结果就是“Oh，i am be interrupted.”\ninterrupt这个方法到底做了什么样的事情呢？在一个线程内部存在着名为interrupt flag的标识，如果一个线程被interrupt，那么它的flag将被设置，但是如果当前线程正在执行可中断方法被阻塞时，调用interrupt方法将其中断，反而会导致flag被清除，关于这点我们在后面还会做详细的介绍。另外有一点需要注意的是，如果一个线程已经是死亡状态，那么尝试对其的interrupt会直接被忽略。\nisInterrupted isInterrupted是Thread的一个成员方法，它主要判断当前线程是否被中断，该方法仅仅是对interrupt标识的一个判断，并不会影响标识发生任何改变，这个与我们即将学习到的interrupted是存在差别的，下面我们看一个简单的程序：\npublic class ThreadisInterrupted { public static void main(String[] args) throws InterruptedException { Thread thread = new Thread() { @Override public void run() { while (true) { //do nothing, just empty loop. } } }; thread.start(); TimeUnit.MILLISECONDS.sleep(2); System.out.printf(\u0026quot;Thread is interrupted ? ％s\\n\u0026quot;, thread.isInterrupted()); thread.interrupt(); System.out.printf(\u0026quot;Thread is interrupted ? ％s\\n\u0026quot;, thread.isInterrupted()); } }  上面的代码中定义了一个线程，并且在线程的执行单元中（run方法）写了一个空的死循环，为什么不写sleep呢？因为sleep是可中断方法，会捕获到中断信号，从而干扰我们程序的结果。下面是程序运行的结果，记得手动结束上面的程序运行，或者你也可以将上面定义的线程指定为守护线程，这样就会随着主线程的结束导致JVM中没有非守护线程而自动退出。\nThread is interrupted ? false Thread is interrupted ? true  可中断方法捕获到了中断信号（signal）之后，也就是捕获了InterruptedException异常之后会擦除掉interrupt的标识，对上面的程序稍作修改，你会发现程序的结果又会出现很大的不同，示例代码如下：\npublic class ThreadisInterrupted { public static void main(String[] args) throws InterruptedException { Thread thread = new Thread() { @Override public void run() { while (true) { try { TimeUnit.MINUTES.sleep(1); } catch (InterruptedException e) { //ignore the exception //here the interrupt flag will be clear. //由于可中断方法的异常被捕获后，会擦除掉interrup标记，所以调用 //isInterrupted返回false System.out.printf(\u0026quot;I am be interrupted ? ％s\u0026quot;, isInterrupted()); } } } }; thread.start(); TimeUnit.MILLISECONDS.sleep(2); System.out.printf(\u0026quot;Thread is interrupted ? ％s\\n\u0026quot;, thread.isInterrupted()); thread.interrupt(); System.out.printf(\u0026quot;Thread is interrupted ? ％s\\n\u0026quot;, thread.isInterrupted()); } }  由于在run方法中使用了sleep这个可中断方法，它会捕获到中断信号，并且会擦除interrupt标识，因此程序的执行结果都会是false，程序输出如下：\nThread is interrupted ? false I am be interrupted ? false Thread is interrupted ? false  其实这也不难理解，可中断方法捕获到了中断信号之后，为了不影响线程中其他方法的执行，将线程的interrupt标识复位是一种很合理的设计。\ninterrupted interrupted是一个静态方法，虽然其也用于判断当前线程是否被中断，但是它和成员方法isInterrupted还是有很大的区别的，调用该方法会直接擦除掉线程的interrupt标识，需要注意的是，如果当前线程被打断了，那么第一次调用interrupted方法会返回true，并且立即擦除了interrupt标识；第二次包括以后的调用永远都会返回false，除非在此期间线程又一次地被打断，下面设计了一个简单的例子，来验证我们的说法：\npublic class ThreadisInterrupted { public static void main(String[] args) throws InterruptedException { Thread thread = new Thread() { @Override public void run() { while (true) { System.out.println(Thread.interrupted()); } } }; thread.setDaemon(true); thread.start(); //shortly block make sure the thread is started. TimeUnit.MILLISECONDS.sleep(2); thread.interrupt(); } }  同样由于不想要受到可中断方法如sleep的影响，在Thread的run方法中没有进行任何短暂的休眠，所以运行上面的程序会出现非常多的输出，但是我们通过对输出的检查会发现如下所示的内容，其足以作为对该方法的解释。\n…… false false true false false ……  在很多的false包围中发现了一个true，也就是interrupted方法判断到了其被中断，立即擦除了中断标识，并且只有这一次返回true，后面的都将会是false。\ninterrupt 注意事项 打开Thread的源码，不难发现，isInterrupted方法和interrupted方法都调用了同一个本地方法：\nprivate native boolean isInterrupted(boolean ClearInterrupted);  其中参数ClearInterrupted主要用来控制是否擦除线程interrupt的标识。isInterrupted方法的源码中该参数为false，表示不想擦除：\npublic boolean isInterrupted() { return isInterrupted(false); }  而interrupted静态方法中该参数则为true，表示想要擦除：\npublic static boolean interrupted() { return currentThread().isInterrupted(true); }  在比较详细地学习了interrupt方法之后，大家思考一个问题，如果一个线程在没有执行可中断方法之前就被打断，那么其接下来将执行可中断方法，比如sleep会发生什么样的情况呢？下面我们通过一个简单的实验来回答这个疑问：\npublic class ThreadisInterrupted { public static void main(String[] args) { //① 判断当前线程是否被中断 // flag=false,清除线程中断标志 System.out.println(\u0026quot;Main thread is interrupted? \u0026quot; + Thread.interrupted()); //②中断当前线程 // flag=true Thread.currentThread().interrupt(); //③判断当前线程是否已经被中断 // 这里不能再调用Thread.interrupted()，因为会将flag清除，达不到实验效果 System.out.println(\u0026quot;Main thread is interrupted? \u0026quot; + Thread.currentThread().isInterrupted()); try { //④ 当前线程执行可中断方法 TimeUnit.MINUTES.sleep(1); } catch (InterruptedException e) { //⑤捕获中断信号 System.out.println(\u0026quot;I will be interrupted still.\u0026quot;); } } }  通过运行上面的程序，你会发现，如果一个线程设置了interrupt标识，那么接下来的可中断方法会立即中断，因此注释⑤的信号捕获部分代码会被执行.\n参考 【1】《Java 高并发编程详解》-汪文君\n","date":"2020年10月24日","permalink":"https://ahamoment.cn/posts/java/java-multithread-interrupt/","summary":"\u003cp\u003e线程interrupt，是一个非常重要的API，也是经常使用的方法，在本文中我们将Thread深入源码对其进行详细的剖析。\u003c/p\u003e","title":"Java 多线程 - 线程中断 Interrupt"},{"contents":" http://www.oracle.com/technetwork/java/javase/memorymanagement-whitepaper-150215.pdf\n JVM在执行Java程序的时候会把对应的物理内存划分成不同的内存区域，每一个区域都存放着不同的数据，也有不同的创建与销毁时机，有些分区会在JVM启动的时候就创建，有些则是在运行时才创建，比如虚拟机栈，根据虚拟机规范，JVM的内存结构如图所示。\n程序计数器 无论任何语言，其实最终都是需要由操作系统通过控制总线向CPU发送机器指令，Java也不例外，程序计数器在JVM中所起的作用就是用于存放当前线程接下来将要执行的字节码指令、分支、循环、跳转、异常处理等信息。在任何时候，一个处理器只执行其中一个线程中的指令，为了能够在CPU时间片轮转切换上下文之后顺利回到正确的执行位置，每条线程都需要具有一个独立的程序计数器，各个线程之间互相不影响，因此JVM将此块内存区域设计成了线程私有的。\nJava 虚拟机栈 与程序计数器内存相类似，Java虚拟机栈也是线程私有的，它的生命周期与线程相同，是在JVM运行时所创建的，在线程中，方法在执行的时候都会创建一个名为栈帧（stack frame）的数据结构，主要用于存放局部变量表、操作栈、动态链接、方法出口等信息，如图所示，方法的调用对应着栈帧在虚拟机栈中的压栈和弹栈过程。\n每一个线程在创建的时候，JVM都会为其创建对应的虚拟机栈，虚拟机栈的大小可以通过-xss来配置，方法的调用是栈帧被压入和弹出的过程，通过上图可以看出，同等的虚拟机栈如果局部变量表等占用内存越小则可被压入的栈帧就会越多，反之则可被压入的栈帧就会越少，一般将栈帧内存的大小称为宽度，而栈帧的数量则称为虚拟机栈的深度。\n本地方法栈 Java中提供了调用本地方法的接口（Java Native Interface），也就是C/C++程序，在线程的执行过程中，经常会碰到调用JNI方法的情况，比如网络通信、文件操作的底层，甚至是String的intern等都是JNI方法，JVM为本地方法所划分的内存区域便是本地方法栈，这块内存区域其自由度非常高，完全靠不同的JVM厂商来实现，Java虚拟机规范并未给出强制的规定，同样它也是线程私有的内存区域。\n堆内存 堆内存是JVM中最大的一块内存区域，被所有的线程所共享，Java在运行期间创建的所有对象几乎都存放在该内存区域，该内存区域也是垃圾回收器重点照顾的区域，因此有些时候堆内存被称为“GC堆”。\n堆内存一般会被细分为新生代和老年代，更细致的划分为Eden区、From Survivor区和To Survivor区，如图所示。\n堆内存一般会被细分为新生代和老年代，更细致的划分为Eden区、From Survivor区和To Survivor区。\n方法区 方法区也是被多个线程所共享的内存区域，他主要用于存储已经被虚拟机加载的类信息、常量、静态变量、即时编译器（JIT）编译后的代码等数据，虽然在Java虚拟机规范中，将堆内存划分为堆内存的一个逻辑分区，但是它还是经常被称为“非堆”，有时候也被称为“持久代”，主要是站在垃圾回收器的角度进行划分，但是这种叫法比较欠妥，在HotSpot JVM中，方法区还会被细划分为持久代和代码缓存区，代码缓存区主要用于存储编译后的本地代码（和硬件相关）以及JIT（Just In Time）编译器生成的代码，当然不同的JVM会有不同的实现。\n","date":"2020年10月15日","permalink":"https://ahamoment.cn/posts/java/java-jvm-jmm/","summary":"http://www.oracle.com/technetwork/java/javase/memorymanagement-whitepaper-150215.pdf JVM在执行Java程序的时候会把对应的物理内存划分成不同的内存区域，每一个区域都存放着不同的数据，也有不同的创建与销毁时机，有些分区会在","title":"JVM 内存结构"},{"contents":"基本概念 在登录QQ的时候，QQ服务器是如何核对你的身份？面对庞大的用户群，如何快速找到用户信息？\n我们已经知道的几种查找方法包括：顺序查找，二分查找（静态查找），二叉搜索树（动态查找）。在这个场景下，如果使用二分查找的话就会面对插入和删除一个新号码要移动大量数据的问题。\n这里我们要用到散列查找的方法，散列（Hashing）的基本思想是：\n 以关键字 key 为自变量，通过一个确定的**函数 h （散列函数）**计算出对应的函数值h(key)，作为数据对象的存储地址 可能不同的关键字会映射到同一个散列地址上，称为**“冲突”**，发生冲突后需要某种冲突解决策略来解决冲突。  散列查找的时间复杂度为 O(1)，即查找时间与问题规模无关。\n一般情况下，设散列表空间大小为m，填入表中的元素个数是n，则称α=n/m为散列表的”装填因子“(Loading Factor)。实用时，通常将散列表大小设计为 0.5-0.8 为宜。\n散列映射法的关键问题有两个：\n 如何设计散列函数，使得发生冲突的概率尽可能小； 当冲突或溢出不可避免的时候，如何处理使得表中没有空单元被浪费，同时插入、删除、查找等操作都正确完成。  散列函数的构造方法 一个好的散列函数一般考虑下列两个因素：\n 计算简单，以便提高转换速度 关键字对应的地址空间分不均匀，以尽量减少冲突。即对于关键字集合中的任何一个关键字，经散列函数映射到地址集合中任何一个地址的概率是基本相等的。实际应用过程中，严格的均匀分布也是不可能的，只是不要过于聚集就行了。  关键字又分为数字关键字和字符串关键字两种类型，分别有不同的散列函数的构造方法：\n数字关键字的散列函数构造   直接定址法\n取关键字的某个线性函数值为散列地址，即\nh(key) = a*key + b (a,b 为常数)    除留余数法\n散列函数为\nh(key)=key mod p  假设散列表长为 TableSize （TableSize 的选取通常由关键字集合的大小 n 和允许最大的装填因子 α 决定，一般 TableSize=n/α），选择一个正整数 p \u0026lt;= TableSize。一般选取 p 为小于或者等于散列表表长 TableSize 的某个最大素数比较好。用素数求得得余数作为散列地址，比较均匀地分布在整个地址空间上的可能性比较大，具体证明可以参考为什么一般hashtable的桶数会取一个素数。例如，TableSize=8，p=7;TableSize=16, p=13。\n  数字分析法\n分析数字关键字在各位上的变化情况，取比较随机的位作为散列地址。散列函数可以取为：\nh(key)=atoi(key+7)    折叠法\n把关键字分割成位数相同的几个部分，然后叠加取部分值。例如：\n折叠法是希望每一位对最后的结果都能产生影响。\n  平方取中法\n如：56793542\n平方取中法和折叠法的目的都是一样的，为了让关键字的每一位都对最后的结果产生影响。\n  字符关键字的散列函数构造   一个简单的散列函数：ASCII 码加和法。对字符型关键字key定义散列函数如下：\n$\\sum key[i]$ mod TableSize\n这种方法会有严重的冲突\n  简单的改进 - 前3个字符移位法\n$h(key)=(key[0]*27^2+key[1]*27+key[2])$ mod TableSize\n这个方法会造成空间的浪费\n  好的散列函数：移位法\n涉及关键字所有n个字符，并且分布的很好：\n$h(key)=(\\displaystyle \\sum^{n-1}_{i=0}{key[n-i-1]*32^i})$ mod TableSize\n该函数用于处理长度位 n 的字符串关键字，每个字符占5位(即 $2^5=32$)，具体实现时并不需要做乘法运算，而是通过一次左移 5 位来完成。\npublic int hashString(String key, int tableSize) { int h = 0; //散列函数值，初始化为0 char[] keyArrays = key.toCharArray(); for (int i = 0; i \u0026lt;= keyArrays.length; i++) { if (keyArrays[i] != '\\0') { h = (h \u0026lt;\u0026lt; 5) + keyArrays[i]; } } return h % tableSize; }    冲突处理方法 常用处理冲突的思路：\n 换个位置：开放地址法 同一位置的冲突对象组织在一起：链地址法  开放定址法 一旦产生了冲突（该地址已有其他元素），就按某种规则去寻找另一空地址。假设发生了第 i 次冲突，试探的下一个地址将增加 $d_i$ , 基本公式是：\n$d_i(key)=(h(key)+d_i)$ mod tableSize （1 \u0026lt;= i \u0026lt; tableSize）\n$d_i$决定了不同的解决冲突方案，包括：线性探测、平方探测、双散列。\n  线性探测\n$d_i=i$\n以增量序列1,2,\u0026hellip;\u0026hellip;,(tableSize-1) 循环试探下一个存储地址。做插入操作的时候，要找到一个空位置，或者直到散列表满为止；做查找操作时，探测一个比较依次一次关键字，直到找到特定的数据对象，或者探测到一个空位置表示查找失败为止。\n线性探测的缺点就是容易产生聚集的现象，因此引入了平方探测法\n  平方探测法\u0026ndash;二次探测\n平方探测的公式：$d_i=\\pm i^2$，每次以增量序列$1^2,-1^2,2^2,-2^2\u0026hellip;\u0026hellip;$循环试探下一个存储地址。\n平方探测法有可能出现散列表中有空间，但是无法探测到的情况。例如:\n散列表的长度为5，插入5,6,7,11这四个元素，散列函数设计为:\nh(key)=key mod 5  当插入11时，散列函数找到的位置为2，和6这个元素所在的位置产生冲突，使用平方探测法，探测序列为:\n1+1=2 1-1=0 (1+2*2)%5=0 (1-2*2)%5=2 (1+3*3)%5=0 (1-3*3)%5=2 ......  可以发现，探测序列一直在0和2这两个位置之间变动，一直找不到空的位置，但是散列表实际上还有空间。有定理显示：如果散列表长度tableSize是某个$4k+3$(k是正整数) 形式的素数时，平方探测法就可以探查到整个散列表空间。\n虽然平方探测法排除了一次聚集，但是散列到同一地址的那些数据对象将探测相同的备选单元，这称为“二次聚集”。\n  双散列探测法\n双散列探测法: $d_i=i*h_2(key)$，$h_2(key)$是另一个散列函数，探测序列为：\n$h_2(key), 2h_2(key), 3h_2(key), \u0026hellip;\u0026hellip;$\n探测序列还应该保证所有的散列存储单元都能被探测到。选择以下形式有良好的效果：\n$h_2(key)=p$ - (key mod p)\n  再散列\n当散列表元素太多（即装填因子α太大）时，查找效率会下降，实际使用的时候，装填因子一般取 0.5\u0026lt;=α\u0026lt;=0.85。\n当装填因子过大时，解决的方法时加倍扩大散列表，这个过程叫做“再散列”。\n  在开放地址散列表中，删除操作要很小心，通常只能 “懒惰删除”，即需要增加一个删除标记(Deleted)，并不是真正删除它。这是因为插入的时候，为了解决冲突问题，这个位置已经被占用了，如果删除掉它，查找的时候就会出现“断链”的现象。\n分离链接法 分离链接法时解决冲突的另一种方法，其做法是将所有关键字为同义词的数据对象通过节点链接存储到同一单向链表中。。\n如上图所示，分裂链接法实际上是用一个数组来组织散列表的数据结构，这个数组称为哈希桶，数组中的每个元素都指向一个链表，当元素冲突的时候，就在链表的头节点上插入冲突的元素。新元素插入到表头，这不仅仅为了方便，而且还因为新近插入的元素最有可能被最先访问，这样可以加快在单向链表中的顺序查找速度。\n散列表的性能分析 散列表的性能使用平均查找长度（ASL）来度量。\n  线性探测法的查找性能满足下列公式\n  平方探测法和双散列探测法的查找性能满足下列公式\n  分离链接法的查找性能满足下列公式\n   参考：浙江大学陈越老师的数据结构课程\n ","date":"2020年10月15日","permalink":"https://ahamoment.cn/posts/algorithm/algorithm-hashtable/","summary":"基本概念 在登录QQ的时候，QQ服务器是如何核对你的身份？面对庞大的用户群，如何快速找到用户信息？ 我们已经知道的几种查找方法包括：顺序查找，二","title":"散列表那些事"},{"contents":"NSCD(Name Service Cache Daemon)是服务缓存守护进程。\nNSCD 安装   RHEL/CentOS\nyum -y install nscd    Debian/Ubuntu\napt-get install nscd    RPM\nhttps://centos.pkgs.org/7/centos-x86_64/nscd-2.17-307.el7.1.x86_64.rpm.html    CentOS7 之后 NSCD 使用 systemd 进行管理。\nNSCD 命令选项 $ nscd --help 用法： nscd [选项...] Name Service Cache Daemon. -d, --debug Do not fork and display messages on the current tty -f, --config-file=名称 从NAME中读取配置数据 -g, --statistics Print current configuration statistics -i, --invalidate=TABLE Invalidate the specified cache -K, --shutdown 关闭服务器 -t, --nthreads=NUMBER 启动 NUMBER 个线程 -?, --help 给出该系统求助列表 --usage 给出简要的用法信息 -V, --version 打印程序版本号 长选项的强制或可选参数对对应的短选项也是强制或可选的。  NSCD 配置文件 NSCD配置文件为/etc/nscd.conf，NSCD程序在启动的时候会读取/etc/nscd.conf文件，每一行指定一个属性和对应的值，或者指定一个服务和对应的值，#表示注释。有效的服务设定是：passwd, group, hosts, services, or netgroup五个。\nNSCD 的缓存文件路径默认为 /var/db/nscd/。\nNSCD 的配置文件相关参数\n#设置日志文件 logfile debug-file-name #设置debug记录的级别，默认是0 debug-level value #程序启动时，等待进去请求的处理线程数，至少5个 threads number #最大线程数，默认32 max-threads number #nscd程序以哪个用户运行,如果设置了该选项，nscd将作为该用户运行，而不是作为root。如果每个用户都使用一个单独的缓存(-S参数)，将忽略该选项。 server-user user #哪个用户可以请求统计用户 stat-user user #在一个缓存项被删除之前允许使用的次数，默认值是5，代表SUCCESS的缓存在内存中会Reload5次 reload-count unlimited | number #是否启用偏执模式，启用会导致nscd周期性重启，默认是no paranoia \u0026lt;yes|no\u0026gt; #如果启用偏执模式，设置的定期重启nscd的时间间隔，默认是3600秒 restart-interval time #开启或者关闭服务缓存，默认是no enable-cache service \u0026lt;yes|no\u0026gt; #为成功请求的元素设置缓存TTL，单位是秒，值越大缓存命中率越高，降低平均响应时间，但会增加缓存的一致性问题 positive-time-to-live service value #为失败查询元素设置缓存TTL，单位是秒，应保持小值，减小缓存一致性问题 negative-time-to-live service value #内部的散列表大小，value应该保持一个素数以达到优化效果。默认值是211 suggested-size service value #启用或者禁用检查文件是否属于指定的服务，这些文件是/etc/passwd、/etc/group、/etc/hosts、/etc/services、/etc/netgroup等 check-files service \u0026lt;yes|no\u0026gt; #设置缓存在服务器重启后，仍旧能提供缓存服务，在使用偏执模式时有用，默认是no persistent service \u0026lt;yes|no\u0026gt; #为客户端共享nscd数据库在内存中做的映射，使客户端可以直接搜索，而不用每次都查询守护进行，默认是no shared service \u0026lt;yes|no\u0026gt; #该数据库的最大大小，单位是bytes，默认是33554432 max-db-size service bytes #此选项仅使用于passwd和group服务 auto-propagate service \u0026lt;yes|no\u0026gt;  NSCD 使用示例：对DNS进行缓存 DNS缓存在服务器上的作用 在需要通过域名与外界进行数据交互的时候,dns缓存就派上用场了,它可以减少域名解析的时间,提高效率。例如以下情况\n 使用爬虫采集网络上的页面数据, 使用auth2.0协议从其他平台(如微博或QQ)获取用户数据, 使用第三方支付接口, 使用短信通道下发短信等.  开启NSCD DNS 缓存服务的优缺点  优点：  本地缓存DNS解析信息，提供解析速度。 DNS服务挂了也没有问题，在缓存服务时间范围内，解析依旧正常。   缺点：  DNS解析信息会滞后，如域名解析更改需要手动刷新缓存，NSCD不适合做实时的切换的应用 多条RR的情况下失去轮询功能，导致缓存周期内单机的负载均衡失效 域名变更生效可能持续一个TTL+15s，对于一部分讲究变更快速生效的域名而言有一定的变更生效延误 对于一部分异常导致解析错误的域名，有可能被NSCD缓存导致一段时间内解析都异常    配置DNS缓存 通过编辑/etc/nscd.conf文件，在其中增加如下一行可以开启本地DNS Cache\nenable-cache hosts yes #这个服务除了dns缓存之外还可以缓存passwd,group,servers  完整配置如下：\n$ cat /etc/nscd.conf logfile /var/log/nscd.log threads 5 max-threads 32 server-user nscd debug-level 0 paranoia no reload-count 5 enable-cache hosts yes enable-cache passwd no enable-cache group no positive-time-to-live hosts 60 negative-time-to-live hosts 20 suggested-size hosts 211 check-files hosts yes persistent hosts yes shared hosts yes max-db-size hosts 33554432  关于主动刷新 reload-count 5 默认值是5，代表SUCCESS的缓存在内存中会Reload 5次  reload的time是DNS应答TTL+CACHE_PRUNE_INTERVAL，reload过程中NSCD会主动发起DNS请求（非客户端发起），如果期间发生解析结果变更会将结果主动更新至NSCD缓存。这里的CACHE_PRUNE_INTERVAL来自于相关的宏定义：\n#define CACHE_PRUNE_INTERVAL 15  关于非success域名的缓存 查看代码发现对于非success域名的缓存，NSCD会读取配置中的negative-time-to-live hosts，将缓存一个negative-time-to-live hosts+CACHE_PRUNE_INTERVAL的时间\ndataset-\u0026gt;head.ttl = ttl == INT32_MAX ? db-\u0026gt;negtimeout : ttl; timeout = dataset-\u0026gt;head.timeout = t + dataset-\u0026gt;head.ttl;  关于缓存的RR轮询 NSCD是直接缓存了GETHOSTBYNAME/GETHOSTBYADD的应答结果，如果存在多条RR的情况下，将只会读取应答结果中的第一条结果作为函数的返回值。多条RR在NSCD的缓存中并没有RR轮询的效果，直到下一次reload更新缓存结果。这里可能导致域名原本的负载均衡机制失效。\n关于CNAME+A的结果 GLIBC的GETHOSTBYNAME/GETHOSTBYADD返回的TTL中直接读取的是A类型的TTL，代码中并没有针对CNAME的TTL做特殊处理，因此在有CNAME+A的级联应答结果中，缓存的timeout将只会读取对应的A记录的TTL。 当DNS应答结果只有CNAME时，DNS请求将被判定为失败，这时CNAME的TTL将不起作用，缓存的时间将遵循非success域名的timeout计算。\nreturn ((qtype == T_A || qtype == T_AAAA) \u0026amp;\u0026amp; ap != host_data-\u0026gt;aliases ? NSS_STATUS_NOTFOUND : NSS_STATUS_TRYAGAIN);  启动 NSCD 进程 默认该服务在Redhat或Centos下是关闭的，可以通过以下指令开启\n$ systemctl start nscd  查看进程，如下所示\n$ ps aux | grep nscd nscd 1284 0.1 0.3 708056 1580 ? Ssl 23:37 0:00 /usr/sbin/nscd  说明已经正常运行了。\nNSCD服务查看和清除 NSCD缓存DB文件在/var/db/nscd下。可以通过nscd -g查看统计的信息，这里列出部分：\n$ nscd -g nscd 配置： 0 服务器调试级别 4s server runtime 5 current number of threads 32 maximum number of threads 0 number of times clients had to wait no paranoia mode enabled 3600 restart internal 5 reload count ... 省略输出信息若干 ... hosts cache: yes cache is enabled yes cache is persistent yes cache is shared 211 suggested size 216064 total data pool size 0 used data pool size 60 seconds time to live for positive entries 20 seconds time to live for negative entries 0 cache hits on positive entries 0 cache hits on negative entries 0 cache misses on positive entries 0 cache misses on negative entries 0% cache hit rate 0 current number of cached values 0 maximum number of cached values 0 maximum chain length searched 0 number of delays on rdlock 0 number of delays on wrlock 0 memory allocations failed yes check /etc/hosts for changes ... 省略输出信息若干 ...  清除指定类型缓存 $ nscd -i passwd $ nscd -i group $ nscd -i hosts  关闭服务 $ nscd -K  参考文章 【1】Linux 下开启缓存服务 NSCD\n【2】阿里DNS: NSCD-DNS缓存详解\n【3】Linux man page\n","date":"2020年09月27日","permalink":"https://ahamoment.cn/posts/linux/linux-nscd/","summary":"NSCD(Name Service Cache Daemon)是服务缓存守护进程。 NSCD 安装 RHEL/CentOS yum -y install nscd Debian/Ubuntu apt-get install nscd RPM https://centos.pkgs.org/7/centos-x86_64/nscd-2.17-307.el7.1.x86_64.rpm.html CentOS7 之后 NSCD 使用 systemd 进行管理。 NSCD 命令选项 $ nscd --help 用法： nscd [选项...] Name Service Cache Daemon. -d,","title":"认识 Linux NSCD 服务缓存"},{"contents":"什么是堆 了解什么是堆之前，我们知道队列的概念，队列的特点是先进先出，但是有一种特殊的队列，取出元素的顺序是按照元素的优先权（关键字）大小，而不是元素进入队列的先后顺序，这就是优先队列(Priority Queue)。\n若采用数组或者链表实现优先队列，总会有插入、删除或者查找中的一项操作的复杂度是$O(N)$ 的。\n若采用二叉搜索树实现，那么插入和删除都跟树的高度有关，也就是$O(log_2N)$ 的复杂度，但是删除的时候，由于每次都要删除最大的或者最小的，这样操作几次后，会造成搜索树失去平衡，所以不能简单的使用二叉搜索树。\n如果采用二叉树结构，我们更关注的应该是删除的操作，那么我们把最大的值放到根结点，左右两边也是最大值作为左右子树的根结点，每次删除只需要删除根结点。同时，为了保证树的平衡性，可以考虑使用完全二叉树来实现优先队列。\n优先队列使用完全二叉树表示如上图所示，数组的第 0 个元素空着，后面的按照层序遍历的顺序存放到数组中。使用完全二叉实现的优先队列，也可以称之为堆，堆的特性如下：\n 结构性：用数组表示的完全二叉树。 有序性：任一结点的关键字是其子树所有结点的最大值（或最小值）  \u0026ldquo;最大堆\u0026rdquo;，也称 \u0026ldquo;大顶堆\u0026rdquo;：堆顶元素是整个树的最大值 \u0026ldquo;最小堆\u0026rdquo;，也称\u0026quot;小顶堆\u0026quot;：堆顶元素是整个树的最小值    如下图所示的几个二叉树，不是堆。\n第一和第二棵二叉树虽然满足有序性，但是不是完全二叉树。第三和第四棵二叉树是完全二叉树，但是不满足有序性的特点。\n 注意：堆从根结点到任意结点路径上的结点顺序都是有序的！\n 最大堆的创建 堆的数据结构包括存储完全二叉树的数组 data，堆中当前元素个数 size，堆的最大容量 capacity。\n数组的元素从1开始，0的位置定义为哨兵，方便以后更快操作。\npublic abstract class Heap { // 堆的类型定义 protected int[] data; //存储元素的数组 protected int size;//堆中当前元素个数 protected int capacity; //堆的最大容量 public Heap() { this.size = 0; this.capacity = 0; } public Heap(int[] data, int capacity) { this.data = data; this.size = 0; this.capacity = capacity; this.data[0] = Integer.MAX_VALUE; } public Heap(int maxSize) { this.data = new int[maxSize + 1];//最大元素从1开始 this.size = 0; this.capacity = maxSize; this.data[0] = Integer.MAX_VALUE;// 定义哨兵，为大于最大堆中所有可能元素的值 } public boolean isFull() { return this.size == this.capacity; } public boolean isEmpty() { return this.size == 0; } public abstract boolean insert(int element); }  最大堆的插入 插入元素时，插入到数组的最后一个位置，这里插入的结点值为20，检查插入后仍然符合堆的两个特性，插入完成。\n当插入的值为35的时候，当前堆的有序性被破坏了，将35和31的位置调换后就可以了。\n当插入的值为58的时候，58 \u0026gt; 31，跟31对调位置，58 \u0026gt; 44 继续跟根结点调换位置。调整后保证了有序性，同时，从58 -\u0026gt; 44 -\u0026gt; 31这条线也是按照从大到小的顺序。\npublic boolean insert(int element) { // 将元素X插入最大堆H，其中H-\u0026gt;Data[0]已经定义为哨兵 int i; if (isFull()) { System.out.println(\u0026quot;最大堆已满\u0026quot;); return false; } i = ++this.size; // i指向插入后堆中的最后一个元素的位置 for (; this.data[i / 2] \u0026lt; element; i /= 2) { data[i] = data[i / 2]; // 向下过滤结点，对调父结点的位置 } data[i] = element; // 将X插入 return true; }  由于我们将数组的第 0 个元素设置为哨兵，哨兵的值为一个非常大的整数值。如果没有哨兵结点，我们在循环中还需要判断 i \u0026gt; 1 这个条件，有了哨兵之后，循环在 i = 0 的时候就会停下来，可以少写一个条件，提高程序效率。\n最大堆的删除 最大堆的删除过程就是取出根结点（最大值）元素，同时删除堆的一个结点。\n删除下图的这个堆的最大值：\n 把 31 移至根 找出 31 的较大的孩子  时间复杂度为： $T(N)=O(logN)$\npublic int deleteMax() { // 从最大堆中取出键值为最大的元素，并删除一个结点 int parent, child; int maxItem, temp;//maxItem-堆顶元素，temp-临时变量 if (isEmpty()) { System.out.println(\u0026quot;最大堆已经为空\u0026quot;); return -1; } maxItem = this.data[1];//取出根结点最大值 // 用最大堆中的最后一个元素从根结点开始向上过滤下层结点 temp = this.data[this.size--]; for (parent = 1; parent * 2 \u0026lt; this.size; parent = child) { child = parent * 2; // 左儿子的位置 if (child != this.size \u0026amp;\u0026amp; this.data[child] \u0026lt; this.data[child + 1]) { child++; //child 指向左右结点的较大者 } if (temp \u0026gt; this.data[child]) {//找到位置了 break; } else {//将子结点与父节点对换 this.data[parent] = this.data[child]; } } this.data[parent] = temp; return maxItem; }  最大堆的建立 建立最大堆是将已经存在的N个元素按最大堆的要求存放在一个一维数组中。\n建堆的过程可以从树的从最后一个结点的父节点开始，到根结点1，将最后一个结点的父节点所在的小堆调整为最大堆，然后向左寻找有儿子的结点，每次调整一个最大堆，直到根结点。\npublic void buildHeap() { //* 调整Data[]中的元素，使满足最大堆的有序性 *//* //* 这里假设所有Size个元素已经存在Data[]中 *//* int i; //* 从最后一个结点的父节点开始，到根结点1 *//* for (i = this.size / 2; i \u0026gt; 0; i--) { preDown(i); } } private void preDown(int p) { //* 下滤：将H中以Data[p]为根的子堆调整为最大堆 *//* int parent, child; int temp; temp = data[p]; //* 取出根结点存放的值 *//* for (parent = p; parent * 2 \u0026lt;= size; parent = child) { //这个过程与删除的过程一样 child = parent * 2; if ((child != size) \u0026amp;\u0026amp; (data[child] \u0026lt; data[child + 1])) child++; //* Child指向左右子结点的较大者 *//* if (temp \u0026gt;= data[child]) break; //* 找到了合适位置 *//* else //* 下滤X *//* data[parent] = data[child]; } data[parent] = temp; }  最小堆 最小堆的建立和操作与最大堆大致上是一样的。\npublic class MinHeap extends Heap{ public MinHeap(int maxSize, int sentinalVal) { super(maxSize, sentinalVal); } public MinHeap(int[] data, int maxSize, int sentinalVal) {//哨兵值 super(data, maxSize, sentinalVal); } public boolean insert(int element) { // 将元素X插入最小堆H，其中H-\u0026gt;Data[0]已经定义为哨兵 int i; if (isFull()) { System.out.println(\u0026quot;最大堆已满\u0026quot;); return false; } i = ++this.size; // i指向插入后堆中的最后一个元素的位置 for (; this.data[i / 2] \u0026gt; element; i /= 2) { data[i] = data[i / 2]; // 向下过滤结点，对调父结点的位置 } data[i] = element; // 将X插入 return true; } public int deleteMin() { // 从最小堆中取出键值为最小的元素，并删除一个结点 int parent, child; int minItem, temp;//minItem-堆顶元素，temp-临时变量 if (isEmpty()) { System.out.println(\u0026quot;最大堆已经为空\u0026quot;); return -1; } minItem = this.data[1];//取出根结点最小值 // 用最小堆中的最后一个元素从根结点开始向上过滤下层结点 temp = this.data[this.size--]; for (parent = 1; parent * 2 \u0026lt; this.size; parent = child) { child = parent * 2; // 左儿子的位置 if (child != this.size \u0026amp;\u0026amp; this.data[child] \u0026gt; this.data[child + 1]) { child++; //child 指向左右结点的较小者 } if (temp \u0026lt; this.data[child]) {//找到位置了 break; } else {//将子结点与父节点对换 this.data[parent] = this.data[child]; } } this.data[parent] = temp; return minItem; } //*----------- 建造最小堆 -----------*//* private void preDown(int p) { //* 下滤：将H中以Data[p]为根的子堆调整为最小堆 *//* int parent, child; int temp; temp = data[p]; //* 取出根结点存放的值 *//* for (parent = p; parent * 2 \u0026lt;= size; parent = child) { //这个过程与删除的过程一样 child = parent * 2; if ((child != size) \u0026amp;\u0026amp; (data[child] \u0026gt; data[child + 1])) child++; //* Child指向左右子结点的较小者 *//* if (temp \u0026lt;= data[child]) break; //* 找到了合适位置 *//* else //* 下滤X *//* data[parent] = data[child]; } data[parent] = temp; } public void buildHeap() { //* 调整Data[]中的元素，使满足最大堆的有序性 *//* //* 这里假设所有Size个元素已经存在Data[]中 *//* int i; //* 从最后一个结点的父节点开始，到根结点1 *//* for (i = this.size / 2; i \u0026gt; 0; i--) { preDown(i); } } }  总结 从堆的几种操作可以发现，删除和建堆的过程，就是从上往下调整堆的有序性的过程，插入元素的过程是从下往上调整堆的有序性的过程。\n参考 【1】数据结构-浙江大学\n","date":"2020年09月15日","permalink":"https://ahamoment.cn/posts/algorithm/algorithm-heap/","summary":"什么是堆 了解什么是堆之前，我们知道队列的概念，队列的特点是先进先出，但是有一种特殊的队列，取出元素的顺序是按照元素的优先权（关键字）大小，而","title":"堆"},{"contents":"现在越来越多的公司将服务通过容器来部署，但这里其实对Java的应用有一个坑。很多超时敏感的应用其实对GC的要求还是比较高的，减少GC的时间变得很重要，比如你可以根据当前机器的CPU核数得到一个较好的并发GC线程数 -XX:ParallelGCThreads，从而减少STW的时长。\n但在早期的JDK版本中，比如我们使用的Jdk1.8u102，当你使用Java的Runtime获取CPU数量时，在容器里面会返回容器所在宿主机的核数，而不是容器自身的：\nint cores = Runtime.getRuntime().availableProcessors();  这其实是JDK的一个问题，已经trace在JDK-8140793，原因是获取CPU核数是通过读取两个环境变量，其中\n   ENV Description     _SC_NPROCESSORS_CONF number of processors configured   _SC_NPROCESSORS_ONLN The number of processors currently online (available)    其中_SC_NPROCESSORS_CONF 就是我们需要容器真实的CPU数量。 获取CPU数量的源码\n怎么解决 第一种办法是使用新版本的Jdku131以上的版本1。\n另外一个办法是使用自编译上面的源代码，通过LD_PRLOAD的方式将修改后的so文件加载进去Mock掉CPU的核数\n Java SE support for Docker CPU and memory limits\n ","date":"2020年09月11日","permalink":"https://ahamoment.cn/posts/cloud/cloud-container-get-cpu/","summary":"现在越来越多的公司将服务通过容器来部署，但这里其实对Java的应用有一个坑。很多超时敏感的应用其实对GC的要求还是比较高的，减少GC的时间变","title":"容器内获取 CPU 核数问题"},{"contents":"背景 最近碰到一个问题，有个应用在启动的时候一直报错，错误信息如下：\njava: error while loading shared libraries: libjli.so: cannot open shared object file: No such file or directory  错误信息是说 java 应用加载不到 libjli.so 文件，我们使用 java -version 命令，同样的错误又出现了。使用 ldd 命令查看一下 java 应用是否加载了这个 so 文件，发现 java 应用加载的 so 文件中存在 libjli.so。\n$ ldd java linux-vdso.so.1 =\u0026gt; (0x00007ffe2a9c7000) /usr/local/lib/libsysconfcpus.so (0x00002ac503ca8000) libz.so.1 =\u0026gt; /lib64/libz.so.1 (0x00002ac503eaa000) libjli.so =\u0026gt; /apps/svr/jdk-14.0.1/bin/./../lib/libjli.so (0x00002ac5040c0000) libpthread.so.0 =\u0026gt; /lib64/libpthread.so.0 (0x00002ac5042d1000) libdl.so.2 =\u0026gt; /lib64/libdl.so.2 (0x00002ac5044ee000) libc.so.6 =\u0026gt; /lib64/libc.so.6 (0x00002ac5046f2000) /lib64/ld-linux-x86-64.so.2 (0x00002ac503883000)  我们接着查看了 LD_LIBRARY_PATH 和 /etc/ld.so.conf.d/xxx.conf 文件的配置，发现都是正常的。通过对比其他应用的启动配置，发现该应用使用了 80 端口启动，但是我们的容器只能使用 apps 权限登录，所以在启动前使用 setcap 命令提升了 java 应用的权限，允许其使用 80 端口，会不会是这个操作导致的呢？在查看原因之前，我们需要先理解几个概念。\nLinux 动态库 动态库(共享库)的代码在可执行程序运行时才载入内存，在编译过程中仅简单的引用，不同的应用程序如果调用相同的库,那么在内存中只需要有一份该动态库(共享库)的实例。这类库的名字一般是libxxx.so，其中so是 Shared Object 的缩写，即可以共享的目标文件。在链接动态库生成可执行文件时，并不会把动态库的代码复制到执行文件中，而是在执行文件中记录对动态库的引用。\nLinux下生成和使用动态库的步骤如下：\n 编写源文件。 将一个或几个源文件编译链接，生成共享库。 通过 -L -lxxx 的gcc选项链接生成的libxxx.so。例如gcc -fPIC -shared -o libmax.so max.c , -fPIC 是编译选项，PIC是 Position Independent Code 的缩写，表示要生成位置无关的代码，这是动态库需要的特性； -shared 是链接选项，告诉gcc生成动态库而不是可执行文件 把libxxx.so放入链接库的标准路径，或指定 LD_LIBRARY_PATH，才能运行链接了libxxx.so的程序。  Linux是通过 /etc/ld.so.cache 文件搜寻要链接的动态库的。而 /etc/ld.so.cache 是 ldconfig 程序读取 /etc/ld.so.conf 文件生成的。 （注意， /etc/ld.so.conf 中并不必包含 /lib 和 /usr/lib，ldconfig程序会自动搜索这两个目录）\n我们把要用的 libxx.so 文件所在的路径添加到 /etc/ld.so.conf 中，再以root权限运行 ldconfig 程序，更新 /etc/ld.so.cache ，程序运行时，就可以找到 libxx.so。另外就是通过配置 LD_LIBRARY_PATH 的方式来指定通过某些路径寻找链接的动态库。\nldd 查看程序依赖 理解了动态库的概念之后，当碰到某个程序报错缺少某个库文件时，我们应该怎么查看该程序当前加载了哪些库文件呢？可以用 ldd 命令。\nldd 命令的作用是用来查看程式运行所需的共享库,常用来解决程式因缺少某个库文件而不能运行的一些问题。\n例如：查看test程序运行所依赖的库:\n[root@localhost testso]# ldd /etc/alternatives/java linux-vdso.so.1 =\u0026gt; (0x00007ffde15f8000) libpthread.so.0 =\u0026gt; /lib64/libpthread.so.0 (0x00007f03f2f8d000) libdl.so.2 =\u0026gt; /lib64/libdl.so.2 (0x00007f03f2d89000) libc.so.6 =\u0026gt; /lib64/libc.so.6 (0x00007f03f29bb000) /lib64/ld-linux-x86-64.so.2 (0x00007f03f33ab000)   第一列：程序需要依赖什么库 第二列: 系统提供的与程序需要的库所对应的库 第三列：库加载的开始地址  通过上面的信息，我们可以得到以下几个信息：\n 通过对比第一列和第二列，我们可以分析程序需要依赖的库和系统实际提供的，是否相匹配 通过观察第三列，我们可以知道在当前的库中的符号在对应的进程的地址空间中的开始位置  如果依赖的某个库找不到，通过这个命令可以迅速定位问题所在.\nLinux capability 从内核 2.2 开始，Linux 将传统上与超级用户 root 关联的特权划分为不同的单元，称为 capabilites。Capabilites 作为线程(Linux 并不真正区分进程和线程)的属性存在，每个单元可以独立启用和禁用。如此一来，权限检查的过程就变成了：在执行特权操作时，如果进程的有效身份不是 root，就去检查是否具有该特权操作所对应的 capabilites，并以此决定是否可以进行该特权操作。\n下面是从 capabilities man page 中摘取的 capabilites 列表：\n   capability 名称 描述     CAP_AUDIT_CONTROL 启用和禁用内核审计；改变审计过滤规则；检索审计状态和过滤规则   CAP_AUDIT_READ 允许通过 multicast netlink 套接字读取审计日志   CAP_AUDIT_WRITE 将记录写入内核审计日志   CAP_BLOCK_SUSPEND 使用可以阻止系统挂起的特性   CAP_CHOWN 修改文件所有者的权限   CAP_DAC_OVERRIDE 忽略文件的 DAC 访问限制   CAP_DAC_READ_SEARCH 忽略文件读及目录搜索的 DAC 访问限制   CAP_FOWNER 忽略文件属主 ID 必须和进程用户 ID 相匹配的限制   CAP_FSETID 允许设置文件的 setuid 位   CAP_IPC_LOCK 允许锁定共享内存片段   CAP_IPC_OWNER 忽略 IPC 所有权检查   CAP_KILL 允许对不属于自己的进程发送信号   CAP_LEASE 允许修改文件锁的 FL_LEASE 标志   CAP_LINUX_IMMUTABLE 允许修改文件的 IMMUTABLE 和 APPEND 属性标志   CAP_MAC_ADMIN 允许 MAC 配置或状态更改   CAP_MAC_OVERRIDE 覆盖 MAC(Mandatory Access Control)   CAP_MKNOD 允许使用 mknod() 系统调用   CAP_NET_ADMIN 允许执行网络管理任务   CAP_NET_BIND_SERVICE 允许绑定到小于 1024 的端口   CAP_NET_BROADCAST 允许网络广播和多播访问   CAP_NET_RAW 允许使用原始套接字   CAP_SETGID 允许改变进程的 GID   CAP_SETFCAP 允许为文件设置任意的 capabilities   CAP_SETPCAP 参考 capabilities man page   CAP_SETUID 允许改变进程的 UID   CAP_SYS_ADMIN 允许执行系统管理任务，如加载或卸载文件系统、设置磁盘配额等   CAP_SYS_BOOT 允许重新启动系统   CAP_SYS_CHROOT 允许使用 chroot() 系统调用   CAP_SYS_MODULE 允许插入和删除内核模块   CAP_SYS_NICE 允许提升优先级及设置其他进程的优先级   CAP_SYS_PACCT 允许执行进程的 BSD 式审计   CAP_SYS_PTRACE 允许跟踪任何进程   CAP_SYS_RAWIO 允许直接访问 /devport、/dev/mem、/dev/kmem 及原始块设备   CAP_SYS_RESOURCE 忽略资源限制   CAP_SYS_TIME 允许改变系统时钟   CAP_SYS_TTY_CONFIG 允许配置 TTY 设备   CAP_SYSLOG 允许使用 syslog() 系统调用   CAP_WAKE_ALARM 允许触发一些能唤醒系统的东西(比如 CLOCK_BOOTTIME_ALARM 计时器)    getcap 命令和 setcap 命令分别用来查看和设置程序文件的 capabilities 属性。\n例如为 ping 命令文件添加 capabilities\n执行 ping 命令所需的 capabilities 为 cap_net_admin 和 cap_net_raw，通过 setcap 命令可以添加它们：\n$ sudo setcap cap_net_admin,cap_net_raw+ep /bin/ping  移除添加的 capabilities ，执行下面的命令：\n$ sudo setcap cap_net_admin,cap_net_raw-ep /bin/ping  命令中的 ep 分别表示 Effective 和 Permitted 集合(接下来会介绍)，+ 号表示把指定的 capabilities 添加到这些集合中，- 号表示从集合中移除(对于 Effective 来说是设置或者清除位)。\n解决问题 回到我们开始的问题，由于我们为非 root 用户赋予了使用 80 端口的权限，调用了如下命令：\nsetcap cap_net_bind_service=+ep /usr/bin/java  当一个可执行文件提升了权限后，运行时加载程序（rtld）— ld.so，它不会与不受信任路径中的库链接。Linux 会为使用了 setcap 或 suid 的程序禁用掉 LD_LIBRARY_PATH。所以就出现了 java 程序加载不到 libjli.so 的情况了，这是 JDK 的一个 bug。\n JDK-7157699 : can not run java after granting posix capabilities\n 那么既然使用 setcap 后不会加载链接库，我们就可以将 libjli.so 所在的路径添加到 /etc/ld.so.conf/xxx.conf中，例如：\n% cat /etc/ld.so.conf.d/java.conf /usr/java/jdk1.8.0_261-amd64/lib/amd64/jli  使用 ldconfig 重载 so 文件。\n[root@localhost jli]# ldconfig -p | grep libjli libjli.so (libc6,x86-64) =\u0026gt; /usr/java/jdk1.8.0_261-amd64/lib/amd64/jli/libjli.so% ldconfig | grep libjli libjli.so -\u0026gt; libjli.so .......  这样再次测试就可以了。\n参考文章 【1】Linux动态库生成与使用指南\n【2】ldd 查看程序依赖库\n【3】Linux Capabiliites 简介\n【4】capabilities(7) - Linux man page\n【5】如何允许非 root 进程绑定低位端口\n【6】[How to get Oracle java 7 to work with setcap cap_net_bind_service+ep](https://unix.stackexchange.com/questions/87978/how-to-get-oracle-java-7-to-work-with-setcap-cap-net-bind-serviceep)\n","date":"2020年09月11日","permalink":"https://ahamoment.cn/posts/linux/linux-cap/","summary":"背景 最近碰到一个问题，有个应用在启动的时候一直报错，错误信息如下： java: error while loading shared libraries: libjli.so: cannot open shared object file: No such file or directory 错误信息是说 java 应用加载不到 libjli.so 文件，我们使","title":"解决setcap导致Java加载libjli.so 失败问题"},{"contents":"平衡二叉树 平衡二叉树也是一种搜索树。\n搜索树节点不同插入次序，将导致不同的深度和平均查找长度 ASL。\n平衡因子（Balance Factor）:BF(T)=hL-hR，其中hL和hR分别是T的左右子树的高度。平衡二叉树（Balanced Binary Tree) 又叫 AVL树，当树不为空时，在任一节点左，右子树高度差的绝对值不超过1，即 |BF(T)|\u0026lt;=1。\n平衡二叉树的高度能够达到 $log_2n$\n平衡二叉树的调整 任何情况都可以归结为四种模式。根据插入节点的位置不同使用不同的查找方法，同时记住平衡二叉树是搜索树，节点小于左边大于右边。平衡二叉树的四种调整方法为：右单旋，左单旋，右左双旋，左右双旋。\n右单旋 按照字母大小插入三个结点，Mar, May, Nov：\n如上图左边所示，在插入 Nov 结点后，二叉树的平衡被破坏，结点Mar 的平衡因子为 -2，这个时候我们称 Mar 为不平衡的发现者，麻烦节点 Nov 在发现者右子树的右边，需要RR旋转（右单旋）。\n右单旋的过程如上图所示，将 B 结点调整为跟结点，$B_L$ 调整为 A 的右结点。上图所示的是插入到右子树的右子树的右边时，当插入到右子树的右子树的位置不同时怎么处理？来看个例子：\n结点插到左边和右边的时候，调整平衡后位置还是不变。\n// 右单旋 public AVLTree\u0026lt;T\u0026gt; singleRightRotation(AVLTree\u0026lt;T\u0026gt; a) { // 注意：A 必须有一个右子节点B // 将 A 与 B 做右单旋，更新A与B的高度，返回新的根结点B AVLTree\u0026lt;T\u0026gt; b = a.right; a.right = b.left; b.left = a; a.height = Math.max(postOrderGetHight(a.left), postOrderGetHight(a.right)); b.height = Math.max(postOrderGetHight(b.right), a.height); return b; }  左单旋 左单旋与右单旋类似，麻烦结点在发现者的左子树的左边，因而叫LL插入，需要 LL 旋转（左单旋）。\n上图是左单旋的调整过程。注意，对于调整，只需要从最下层的开始调整就行。下层平衡了，上层自然也会平衡。\n// 左单旋(LL旋转) public AVLTree\u0026lt;T\u0026gt; singleLeftRotation(AVLTree\u0026lt;T\u0026gt; a) { // 注意：A必须有一个左子结点B //将 A与B做左单旋，更新A与B的高度，返回新的根结点B AVLTree\u0026lt;T\u0026gt; b = a.left; a.left = b.right; b.right = a; a.height = Math.max(postOrderGetHight(a.left), postOrderGetHight(a.right)); b.height = Math.max(postOrderGetHight(b.left), a.height); return b; }  左右双旋 对于左右双旋，麻烦结点出现在左子树的右边，因而叫 LR 插入，需要做 LR 旋转。\n当插入的结点在 C 的左子树或者右子树下边时，就需要左右双旋，左右双旋可以看成做了两次单旋，目的是调整 A,B,C 这三个结点的位置。先将B和C 做一次右单旋，再将 C 和 A 做一次左单旋。\npublic AVLTree\u0026lt;T\u0026gt; doubleLeftRightRotation(AVLTree\u0026lt;T\u0026gt; a) { // 注意：A必须有一个左子节点B，且B必须有一个右子节点C // 将A、B与C做两次单旋，返回新的根结点C // 将B与C做右单旋，C被返回 a.left = singleLeftRotation(a.left); // 将A与C做左单旋，C被返回 return singleLeftRotation(a); }  右左双旋 右左双旋时，麻烦结点出现在右子树的左边，因而又叫 RL 插入。\n同样的，我们也可将右左双旋看成是两次单旋，将 B 和 C 做左单旋，再将 A 与 C 做右单旋。\npublic AVLTree\u0026lt;T\u0026gt; doubleRightLeftRotation(AVLTree\u0026lt;T\u0026gt; a) { // 注意：A必须有一个右子节点B，且B必须有一个左子节点C // 将A、B与C做两次单旋，返回新的根结点C // 将B与C做左单旋，C被返回 a.right = singleRightRotation(a.right); // 将A与C做右单旋，C被返回 return singleLeftRotation(a); }  注意调整时，可能节点的位置不变，但是平衡因子需要更新。\n最后，我们来看一下平衡二叉树的插入，每一次的插入都要判断平衡是否被破坏，如果发现树的平衡被破坏，根据插入的位置做上面的四种旋转重新调整成平衡二叉树。\npublic AVLTree\u0026lt;T\u0026gt; insert(AVLTree\u0026lt;T\u0026gt; t, T x) { /* 将X插入AVL树T中，并且返回调整后的AVL树 */ if (t != null) { /* 若插入空树，则新建包含一个结点的树 */ t = new AVLTree\u0026lt;\u0026gt;(x, null, null); t.height = 0; } /* if (插入空树) 结束 */ else if (x.compareTo(t.data) \u0026lt; 0) { /* 插入T的左子树 */ t.left = insert(t.left, x); /* 如果需要左旋 */ if (postOrderGetHight(t.left) - postOrderGetHight(t.right) == 2) if (x.compareTo(t.left.data) \u0026lt; 0) t = singleLeftRotation(t); /* 左单旋 */ else t = doubleLeftRightRotation(t); /* 左-右双旋 */ } /* else if (插入左子树) 结束 */ else if (x.compareTo(t.data) \u0026gt; 0) { /* 插入T的右子树 */ t.right = insert(t.right, x); /* 如果需要右旋 */ if (postOrderGetHight(t.left) - postOrderGetHight(t.right) == -2) if (x.compareTo(t.right.data) \u0026gt; 0) t = singleRightRotation(t); /* 右单旋 */ else t = doubleRightLeftRotation(t); /* 右-左双旋 */ } /* else if (插入右子树) 结束 */ /* else X == T-\u0026gt;Data，无须插入 */ /* 别忘了更新树高 */ t.height = Math.max(postOrderGetHight(t.left), postOrderGetHight(t.right)) + 1; return t; }  参考 【1】浙江大学陈越老师的数据结构课程\n","date":"2020年08月31日","permalink":"https://ahamoment.cn/posts/algorithm/algorithm-balanced-binary-tree/","summary":"平衡二叉树 平衡二叉树也是一种搜索树。 搜索树节点不同插入次序，将导致不同的深度和平均查找长度 ASL。 平衡因子（Balance Factor）:B","title":"平衡二叉树"},{"contents":" 本文翻译自LEARN UNIX，博主在原文的基础上添加了一些内容。如果没有Linux 机器，推荐使用该网站 https://www.tutorialspoint.com/execute_ksh_online.php 作为shell在线demo的环境。\n 1. Shell 是什么 Shell为您提供了与Unix系统的接口。它收集您的输入，并根据该输入执行程序。程序完成执行后，将显示该程序的输出。 Shell 是可以运行我们的命令，程序，shell 脚本的环境。Shell 有不同的类型，每种Shell都有它自己的命令和功能。\n1.1 Shell 类型 Linux 系统中，主要有两种类型的Shell：\n Bourne shell - 如果使用Bourne类型的shell，默认的提示符就是$，可以通过修改环境变量PS1来更改你的提示符。 C shell - C 类型的shell，默认的提示符是%。  Bourne Shell 又有下面几种类型：\n Bourne shell (sh) Korn shell (ksh) Bourne Again shell (bash) POSIX shell (sh)  C 类型 的Shell包括以下两种：\n C shell (csh) TENEX/TOPS C shell (tcsh)  在大多数 Linux 版本中，Bourne shell 通常都安装为/bin/sh。由于这个原因，它是不同版本的Linux 的首选 Shell。在本文中，我们将介绍大多数基于Bourne Shell的Shell概念。\n1.2 Shell 脚本 Shell脚本的基本概念是命令列表，按执行顺序列出命令。 Shell 脚本文件以 .sh 结尾，例如 test.sh 。Shell 脚本的内容以 #! 开头，告诉系统接下来的命令将会被 Bourne Shell 执行。我们来创建一个Shell脚本，并往脚本中添加一点注释，注释以 # 开头：\n#!/bin/bash # Author: xueqiang.chen # Script follows here: pwd ls  保存命令并执行脚本，可以看到以下输出内容：\n$ chmod +x test.sh $ ./test.sh /home/centos go.sh test.sh  Shell 脚本可以有很复杂的结构，毕竟，shell是一种真正的编程语言，它包括变量，控制结构等。无论脚本变得多么复杂，它仍然只是顺序执行的命令的列表。\n2. Shell 变量 变量是我们为其分配值的字符串，变量的值包括数字，文本，文件名，设备，或是任意的数据类型。变量只是实际数据的一个指针，我们可以创建，赋值，删除变量。\n2.1 变量类型 当Shell运行时，存在三种主要类型的变量：\n 局部变量：局部变量是存在于Shell当前实例中的变量。它不适用于由 Shell 启动的程序。它们在命令提示符下设置。 环境变量：环境变量可用于Shell的任何子进程。某些程序需要环境变量才能正常运行。通常，shell脚本仅定义其运行的程序所需的那些环境变量。 Shell变量：Shell变量是Shell设置的特殊变量，shell要求Shell变量才能正常运行。这些变量中的一些是环境变量，而另一些是局部变量。  2.2 变量名 变量名称只能包含字母（a到z或A到Z），数字（0到9）或下划线字符（_）。按照约定，Unix shell变量将以大写字母命名。 下面的例子是一些有效和无效的变量名：\n#!/bin/bash # valid variable names _ALL TOKEN_A VAR_1 VAR_2 # invaild variable names 2_VAR -VARIABLE VAR1-VAR2 VAR_A!  在shell 中，!, *, - 是含有特殊意义的，因此不能在变量名中使用。\n2.3 定义变量 变量的定义方式如下：\nvariable_name=variable_value  注意，= 号两边不能有空格。\n#!/bin/bash NAME=\u0026quot;Zara Ali\u0026quot;  这个例子中定义了一个变量，并给这个变量赋值，这种变量的类型称为标量， 标量只一次只能有一个值。\n2.4 使用变量 Shell 中通过 $ 符号获取变量的值。例如，下面的例子将获取变量NAME的值并将其打印在标准输出中：\n#!/bin/bash NAME=\u0026quot;Zara Ali\u0026quot; echo $NAME  2.5 只读变量 Shell 允许通过 readonly 命令将一个变量变成只读变量。当变量成为只读变量后，它的值就不能更改了。\n#!/bin/bash NAME=\u0026quot;Zara Ali\u0026quot; readonly NAME NAME=\u0026quot;Qadiri\u0026quot;  上面的脚本会发生错误。\n/bin/sh: NAME: This variable is read only.  2.6 重置变量 重置或删除变量将指示Shell程序从其跟踪的变量列表中删除该变量。重置变量后，将无法访问该变量中的存储值。 以下是使用 unset 命令取消定义的变量的语法：\nunset variable_name  上面的命令重置定义的变量的值，下面的这个例子简单说明这个命令是如何工作的：\n#!/bin/bash NAME=\u0026quot;Zara Ali\u0026quot; unset NAME echo $NAME  上面的例子不会打印出任何内容，你可以使用 unset 命令来重置被标记为 readonly 的变量。\n3. 特殊变量 在上一节中，我们了解了在变量名称中使用某些非字母数字字符时应注意的事项。这是因为这些字符用在特殊的Unix变量的名称中。这些变量保留用于特定功能。\n下面的表格列出了可以在我们的脚本中使用的特殊变量：\n   变量 说明     $0 当前脚本的文件名   $n 这个变量对应于调用脚本的参数。其中参数 n 是正整数，代表参数的位置。例如第一个参数就是 $1 ， 第二个参数就是 $2， 以此类推。   $# 脚本参数的数量   $* 输出整个参数列表，将整个列表作为一个参数，且之间使用空格隔开。   $@ 与$* 的作用是一致的，不同的是该变量输出时会将参数列表分成单独的参数。   $$ 当前shell的进程号。对于Shell脚本，这是它们执行时的进程ID。   $! 最后一个后台命令的进程号。   以下脚本使用与命令行相关的各种特殊变量:     #!/bin/sh echo \u0026quot;File Name: $0\u0026quot; echo \u0026quot;First Parameter : $1\u0026quot; echo \u0026quot;Second Parameter : $2\u0026quot; echo \u0026quot;Quoted Values: $@\u0026quot; echo \u0026quot;Quoted Values: $*\u0026quot; echo \u0026quot;Total Number of Parameters : $#\u0026quot;  运行结果：\n$./test.sh Zara Ali File Name : ./test.sh First Parameter : Zara Second Parameter : Ali Quoted Values: Zara Ali Quoted Values: Zara Ali Total Number of Parameters : 2  3.1 错误状态 $? 变量表示上一个命令的退出状态。 退出状态是每个命令完成后返回的数值。通常，如果大多数命令成功，则返回退出状态0；如果不成功，则返回1。 某些命令出于特殊原因会返回其他退出状态。例如，某些命令区分错误的种类，并将根据故障的特定类型返回各种退出值。\n下面的这个例子返回的是成功命令的状态：\n$./test.sh Zara Ali File Name : ./test.sh First Parameter : Zara Second Parameter : Ali Quoted Values: Zara Ali Quoted Values: Zara Ali Total Number of Parameters : 2 $echo $? 0 $  4. 数组 Shell变量足以容纳单个值。这些变量称为标量变量。\nShell支持另一种类型的变量，称为数组变量。它可以同时保存多个值。数组提供了一种对一组变量进行分组的方法。数组的命名参考变量的命名规则。\n4.1 定义数组 数组变量和标量变量之间的差异可以解释如下。 假设您尝试将各个学生的姓名表示为一组变量。每个单独的变量都是标量变量，如下所示：\n#!/bin/bash NAME01=\u0026quot;Zara\u0026quot; NAME02=\u0026quot;Qadir\u0026quot; NAME03=\u0026quot;Mahnaz\u0026quot; NAME04=\u0026quot;Ayan\u0026quot; NAME05=\u0026quot;Daisy\u0026quot;  我们可以使用单个数组来存储所有上述名称。以下是创建数组变量的最简单方法。\narray_name[index]=value  这里 array_name 是数组的名字，index 是要设置的数组中的索引， value 就是你要为该元素设置的值。如下所示：\n#!/bin/bash NAME[0]=\u0026quot;Zara\u0026quot; NAME[1]=\u0026quot;Qadir\u0026quot; NAME[2]=\u0026quot;Mahnaz\u0026quot; NAME[3]=\u0026quot;Ayan\u0026quot; NAME[4]=\u0026quot;Daisy\u0026quot;  如果使用的是 bash shell ，也可以通过以下这种方法进行数组初始化：\narray_name=(value1 ... valuen)  4.2 使用数组 为变量赋值之后，访问变量可以使用以下方式：\n${array_name[index]}  这里 array_name 是数组名， index 是要访问那个数组项的索引。具体的例子如下：\n#!/bin/bash NAME[0]=\u0026quot;Zara\u0026quot; NAME[1]=\u0026quot;Qadir\u0026quot; NAME[2]=\u0026quot;Mahnaz\u0026quot; NAME[3]=\u0026quot;Ayan\u0026quot; NAME[4]=\u0026quot;Daisy\u0026quot; echo \u0026quot;First Index: ${NAME[0]}\u0026quot; echo \u0026quot;Second Index: ${NAME[1]}\u0026quot; echo \u0026quot;First Method: ${NAME[*]}\u0026quot; echo \u0026quot;Second Method: ${NAME[@]}\u0026quot;  运行结果：\n$./test.sh First Index: Zara Second Index: Qadir First Method: Zara Qadir Mahnaz Ayan Daisy Second Method: Zara Qadir Mahnaz Ayan Daisy  上面的示例中通过 * 和 @ 来获取整个数组的值。\n${array_name[*]} ${array_name[@]}  5. 运算符 每个Shell都支持不同的运算符，这里我们主要讨论的是 bash shell 的运算符。\n5.1 算术运算符 Bourne shell 最初没有任何执行简单算术运算的机制，它使用 awk 或 expr 来进行计算。如下所示：\n#!/bin/sh val=`expr 2 + 2` echo \u0026quot;Total value : $val\u0026quot;  运行结果：\nTotal value : 4  上面的例子我们需要注意的有两点：\n 表达式和运算符之间必须用空格隔开，例如 2+2 是错误的，应该写成 2 + 2 整个表达式要用反引号 `` 来包起来。  假设变量 a 等于 10， 变量 b 等于 20， 我们来看一下 bash shell 支持的算术运算符是如何计算这两个值得：\n   运算符 示例     + expr $a + $b = 30   - expr $a - $b = -10   * expr $a \\* $b = 200   / expr $b / $a = 2   % expr $b % $a = 0   = a = $b 将b的值赋给a   == [ $a == $b ] 将会返回 false   ！= [ $a != $b ]将会返回 true    注意：[ $a == $b ] 不能写成 [$a==$b]。\n5.2 关系运算符 Bash 支持以下特定于数值的关系运算符。 假设变量a = 10，变量b = 20,\n   运算符 描述 例子     -eq 检查运算符两边的值是否相等，相等返回 true [ $a -eq $b ] is not true.   -ne 检查运算符两边的值是否相等，不相等返回 true [ $a -ne $b ] is true   -gt gt 是 greater than 的缩写，检查运算符左边的值是否大于右边，是的话返回 true [ $a -gt $b ] is not true   -lt lt 是 less than 的缩写，检查运算符左边的值是否小于右边，是的话返回true [ $a -lt $b ] is true   -ge ge 是 greater than or equal 的缩写，检查运算符左边的值是否大于或等于右边的值，是的话返回true [ $a -ge $b ] is not true   -le le 是 less than or equal 的缩写，检查运算符左边的值是否小于或等于右边的值，是的话返回true [ $a -le $b ] is true    注意：所有条件表达式应放在方括号内并在其周围留有空格。\n5.3 布尔运算符 Bash 支持以下布尔运算符。假设变量 a 的值是 10， 变量 b 的值是20：\n   运算符 描述 示例     ！ 逻辑否。这会将真实条件转换为错误条件，反之亦然。 [ ! false ] is true.   -o 逻辑或。如果运算符两边之一为真，则条件为真。 [ $a -lt 20 -o $b -gt 100 ] is true.   -a 逻辑与。如果运算符两边都是真的，则条件为真。 [ $a -lt 20 -a $b -gt 100 ] is false.    5.4 字符串运算符 Bash 运算符支持以下操作。 假设变量 a 的值为 \u0026ldquo;abc\u0026rdquo;，变量b的值为 \u0026ldquo;efg\u0026rdquo;：\n   运算符 描述 示例     = 检查运算符两边的值是否相等；如果是，则条件变为真。 [ $a = $b ] is not true.   != 检查运算符两边的值是否相等；如果值不相等，则条件为真 [ $a != $b ] is true.   -z 检查给定的字符串操作数大小是否为零；如果长度为零，则返回true。 [ -z $a ] is not true.   -n 检查给定的字符串操作数大小是否为非零；如果长度非零，则返回true。 [ -n $a ] is not false.   str Checks if str is not the empty string; if it is empty, then it returns false. [ $a ] is not false.    5.5 文件测试运算符 我们有一些运算符可用于测试与文件相关的各种属性。 假设有个文件变量 file 的值为一个存在的名为 \u0026ldquo;test\u0026rdquo; 的文件，该文件的大小为100bytes，并且有读写和执行的权限。\n   运算符 描述 示例     -b file 检查文件是否是一个块文件，如果是就返回true。 [ -b $file ] 返回 false。   -c file 检测文件是否是字符设备文件，如果是，则返回 true。 [ -c $file ] 返回 false。   -d file 检测文件是否是目录，如果是，则返回 true。 [ -d $file ] 返回 false。   -f file 检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。 [ -f $file ] 返回 true。   -g file 检测文件是否设置了 SGID 位，如果是，则返回 true。 [ -g $file ] 返回 false。   -k file 检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。 [ -k $file ] 返回 false。   -p file 检测文件是否是有名管道，如果是，则返回 true。 [ -p $file ] 返回 false。   -t file 检查文件描述符是否打开并与终端关联；如果是，则条件变为真。 [ -t $file ] is false.   -u file 检测文件是否设置了 SUID 位，如果是，则返回 true。 [ -u $file ] 返回 false。   -r file 检测文件是否可读，如果是，则返回 true。 [ -r $file ] 返回 true。   -w file 检测文件是否可写，如果是，则返回 true。 [ -w $file ] 返回 true。   -x file 检测文件是否可执行，如果是，则返回 true。 [ -x $file ] 返回 true。   -s file 检测文件是否为空（文件大小是否大于0），不为空返回 true。 [ -s $file ] 返回 true。   -e file 检测文件（包括目录）是否存在，如果是，则返回 true。 [ -e $file ] 返回 true。    5.6 其他shell的运算符  C Shell 运算符：C Shell Operators Korn Shell 运算符：Korn Shell Operators  6. 条件表达式 在编写shell脚本时，可能需要从给定的两个路径中采用一个路径。因此，您需要使用条件语句，这些条件语句允许您的程序做出正确的决定并执行正确的操作。\n7. 循环 循环是功能强大的编程工具，使您能够重复执行一组命令。在本章中，我们将研究以下可供Shell程序员使用的循环类型-\n while 循环 for 循环 until 循环 select 循环  您将根据情况使用不同的循环。\n7.1 嵌套循环 所有循环都支持嵌套概念，这意味着您可以将一个循环放入另一个类似的或不同的循环中。根据您的要求，此嵌套最多可以无限次。\n这是嵌套while循环的示例。其他循环可以根据编程要求以类似的方式嵌套-\n可以将while循环用作另一个while循环主体的一部分。 语法：\nwhile command1 ; # this is loop1, the outer loop do Statement(s) to be executed if command1 is true while command2 ; # this is loop2, the inner loop do Statement(s) to be executed if command2 is true done Statement(s) to be executed if command1 is true done  示例：\n#!/bin/sh a=0 while [ \u0026quot;$a\u0026quot; -lt 10 ] # this is loop1 do b=\u0026quot;$a\u0026quot; while [ \u0026quot;$b\u0026quot; -ge 0 ] # this is loop2 do echo -n \u0026quot;$b \u0026quot; b=`expr $b - 1` done echo a=`expr $a + 1` done  运行结果：\n0 1 0 2 1 0 3 2 1 0 4 3 2 1 0 5 4 3 2 1 0 6 5 4 3 2 1 0 7 6 5 4 3 2 1 0 8 7 6 5 4 3 2 1 0 9 8 7 6 5 4 3 2 1 0  ","date":"2020年08月27日","permalink":"https://ahamoment.cn/posts/linux/linux-bash-tutorials/","summary":"本文翻译自LEARN UNIX，博主在原文的基础上添加了一些内容。如果没有Linux 机器，推荐使用该网站 https://www.tutorialspoint.com/execute_ksh_online.php 作为shell在线demo的环境。 1.","title":"Linux Shell Script 基础教程"},{"contents":"1. 二叉搜索树的概念 二叉搜索树，也称为二叉排序树或二叉查找树。一棵不为空的二叉搜索树满足以下性质：\n 非空左子树的所有键值小于其根结点的键值。 非空右子树的所有键值大于其根结点的键值。 左，右子树都是二叉搜索树。  2. 二叉搜索树的查找 从二叉搜索树BST中查找元素X，返回其所在结点的地址。二叉搜索树的查找过程可以描述为以下步骤：\n 查找从根结点开始，如果树为空，直接返回 null 若查找树非空，则根结点关键字与 X 进行比较，并进行不同的处理。  x 小于根结点的键值，在左子树中搜索； x 大于根结点的键值，在右子树中搜索； 若两者比较的结果相等，搜索完成，直接返回指向结点的指针。    我们用递归来实现查找过程，\npublic TreeNode\u0026lt;T\u0026gt; find(T x, TreeNode\u0026lt;T\u0026gt; bst) { if (bst == null) { System.out.println(\u0026quot;树为空，查找失败！\u0026quot;); return null; //查找失败 } int result = x.compareTo(bst.data); if (result \u0026gt; 0) { // x 大于根结点的值，向右子树递归查找 return find(x, bst.right); } else if (result \u0026lt; 0) { // x 小于根结点的值，向左子树递归查找 return find(x, bst.left); } else { // 找到 x 的值，直接返回结点所在的指针 return bst; } }  在这个递归实现中，两个递归过程都是尾递归，可以改成用迭代函数来实现，提高执行效率。\npublic TreeNode\u0026lt;T\u0026gt; findNonRecursive(T x, TreeNode\u0026lt;T\u0026gt; bst) { while (bst != null) { int result = x.compareTo(bst.data); if (result \u0026gt; 0) { // 向右子树中移动，继续查找 bst = bst.right; } else if (result \u0026lt; 0) { // 向左子树中移动，继续查找 bst = bst.left; } else { // x == bst.data // 查找成功，返回结点的找到结点的地址 return bst; } } // 查找失败 return null; }  除了通用的查找外，二叉搜索树经常要使用到最大和最小元素的查找。对于一棵二叉搜索树来说，最大元素一定是在树的最右分支的端结点上，最小元素一定是在树的最左分支的端结点上。\n最大元素和最小元素的递归和非递归实现为：\n// 查找树的最小值 public TreeNode\u0026lt;T\u0026gt; findMin(TreeNode\u0026lt;T\u0026gt; bst) { if (bst == null) { // 空的二叉搜索树，返回NULL return null; } else if (bst.left == null) { // 找到最左叶结点并返回 return bst; } else { // 沿左分支继续查找 return findMin(bst.left); } } // 查找树的最大值 public TreeNode\u0026lt;T\u0026gt; findMax(TreeNode\u0026lt;T\u0026gt; bst) { if (bst == null) { // 空的二叉搜索树 return null; } else if (bst.right == null) { // 找到最右的叶结点并返回 return bst; } else { // 沿右分支继续查找 return findMax(bst.right); } } // 最小值非递归实现 public TreeNode\u0026lt;T\u0026gt; findMinNonRecursive(TreeNode\u0026lt;T\u0026gt; bst) { if (bst != null) { while (bst.left != null) { // 沿左分支继续查找，直到最右叶结点 bst = bst.left; } } return bst; } // 查找最大值非递归实现 public TreeNode\u0026lt;T\u0026gt; findMaxNonRecursive(TreeNode\u0026lt;T\u0026gt; bst) { if (bst != null) { while (bst.right != null) { // 沿右分支继续查找，直到最右叶结点 bst = bst.right; } } return bst; }  3. 二叉搜索树的插入 插入指定结点到一棵二叉树中，这个过程我们关键要找到这个结点要插入的位置，可以采用与查找类似的方法。\n// 插入操作 public TreeNode\u0026lt;T\u0026gt; insert(T x, TreeNode\u0026lt;T\u0026gt; bst) { if (bst == null) { // 若原树为空，生成并返回一个结点的二叉搜索树 bst = new TreeNode\u0026lt;\u0026gt;(); bst.data = x; bst.left = bst.right = null; } else { // 开始查找要插入元素的位置 int result = x.compareTo(bst.data); if (result \u0026lt; 0) { // 递归插入左子树 bst.left = insert(x, bst.left); } else if (result \u0026gt; 0) { // 递归插入右子树 bst.right = insert(x, bst.right); } /*else { // x 元素已经存在, 不做任何操作 // }*/ } return bst; }  通过递归左右子树，找到要插入的位置，然后插入结点。\n4. 二叉搜索树的删除 删除一棵二叉树，分为三种情况，分别来分析一下：\n第一种，要删除的结点是树的叶子结点，那么可以直接删除，并修改其父结点的指针为 null，例如，删除下面这棵二叉搜索树 35 这个结点：\n第二种，要删除的结点有一个孩子结点，那么删除该结点后，要修改其父结点的指针到要删除结点的孩子结点上。例如，删除下面这棵二叉搜索树的33结点。\n第三种，要删除的结点有左、右两个子树，用另一结点替代被删除结点：右子树的最小元素或者左子树的最大元素。例如，删除41 这个结点的时候。\n实现如下：\npublic TreeNode\u0026lt;T\u0026gt; delete(T x, TreeNode\u0026lt;T\u0026gt; bst) { TreeNode\u0026lt;T\u0026gt; tmp; if (bst == null) { System.out.println(\u0026quot;树为空，要删除的元素未找到！\u0026quot;); return null; } else if (x.compareTo(bst.data) \u0026lt; 0) { //左子树递归删除 bst.left = delete(x, bst.left); } else if (x.compareTo(bst.data) \u0026gt; 0) { bst.right = delete(x, bst.right); //右子树递归删除 } else { //找到要删除的节点 if (bst.left != null \u0026amp;\u0026amp; bst.right != null) { //被删除结点有左右两个子结点 tmp = findMin(bst.right); // 在右子树中找最小的元素填充删除结点 bst.data = tmp.data; bst.right = delete(bst.data, bst.right); } else { // 被删除结点有一个或无子结点 if (bst.left == null) { // 有右孩子或无子结点 bst = bst.right; } else if (bst.right == null) { bst = bst.left; } } } return bst; }  题目：是否是同一棵二叉搜索树\n给定一个插入序列就可以唯一确定一棵二叉搜索树。然后，给定一棵二叉搜索树可以由多种不同的插入序列得到。\n例如，按照序列(2,1,3)和（2，3，1）插入初始为空的二叉搜索树，得到一样的结果。\n问题：对于输入的各种插入序列，你需要判断他们是否能生成一样的二叉搜索树。\n参考 【1】浙江大学陈越老师的数据结构课程\n","date":"2020年08月19日","permalink":"https://ahamoment.cn/posts/algorithm/algorithm-binary-search-tree/","summary":"1. 二叉搜索树的概念 二叉搜索树，也称为二叉排序树或二叉查找树。一棵不为空的二叉搜索树满足以下性质： 非空左子树的所有键值小于其根结点的键值。 非空","title":"二叉搜索树"},{"contents":"摘要：二叉树的定义，遍历二叉树\n1. 二叉树的定义  二叉树：一个有穷的结点集合。这个集合可以为空，若不为空，则它是由根结点和称为其左子树 TL 和右子树 TR 的两个不想交的二叉树组成。\n   二叉树具体五种基本形态\n  二叉树的子树有左右顺序之分\n  特殊二叉树   斜二叉树（Skewed Binary Tree）\n  完美二叉树（Perfect Binary Tree），又称为满二叉树（Full Binary Tree）\n一棵完美二叉树所有的结点都有左右两个子结点。\n  完全二叉树（Complete Binary Tree）\n完全二叉树的最后一层可以却是部分结点，例如右半部分缺失了12-15号结点，但是这样的二叉树不是完全二叉树：\n2. 二叉树的几个重要性质   一个二叉树第 i 层的最大结点数为：$2^{i-1},i\\geq1$\n  深度为 K 的二叉树有最大结点总数为：$2^{k}-1, k\\geq1$\n  对任何非空的二叉树 T， 若 $n_0$ 表示叶结点的个数，$n_2$ 是度为2的非叶结点个数，那么两者满足关系 $n_0=n_2+1$\n这个关系很容易可以推导出来，从边的角度出发，我们知道每个二叉树的边是确定的，从最后一层往上看，除根结点外，每个结点都有一条边和上一个结点连接，总的边数为: $n_0+n_1+n_2-1$；从根结点往下，度为2的结点对有两条边跟其他结点连接，度为1的结点为一条，叶结点为0条，即：$2n_2+n_1+0n_0$。 这两个式子相等： $$ n_0+n_1+n_2-1=2n_2+n_1 $$ 化简后就可以得到上面那个式子。\n  3. 二叉树的抽象数据类型定义 二叉树的结构由数据，左子树，右子树组成，操作集包括：判空，遍历，创建一个二叉树。\n/** * 二叉树树节点定义 * @param \u0026lt;T\u0026gt; */ public class TreeNode\u0026lt;T\u0026gt; { public T data; //节点数据 public TreeNode\u0026lt;T\u0026gt; left; //指向左子树 public TreeNode\u0026lt;T\u0026gt; right; //指向右子树 public TreeNode(T data) { this.data = data; } public TreeNode(T data, TreeNode\u0026lt;T\u0026gt; left, TreeNode\u0026lt;T\u0026gt; right) { this.data = data; this.left = left; this.right = right; } } public interface Tree\u0026lt;T\u0026gt; { /** * 判别二叉树是否为空 * @param treeNode * @return */ boolean isEmpty(TreeNode\u0026lt;T\u0026gt; treeNode); /** * 遍历，按某顺序访问每个结点 * 遍历方法有四种： * 1. 先序遍历：preOrderTraversal * 2. 中序遍历：inOrderTraversal * 3. 后序遍历：postOrderTraversal * 4. 层次遍历：levelOrderTraversal * @param treeNode */ void traversal(TreeNode\u0026lt;T\u0026gt; treeNode); /** * 创建一个二叉树 * @return */ TreeNode\u0026lt;T\u0026gt; createBinTree(); }  其中，最重要的就是二叉树的遍历，包括先序，中序，后序和层次遍历四种，后面我们会详细讲一下。\n  4. 二叉树的存储结构 4.1 顺序存储结构 二叉树可以采用顺序存储结构来存储，对于一棵完全二叉树来说，按照从上到下，从左到右的顺序存储 n 个结点的完全二叉树的结点父子关系：\n 非根结点的父结点的序号是 $i/2$ 向下取整的值 结点的左孩子结点的序号是 $2i$，（若$2i\\leq n$，否则没有左孩子 ） 结点的右孩子的序号为$2i+1$，（若 $2i+1 \\leq n$, 否则没有右孩子）  一般的二叉树也可以采用这种结构，但是需要补充空的结点，会造成空间浪费。\n4.2. 链表存储 链表的存储结构，我们在前面已经展示过，这里不再过多赘述。\n5. 二叉树的遍历 二叉树的遍历方式一共有四种，分别是：先序遍历，中序遍历，后序遍历和层序遍历。其中先序，中序和后序遍历的实现方式又有两种，分别是递归和非递归。下面，我们就来详细的介绍一下：\n5.1 递归实现 5.1.1 先序遍历 先序遍历的过程可以描述为：\n 访问根结点 先序遍历其左子树 先序遍历其右子树  例如，我们有这样一棵树：\n按照先序遍历的顺序将结点打印出来，依此是：A B D F E C G H I，程序描述为：\npublic void preOrderTraversal(TreeNode\u0026lt;T\u0026gt; binTree) { if (binTree != null) { System.out.println(binTree.data); preOrderTraversal(binTree.left); preOrderTraversal(binTree.right); } }  5.1.2 中序遍历 中序遍历的过程可以描述为：\n 中序遍历其左子树 访问根结点 中序遍历其右子树  按照中序遍历的方式将上面这棵树的结果输出，依次是：D B E F A G H C I ,使用程序描述为：\npublic void inOrderTraversal(TreeNode\u0026lt;T\u0026gt; binTree) { if (binTree != null) { inOrderTraversal(binTree.left); System.out.println(binTree.data); inOrderTraversal(binTree.right); } }  5.1.3 后序遍历 后序遍历的过程为：\n 后序遍历其左子树 后序遍历其右子树 访问根结点  按照后序遍历的方式将上面这棵树的结点输出，依此是：D E F B H G I C A\n5.2 非递归实现 递归的本质是利用堆栈来做的，那么我们直接使用堆栈来实现上面的三种方式。\n先从中序遍历开始，中序遍历的非递归实现过程可以描述为以下几个步骤：\n  遇到一个结点，就把它压栈，并去遍历它的左子树；\n  当左子树遍历结束后，从栈顶弹出这个结点并访问它；\n  然后按其右指针再去中序遍历该结点的右子树。\n  对于上面的这样一棵树，我们按照中序遍历的过程操作堆栈：\n入栈和出栈的过程如上图所示。\npublic void nonRecursiveInOrderTraversal(TreeNode\u0026lt;T\u0026gt; binTree) { TreeNode\u0026lt;T\u0026gt; tmpTree = binTree; Stack\u0026lt;TreeNode\u0026lt;T\u0026gt;\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); while ( tmpTree != null || !stack.isEmpty(stack)) { // 一直向左并将沿途结点压入堆栈 while (tmpTree != null) { stack.push(stack, tmpTree); tmpTree = tmpTree.left; } if (!stack.isEmpty(stack)) { // 结点弹出堆栈 tmpTree = stack.pop(stack); // 访问结点 System.out.print(tmpTree.data + \u0026quot; \u0026quot;); // 转向右子树 tmpTree = tmpTree.right; } } }  先序遍历的过程跟中序遍历类似，只需要在第一次遍历到结点的时候把结点的值打印出来即可。\npublic void nonRecursivePreOrderTraversal(TreeNode\u0026lt;T\u0026gt; binTree) { TreeNode\u0026lt;T\u0026gt; tmpTree = binTree; Stack\u0026lt;TreeNode\u0026lt;T\u0026gt;\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); while ( tmpTree != null || !stack.isEmpty(stack)) { // 一直向左并将沿途结点压入堆栈 while (tmpTree != null) { // 访问结点 System.out.print(tmpTree.data + \u0026quot; \u0026quot;); stack.push(stack, tmpTree); tmpTree = tmpTree.left; } if (!stack.isEmpty(stack)) { // 结点弹出堆栈 tmpTree = stack.pop(stack); // 转向右子树 tmpTree = tmpTree.right; } } }  后序遍历的方式略有不同， 后序遍历应该把数据两次压入堆栈，第二次pop出来再 print 鉴于没有记录访问次数的结构，第二次pop的时候要么右节点是空的，要么右节点刚刚被print。所以，需要另一个指针pt来记录被刚刚print的节点。\npublic void nonRecursivepostOrderTraversal(TreeNode\u0026lt;T\u0026gt; binTree) { TreeNode\u0026lt;T\u0026gt; tmpTree = binTree; TreeNode\u0026lt;T\u0026gt; pt = null; Stack\u0026lt;TreeNode\u0026lt;T\u0026gt;\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); while ( tmpTree != null || !stack.isEmpty(stack)) { while (tmpTree != null) { stack.push(stack, tmpTree); tmpTree = tmpTree.left; } if (!stack.isEmpty(stack)) { // 结点弹出堆栈 tmpTree = stack.pop(stack); if ((tmpTree.right== null)||(tmpTree.right == pt)) {//判断右节点为空或者右节点已经输出 System.out.print(tmpTree.data + \u0026quot; \u0026quot;); pt = tmpTree; //记录下上一个被输出的 tmpTree = null; } else { stack.push(stack, tmpTree); //第二次入栈（相当于T没有出栈） tmpTree = tmpTree.right; //转向右子树 } } } }  5.3 层序遍历 层序遍历即将树从上到下，从左到右输出，例如下面这样的一棵树，按照层序遍历输出的结果为：A B C D F G I E H.\n层序遍历可以通过队列来实现，遍历从根结点开始，首先将根结点入队，然后开始执行循环：结点入队，访问该结点、其左右儿子入队。\n层序基本过程：先根结点入队，然后：\n 从队列中取出一个元素； 访问该元素所指的结点； 若该元素所指结点的左、右孩子结点非空，则将其左、右孩子的指针顺序入队。  public void levelOrderTraveral(TreeNode\u0026lt;T\u0026gt; binTree) { SeqQueue\u0026lt;TreeNode\u0026lt;T\u0026gt;\u0026gt; queue = new SeqQueue\u0026lt;\u0026gt;(20); TreeNode\u0026lt;T\u0026gt; t; // 若是空树则直接返回 if (binTree == null) { return; } queue.add(binTree); while (!queue.isEmpty()) { t = queue.delete(); System.out.print(t.data + \u0026quot; \u0026quot;);//访问取出队列中的结点 if (t.left != null) { queue.add(t.left);//左结点不为空，则左节点入队 } if (t.right != null) { queue.add(t.right); //右节点不为空，则右节点入队 } } }  6. 二叉树应用的例子  例：输出二叉树中的叶子结点\n 在二叉树的遍历算法中增加检测结点的判断：左右子树是否都为空\npublic void preOrderPrintLeaves(TreeNode\u0026lt;T\u0026gt; binTree) { if (binTree != null) { //叶子结点的左右都为空 if (binTree.left == null \u0026amp;\u0026amp; binTree.right == null) { System.out.print(binTree.data + \u0026quot; \u0026quot;); } preOrderPrintLeaves(binTree.left); preOrderPrintLeaves(binTree.right); } }   求二叉树的高度\n 二叉树的高度是左子树和右子树两者中最大的一个再加上根结点的高度1.\npublic int postOrderGetHight(TreeNode\u0026lt;T\u0026gt; binTree) { int hl, hr, maxH; if (binTree != null) { hl = postOrderGetHight(binTree.left); //求左子树的高度 hr = postOrderGetHight(binTree.right); //求右子树的高度 maxH = Math.max(hl, hr); //取左右子树较大的深度 return maxH+1; } else { return 0; } }   由两种遍历序列确定二叉树，已知三种遍历中的任意两种遍历序列，能否唯一确定一棵二叉树呢？\n 答案是：必须要有中序遍历才行。\n假如没有中序遍历，来看个例子：先序：A B，后序：B A，就会出现两种不同的结构。\n在知道中序的情况下，可以利用中序序列分割出左右两个子序列。\n 参考：浙江大学陈越老师的数据结构课程\n","date":"2020年08月11日","permalink":"https://ahamoment.cn/posts/algorithm/algorithm-binary-tree/","summary":"\u003cp\u003e摘要：二叉树的定义，遍历二叉树\u003c/p\u003e","title":"二叉树及存储结构"},{"contents":"这一部分主要介绍一下数据结构中很重要的一个概念：树。那么什么是树呢？在说明这个概念之前，我们先来看看和它相关的一些内容。\n1. 查找 查找是根据某个给定关键字K ，从集合R中找出关键字与K相同的记录。查找又分为静态查找和动态查找，静态查找的集合中记录是固定的，没有插入和删除操作，只有查找，而动态查找的集合中记录是动态变化的，除了查找外，还可能发生插入和删除操作。\n1.1 静态查找 方法一：顺序查找 顺序查找就是从数组中一个一个地找，直到找到我们想要的元素为止。\n如图所示，在长度为8的数组中查找元素K，如果我们从最后一个元素找起来，查找成功就返回所在单元下表，不成功返回0。查找过程中，在第一个几点建立哨兵，哨兵的作用可以让程序知道什么时候应该停下来，同时可以少些一个判断条件。\npublic int sequentialSearch(int[] array, int k) { int i; array[0] = k; // 建立哨兵 for (i = array.length - 1; array[i] != k; i--) ; // 查找成功返回所在单元下标，不成功返回0 return i; } public static void main(String[] args) { Search search = new Search(); int[] array = new int[9]; for (int i = array.length - 1; i != 0; i--) { array[i] = i; } int target = 7; int result = search.sequentialSearch(array, target); System.out.println(\u0026quot;search for \u0026quot; + target + \u0026quot; in array is \u0026quot; + result); }  方法二：二分查找(Binary Search) 二分查找也称折半查找（Binary Search），它是一种效率较高的查找方法。 但是，折半查找要求线性表必须采用顺序存储结构，而且表中元素按关键字有序排列。\n注意，二分查找的前提是连续存放（数组）是有序的。\n二分查找示例：\n在一个按从小到大排序的数组中查找 444 这个元素，用三个指针分别代表左边，右边和中间，每次查找都将中间为止的值和目标值对比，若大于目标值，则在左半部分做二分查找，若小于目标值，则在右半部分做二分查找。\npublic int binarySearch(int[] array, int k) { /*在表Tbl中查找关键字为K的数据元素*/ int left, right, mid, NoFound = -1; left = 1; /*初始左边界*/ right = array.length; /*初始右边界*/ while (left \u0026lt;= right) { mid = (left + right) / 2; /*计算中间元素坐标*/ if (k \u0026lt; array[mid]) right = mid - 1; /*调整右边界*/ else if (k \u0026gt; array[mid]) left = mid + 1; /*调整左边界*/ else return mid; /*查找成功，返回数据元素的下标*/ } return NoFound; /*查找不成功，返回-1*/ }  二分查找的时间复杂度为O(logN)\n二分查找是一种效率比较高的查找算法，整个二分查找的过程可以描述为以下的这种树形结构：\n结点表示的是数组的下标，这样的结构称为二分查找判定树，判定树上每个结点需要的查找次数刚好为该结点所在的层数。反过来讲，我们如果将数据按照树的这种形势存储起来，是不是也能达到二分查找这种效率呢？\n2. 树的定义  树是 n （n\u0026gt;=0）个结点构成的有限集合。当n=0时，称为空树。\n 对于任何一棵非空树，具备以下性质：\n 树中有一个称为 根（root） 的特殊结点 其余结点可分为 m（m\u0026gt;0) 个互不相交的有限集， 其中每个集合本身又是一棵树，称为原来树的子树。  2.1 树的一些基本术语  结点的度（Degree）：结点的子树个数。 树的度：树的所有结点中最大的度数。 叶结点：度为 0 的结点。 父结点：有子树的结点是其子树的根结点的父结点。 子结点：若A结点是B结点的父结点，则称B结点是A结点的子结点；子结点也称孩子结点。 兄弟结点（sibling）：具有同一父结点的各结点彼此是兄弟结点。 路径和路径长度：从结点n1到nk的路径为一个结点序列n1, n2,… , nk , ni是ni+1的父结点。路径所包含边的个数为路径的长度。 祖先结点(Ancestor)：沿树根到某一结点路径上的所有结点都是这个结点的祖先结点。 子孙结点(Descendant)：某一结点的子树中的所有结点是这个结点的子孙。 结点的层次（Level）：规定根结点在1层，其它任一结点的层数是其父结点的层数加1。 树的深度（Depth）：树中所有结点中的最大层次是这棵树的深度。  2.2 树的表示 采用儿子-兄弟表示法来表示一个树的结点，其中左边的指针指向第一个子节点，右边的指针指向相邻的兄弟结点，兄弟结点或子结点为空则用Null表示。\n 参考：浙江大学-陈越老师的数据结构课程\n ","date":"2020年08月11日","permalink":"https://ahamoment.cn/posts/algorithm/algorithm-tree/","summary":"这一部分主要介绍一下数据结构中很重要的一个概念：树。那么什么是树呢？在说明这个概念之前，我们先来看看和它相关的一些内容。 1. 查找 查找是根据某个","title":"树的定义及表示"},{"contents":"1. 什么是队列  具有一定操作约束的线性表。插入和删除操作，只能在一端插入，另一端删除。\n 数据插入称之为入队(addQ)，数据删除称之为出队(deleteQ)，队列最重要的特征就是先进先出(FIFO)。生活中有很多跟队列相关的例子，例如超市排队。\n2. 队列的抽象数据类型描述 与队列相关的操作主要包括以下几种：\n 创建队列：生成长度为 size 的空队列。 判断队列是否满了。 判断队列是否为空。 将数据元素插入到队列中。 将数据元素从队列中删除。  3. 队列的顺序存储实现 队列的顺序存储结构通常由一个一维数组和一个记录队列头元素位置的变量front以及一个记录队列尾元素位置的变量rear组成。\npublic class SeqQueue\u0026lt;T\u0026gt; implements Queue\u0026lt;T\u0026gt; { private T elementData[]; private int front, rear; }  如下图所示，用顺序存储实现队列，由于数组的元素从 0 开始，所以 front 和 rear 同时指向 -1 这个位置，添加 Job1，rear 往后移动一个位置，删除 Job1 ，front 往后移动一个位置。当队列满了的时候，就无法添加元素了，但是很明显就能发现，此时之前删除的位置还是空的，队列中还有位置，只是无法添加而已，这样的结构会造成空间浪费，我们需要用循环结构来解决。\n循环队列的机构如下图所示。循环结构中，front 和 rear 开始时同时指向 0 这个位置，之后，每一次入队，rear 向着顺时针方向移动一个位置，每一次出队列，front 向顺时针方向移动一个位置。那么这里就有个问题：队列空和满的判别条件是什么？队列空和满的时候，front=rear，那么就造成无法判断队列空还是满了。那么要如何解决呢？这里提供两个解决方法：\n 使用额外标记： Size或者tag 。size 用来记录当前元素的个数，当你加入一个元素的时候，size 加 1，删除一个元素的时候，size减1，所以只要根据size是0还是n就知道是空还是满的。tag （0，1）标记，添加一个元素，tag=1，删除一个元素 tag=0，当我们想判断队列是满还是空时，只要判断 tag 的值就知道最后一次操作是添加还是删除。 仅使用n-1个数组空间。  我们采用第二种方案，使用求余函数来查看列队是否已满。\n(rear + 1) % size == front  看看具体的实现代码：\npackage leetcode.editor.datastructure.queue; import java.io.Serializable; public class SeqQueue\u0026lt;T\u0026gt; implements Queue\u0026lt;T\u0026gt;, Serializable { private final static int DEAFULT_SIZE = 10; private T elementData[]; private int front, rear; private int size; public SeqQueue() { elementData = (T[]) new Object[DEAFULT_SIZE]; front = 0; rear = 0; } public SeqQueue(int size) { this.size = size; elementData = (T[]) new Object[size]; front = 0; rear = 0; } public void add(T data) { if ((rear + 1) % this.elementData.length == front) { System.out.println(\u0026quot;队列已满\u0026quot;); return; } rear = (rear + 1) % this.elementData.length; elementData[rear] = data; } public boolean isEmpty() { return this.elementData.length == 0; } public T delete() { if (front == rear) { System.out.println(\u0026quot;队列为空\u0026quot;); return null; } else { front = (front + 1) % this.elementData.length; return elementData[front]; } } }  4. 队列的链式存储实现  队列的链式存储结构也可以用一个单链表实现。插入和删除操作分别在链表的两头进行；队列指针front和rear应该分别指向链表的表头和表尾。\n 整个队列的结构如下图所示：\n与顺序结构不同的是，链式存储实现的队列，出队需要在表头进行，因为是单向链表，如果在表尾进行删除操作，我们无法知道前一个元素是多少。因此入队和出队操作为：\npublic class LinkedQueue\u0026lt;T\u0026gt; implements Queue\u0026lt;T\u0026gt; { private Node\u0026lt;T\u0026gt; front; //指向队头节点 private Node\u0026lt;T\u0026gt; rear; //指向队尾节点 public LinkedQueue() { this.front = null; this.rear = null; } @Override public void add(T data) { Node\u0026lt;T\u0026gt; node = new Node\u0026lt;\u0026gt;(data, null); if (this.front == null) {//空队列插入 this.front = node; } else {//非空队列,尾部插入 this.rear.next = node; } this.rear = node; } @Override public boolean isEmpty() { return front == null \u0026amp;\u0026amp; rear == null; } @Override public T delete() { Node\u0026lt;T\u0026gt; frontCell; T frontElem; if (this.front == null) { System.out.println(\u0026quot;队列为空\u0026quot;); return null; } frontCell = front; if (front == rear) //若队列只有一个元素 front = rear = null; //删除后队列置为空 else front = front.next();//front移动到下一个元素 frontElem = frontCell.data; return frontElem; } } public class Node\u0026lt;T\u0026gt; { public T data; public Node\u0026lt;T\u0026gt; next; public Node(T data) { this.data = data; next = null; } public Node(T data, Node\u0026lt;T\u0026gt; next) { this.data = data; this.next = next; } public Node\u0026lt;T\u0026gt; next() { return this.next; } }   参考：浙江大学陈越老师的数据结构课程\n ","date":"2020年08月06日","permalink":"https://ahamoment.cn/posts/algorithm/algorithm-queue/","summary":"1. 什么是队列 具有一定操作约束的线性表。插入和删除操作，只能在一端插入，另一端删除。 数据插入称之为入队(addQ)，数据删除称之为出队(del","title":"队列及其实现"},{"contents":"锁是非常有用的工具，运用场景非常多，因为它使用起来非常方便，而且易于理解。但同时它也会带来一些困扰，那就是可能引起死锁。\n1. 什么是死锁  百度百科中对于死锁的定义：死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。\n 简而言之，当线程1持有资源A，线程2持有资源B。此时线程1想要获取资源B，线程2想要获取资源A。两个线程都想要获取对方手中的资源，自己又不肯让出已有资源，一直僵持不下就形成了死锁。\n2. 死锁产生的四个条件  互斥条件：指进程对所分配到的资源进行排它性使用，即在一段时间内某资源只由一个进程占用。如果此时还有其它进程请求资源，则请求者只能等待，直至占有资源的进程用毕释放。 请求和保持条件：指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放。 不剥夺条件：指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放。 环路等待条件：指在发生死锁时，必然存在一个进程——资源的环形链，即进程集合{P0，P1，P2，···，Pn}中的P0正在等待一个 P1 占用的资源；P1 正在等待 P2 占用的资源，……，Pn 正在等待已被 P0 占用的资源。  3. 案例 public class DeadLock { private OtherService otherService; public void setOtherService(OtherService otherService) { this.otherService = otherService; } // DeadLock的实例的锁-资源A private final Object LOCK = new Object(); public void m1() { synchronized (LOCK) { System.out.println(\u0026quot;********m1********\u0026quot;); otherService.s1(); } } public void m2() { synchronized (LOCK) { System.out.println(\u0026quot;********m2********\u0026quot;); } } } public class OtherService { private DeadLock deadLock; public void setDeadLock(DeadLock deadLock) { this.deadLock = deadLock; } // OtherService的实例的锁-资源B private final Object LOCK = new Object(); public void s1() { synchronized (LOCK) { System.out.println(\u0026quot;========s1========\u0026quot;); } } public void s2() { synchronized (LOCK) { System.out.println(\u0026quot;========s2========\u0026quot;); deadLock.m2(); } } } public class DeadLockTest { public static void main(String[] args) { DeadLock deadLock = new DeadLock(); OtherService otherService = new OtherService(); deadLock.setOtherService(otherService); otherService.setDeadLock(deadLock); new Thread(() -\u0026gt; { while (true) { deadLock.m1(); } }, \u0026quot;T1\u0026quot;).start(); new Thread(() -\u0026gt; { while (true) { otherService.s2(); } }, \u0026quot;T2\u0026quot;).start(); } }  上面的案例中，两个线程 T1 和 T2 , 其中 T1 线程调用 DeadLock 的 m1 方法，在 m1 方法内部又调用了 OtherService 的 s1 方法，s1 和 m1 这两个方法都含有用 synchronized 关键字修饰的同步代码块。 T2 线程调用 OtherService 的 s2 方法，在 s2 方法内又调用的 DeadLock 的 m2 方法，同样的，s2 和 m1 这两个方法都含有用 synchronized 关键字修饰的同步代码块。整个程序如图所示：\n当 T1 线程执行的时候，m1 方法获取 DeadLock 的 LOCK 锁，并调用 OtherService 的 s1 方法，同时，T2 线程也开始执行，T2 线程获取到 OtherService 的 LOCK 锁，并调用 DeadLock 的 m2 方法，但是由于 m2 的方法的锁此时已经被 T1 线程占有，T2 线程只能等待 T1 线程释放锁，同理，T1 线程也在等待 T2 线程释放锁，于是就形成了死锁。\n我们使用 jstack 来观察一下死锁。\n首先看到这两个线程互相持有对象的锁，在等待对方释放锁。\njstack 的信息最后也会告诉我们找到一个死锁。\n4. 如何避免死锁  避免一个线程同时获取多个锁。 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。 尝试使用定时锁，使用 lock.tryLock(timeout) 来替代使用内部锁机制。 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。  ","date":"2020年06月10日","permalink":"https://ahamoment.cn/posts/java/java-multithread-dead-lock/","summary":"锁是非常有用的工具，运用场景非常多，因为它使用起来非常方便，而且易于理解。但同时它也会带来一些困扰，那就是可能引起死锁。 1. 什么是死锁 百度百科","title":"Java 多线程 - 死锁问题"},{"contents":"Synchronized 简介  本文出自汪文君老师的《Java 并发编程》课程，如需转载，请注明源出处！\n 先来看一个例子，这个例子是模拟银行叫号的，使用三个线程模拟三个柜台一起叫号，总共50个号。在不加 synchronized 的关键字的情况下，很容易就会出现并发问题。\npublic class BankRunnable { public static void main(String[] args) { // 一个runnable实例被多个线程共享 TicketWindowRunnable ticketWindow = new TicketWindowRunnable(); Thread windowThread1 = new Thread(ticketWindow, \u0026quot;一号窗口\u0026quot;); Thread windowThread2 = new Thread(ticketWindow, \u0026quot;二号窗口\u0026quot;); Thread windowThread3 = new Thread(ticketWindow, \u0026quot;三号窗口\u0026quot;); windowThread1.start(); windowThread2.start(); windowThread3.start(); } } public class TicketWindowRunnable implements Runnable { private int index = 1; private static final int MAX = 50; @Override public void run() { while (true) { if (index \u0026gt; MAX) {//1 break; } try { Thread.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName()+\u0026quot; 的号码是：\u0026quot;+(index++));//2 } } }  多运行几遍程序，就会出现下面这个问题：\n在一号窗口拿完最后一个号码之后，二号窗口和三号窗口又后续拿到了 52 和 51 号。为什么会出现这种现象呢？\n首先当 index=499 的时候，三个线程均不满足 index \u0026gt; MAX，都会向下执行。三个线程都可以向下执行，将 index 加 1。\n为了解决这个问题，这里引入了 synchronized 。\n什么是 synchronized  synchronized关键字可以实现一个简单的策略来防止线程干扰和内存一致性错误，如果一个对象对多个线程是可见的，那么对该对象的所有读或者写都将通过同步的方式来进行。\n 上面这段话是oracle官网对synchronized关键字的解释，具体表现如下：\n synchronized关键字提供了一种锁的机制，能够确保共享变量的互斥访问，从而防止数据不一致问题的出现。 synchronized关键字包括monitor enter和monitor exit两个JVM指令，它能够保证在任何时候任何线程执行到monitor enter成功之前都必须从主内存中获取数据，而不是从缓存中，在monitor exit运行成功之后，共享变量被更新后的值必须刷入主内存（在本书的第三部分会重点介绍）。 synchronized的指令严格遵守java happens-before规则，一个monitor exit指令之前必定要有一个monitor enter。  synchronized关键字的用法 Java通过 synchronized 对共享数据的线程访问提供了一种避免竞争条件的机制。synchronized 可以修饰方法或者代码块，被修饰的方法或者代码块同一时间只会允许一个线程执行，这条执行的线程持有同步部分的锁。synchronized 方法不能用于对class及其变量进行修饰。\nsynchronized 关键字可以修饰方法或者代码块，那么这两者有什么区别呢？\n// 同步代码块 public class TicketWindowRunnable implements Runnable { private int index = 1; private static final int MAX = 500; private final Object MONITOR = new Object(); @Override public void run() { while (true) { synchronized (MONITOR) { if (index \u0026gt; MAX) { break; } try { Thread.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + \u0026quot; 的号码是：\u0026quot; + (index++)); } } } }  synchronized 方法修饰代码块的时候，使用的是 monitor 锁。再来用 synchronized 修饰一下同步方法：\n@Override public synchronized void run() { while (true) { if (index \u0026gt; MAX) { break; } try { Thread.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + \u0026quot; 的号码是：\u0026quot; + (index++)); } }  运行之后发现都是同一个线程在跑，另外两个线程无法执行。这是因为 synchronized 在修饰方法的时候使用的是 this 锁，当其中一个线程拿到锁进到 while 循环之后，就一直去做事情，直到满足条件退出为止。将 while 里面的代码抽出来放到一个方法里，用 synchronized 来修饰该方法就可以解决这个问题。\n@Override public void run() { while (true) { if (ticket()) { break; } } } private synchronized boolean ticket() { if (index \u0026gt; MAX) { return true; } try { Thread.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + \u0026quot; 的号码是：\u0026quot; + (index++)); return false; }  synchronized 修饰方法时默认是使用的 this 锁，修饰代码块时使用的是对象锁。synchronized 关键字还可以用来修饰静态方法和静态代码块。\npublic class SynchronizedStatic { public synchronized static void m1() { System.out.println(\u0026quot;m1 \u0026quot; + Thread.currentThread().getName()); try { Thread.sleep(10_000); } catch (InterruptedException e) { e.printStackTrace(); } } public synchronized static void m2() { System.out.println(\u0026quot;m2 \u0026quot; + Thread.currentThread().getName()); try { Thread.sleep(10_000); } catch (InterruptedException e) { e.printStackTrace(); } } } public class SynchronizedStaticTest { public static void main(String[] args) { new Thread(\u0026quot;T1\u0026quot;) { @Override public void run() { SynchronizedStatic.m1(); } }.start(); new Thread(\u0026quot;T2\u0026quot;) { @Override public void run() { SynchronizedStatic.m2(); } }.start(); } } // output m1 T1 m2 T2  静态方法 m1 和 m2 同时被 synchronized 修饰，这个时候线程 T2 会等到线程 T1 执行完再执行，说明这两个方法使用的是同一把锁，这就是 Class 锁。我们把 sleep 的时间变长一点来观察一下是不是 Class 锁。\n可以看到，线程 T1 执行的时候，持有的是 Class 锁，此时线程 T2 在等待 T1 执行完释放锁，当 T1 执行完之后，T2 拿到 Class 锁执行代码。\n了解了 synchronized 修饰静态方法使用的是 Class 锁之后，我们再来验证一下当它修饰静态方法的时候是不是也是使用 Class 锁？\npublic class SynchronizedStatic { public synchronized static void m1() { System.out.println(\u0026quot;m1 \u0026quot; + Thread.currentThread().getName()); try { Thread.sleep(100_000); } catch (InterruptedException e) { e.printStackTrace(); } } public static void m3() { System.out.println(\u0026quot;m3 \u0026quot; + Thread.currentThread().getName()); try { Thread.sleep(10_000); } catch (InterruptedException e) { e.printStackTrace(); } } } public class SynchronizedStaticTest { public static void main(String[] args) { new Thread(\u0026quot;T1\u0026quot;) { @Override public void run() { SynchronizedStatic.m1(); } }.start(); new Thread(\u0026quot;T3\u0026quot;) { @Override public void run() { SynchronizedStatic.m3(); } }.start(); } }  这里加了一个没有 synchronized 修饰的静态方法 m3，运行之后很容易知道，这两个线程是同时运行的。我们在 SynchronizedStatic 开始的地方加一个静态代码块，静态代码块内部使用 synchronized 锁。\npublic class SynchronizedStatic { static { synchronized (SynchronizedStatic.class) { System.out.println(\u0026quot;static \u0026quot; + Thread.currentThread().getName()); try { Thread.sleep(10_000); } catch (InterruptedException e) { e.printStackTrace(); } } } public synchronized static void m1() { System.out.println(\u0026quot;m1 \u0026quot; + Thread.currentThread().getName()); try { Thread.sleep(100_000); } catch (InterruptedException e) { e.printStackTrace(); } } public static void m3() { System.out.println(\u0026quot;m3 \u0026quot; + Thread.currentThread().getName()); try { Thread.sleep(10_000); } catch (InterruptedException e) { e.printStackTrace(); } } } //output static T1 m1 T1 m3 T3  可以发现，T1 线程要先执行静态代码块才能往下走，说明静态代码块使用的锁和静态方法是一样的，另外这个时候没有用 synchronized 修饰的 m3 也要等静态代码块执行实例化才行。\n总结一下，synchronized 关键字能够避免多线程竞争导致的数据不一致，被 synchronized 修饰的方法或者代码块同一时间只会允许一个线程执行，这条执行的线程持有同步部分的锁。synchronized 关键字修饰普通方法时，使用的是 this 锁，修饰静态方法和静态代码块时，使用 Class 锁，修饰代码块时，使用 LOCK 锁。\n","date":"2020年06月10日","permalink":"https://ahamoment.cn/posts/java/java-multithread-synchronized/","summary":"Synchronized 简介 本文出自汪文君老师的《Java 并发编程》课程，如需转载，请注明源出处！ 先来看一个例子，这个例子是模拟银行叫号的，使用三个线程模拟三个柜","title":"Java 多线程 - 初识 Synchronized"},{"contents":" 本文翻译自20 Practical Examples of RPM Commands in Linux\n RMP (Red Hat Package Manager) 是一款 Red Hat 系统的开源包管理工具，支持安装、更新、卸载、查询、验证和管理系统软件包。RPM以前称为 .rpm 文件，文件内包含编译好的软件和包所需要的库。\n这篇文章主要介绍了 20 个常用的 RPM 命令。\n关于 RPM 的一些常识   RPM 是免费的，并且遵循 GPL 开源协议\n  RPM 将所有已安装软件包的信息保存在 /var/lib/rpm 数据库中。\n  RPM 是在 Linux 系统下安装软件包的唯一方法，如果您使用源代码安装了软件包，则 rpm 将无法对其进行管理。\n  RPM 处理 .rpm 文件，其中包含有关软件包的实际信息，例如：它是什么，它来自哪里，软件依赖信息，版本信息等。\n  RPM命令的五个基本模式  Install : 使用于安装任意的 RPM 包。 Remove ：用于擦除，删除或卸载任何 RPM 软件包。 Upgrade : 用于更新已经存在的 RPM 软件包。 Verify ：用来验证 RPM 软件包。 Query：用来查询 RPM 软件包。  查找和下载 RPM 包 以下是rpm网站的列表，您可以在其中找到和下载所有RPM软件包。\n http://rpmfind.net http://www.redhat.com http://freshrpms.net/ http://rpm.pbone.net/  1. 检查 RPM 包的签名 在将软件包安装在Linux系统上之前，先检查软件包的 PGP 签名，并确保其完整性和来源是正确的。使用 –-checksig (check signature) 命令检查 RPM 包的签名。\n[root@tecmint]# rpm --checksig pidgin-2.7.9-5.el6.2.i686.rpm pidgin-2.7.9-5.el6.2.i686.rpm: rsa sha1 (md5) pgp md5 OK  2. 安装 RPM 包 使用 -i 选项安装 RPM 包\n[root@localhost ~]# rpm -ivh tree-1.6.0-10.el7.x86_64.rpm 准备中... ################################# [100%] 正在升级/安装... 1:tree-1.6.0-10.el7 ################################# [100%]  RPM 命令和选项\n  -i : 安装包\n  -v : 详细显示\n  -h 在打包归档文件解压缩时打印哈希标记。\n  3. 安装 RPM 包之前检查包依赖 [root@localhost ~]# rpm -qpR tree-1.6.0-10.el7.x86_64.rpm libc.so.6()(64bit) libc.so.6(GLIBC_2.14)(64bit) libc.so.6(GLIBC_2.2.5)(64bit) libc.so.6(GLIBC_2.3)(64bit) libc.so.6(GLIBC_2.3.4)(64bit) libc.so.6(GLIBC_2.4)(64bit) rpmlib(CompressedFileNames) \u0026lt;= 3.0.4-1 rpmlib(FileDigests) \u0026lt;= 4.6.0-1 rpmlib(PayloadFilesHavePrefix) \u0026lt;= 4.0-1 rtld(GNU_HASH) rpmlib(PayloadIsXz) \u0026lt;= 5.2-1  命令和选项说明：\n -q : 查询一个包 -p: 列出此软件包提供的功能。 -R: 列出此程序包所依赖的功能。  4. 忽略依赖安装 RPM 包 如果已经知道所有必需的软件包都已安装，那么可以在安装软件包之前使用 -–nodeps(no dependencies check) 选项来忽略那些依赖项。\n[root@localhost ~]# rpm -ivh --nodeps tree-1.6.0-10.el7.x86_64.rpm 准备中... ################################# [100%] 软件包 tree-1.6.0-10.el7.x86_64 已经安装  上面的命令通过忽略依赖项错误来强制安装rpm软件包，但是如果缺少那些依赖项文件会导致程序将无法运行。\n5. 查找一个已经安装的 RPM 包 在软件包名称中使用 -q 选项，将显示是否已安装 rpm 包。\n[root@localhost ~]# rpm -q tree tree-1.6.0-10.el7.x86_64  6. 列出已安装的RPM软件包的所有文件 要查看已安装的rpm软件包的所有文件，请使用 -ql（query list） rpm 命令。\n[root@localhost ~]# rpm -ql tree /usr/bin/tree /usr/share/doc/tree-1.6.0 /usr/share/doc/tree-1.6.0/LICENSE /usr/share/doc/tree-1.6.0/README /usr/share/man/man1/tree.1.gz  7. 列出最近安装的 RPM 包 使用 -qa(query all) 命令，会列出最近安装的所有 RPM 包\n[root@localhost ~]# rpm -qa --last tree-1.6.0-10.el7.x86_64 2020年06月01日 星期一 19时04分28秒 perl-Git-1.8.3.1-22.el7_8.noarch 2020年06月01日 星期一 14时18分37秒 git-1.8.3.1-22.el7_8.x86_64 2020年06月01日 星期一 14时18分36秒 perl-TermReadKey-2.30-20.el7.x86_64 2020年06月01日 星期一 14时18分34秒 rsync-3.1.2-10.el7.x86_64 2020年06月01日 星期一 14时18分33秒 perl-Error-0.17020-2.el7.noarch 2020年06月01日 星期一 14时18分33秒 nux-dextop-release-0-5.el7.nux.noarch 2020年05月22日 星期五 19时40分35秒 gpg-pubkey-85c6cd8a-4e060c35 2020年05月22日 星期五 19时40分19秒 epel-release-7-11.noarch 2020年05月22日 星期五 19时39分27秒 libtirpc-0.2.4-0.16.el7.x86_64 2020年05月22日 星期五 18时58分40秒 vim-enhanced-7.4.629-6.el7.x86_64 2020年05月22日 星期五 17时48分48秒 vim-common-7.4.629-6.el7.x86_64 2020年05月22日 星期五 17时48分48秒 vim-filesystem-7.4.629-6.el7.x86_64 2020年05月22日 星期五 17时48分44秒 ...  8. 列出所有已安装的 RPM 包 键入以下命令以打印Linux系统上已安装软件包的所有名称。该命令和 grep 一起使用，即可搜索到我们是否安装过某个包，例如 rpm -qa | grep git，查看我们是否安装过 git 。\n[root@localhost ~]# rpm -qa kexec-tools-2.0.15-43.el7.x86_64 grub2-common-2.02-0.81.el7.centos.noarch openssh-clients-7.4p1-21.el7.x86_64 setup-2.8.71-11.el7.noarch authconfig-6.2.8-30.el7.x86_64 basesystem-10.0-7.el7.centos.noarch postfix-2.10.1-9.el7.x86_64 ncurses-base-5.9-14.20130511.el7_4.noarch kbd-1.15.5-15.el7.x86_64 kbd-misc-1.15.5-15.el7.noarch qemu-guest-agent-2.12.0-3.el7.x86_64 ...  9. 更新 RPM 包 使用 -U(upgrade) 选项来升级 RPM 包。该命令不仅会将某个 rpm 包升级到最新版本，而且还会维护旧软件包的备份，以便在新的升级软件包不能使用的时候还能使用旧的 RPM 包。\n[root@localhost ~]# rpm -Uvh tree-1.6.0-10.el7.x86_64.rpm 准备中... ################################# [100%] 软件包 tree-1.6.0-10.el7.x86_64 已经安装  10. 删除 RPM 包 使用 -e (erase) 命令来移除已安装的 rpm 包。如果要移除的 RPM 包不存在，就会有错误提示。\n[root@localhost ~]# rpm -evv tree D: loading keyring from pubkeys in /var/lib/rpm/pubkeys/*.key D: couldn't find any keys in /var/lib/rpm/pubkeys/*.key D: loading keyring from rpmdb D: opening db environment /var/lib/rpm cdb:0x401 D: opening db index /var/lib/rpm/Packages 0x400 mode=0x0 D: locked db index /var/lib/rpm/Packages D: opening db index /var/lib/rpm/Name 0x400 mode=0x0 D: read h# 302 头 SHA1 摘要： OK (489efff35e604042709daf46fb78611fe90a75aa) D: added key gpg-pubkey-f4a80eb5-53a7ff4b to keyring D: read h# 371 头 SHA1 摘要： OK (052c9c3b53cea0014763d9f82c173a87dc743eea) D: added key gpg-pubkey-85c6cd8a-4e060c35 to keyring D: Using legacy gpg-pubkey(s) from rpmdb D: read h# 380 头V3 RSA/SHA256 Signature, 密钥 ID f4a80eb5: OK D: opening db index /var/lib/rpm/Conflictname 0x400 mode=0x0 D: ========== --- tree-1.6.0-10.el7 x86_64/linux 0x2 D: opening db index /var/lib/rpm/Requirename 0x400 mode=0x0 D: ========== recording tsort relations D: ========== tsorting packages (order, #predecessors, #succesors, depth) D: 0 0 0 1 -tree-1.6.0-10.el7.x86_64 D: erasing packages D: closed db index /var/lib/rpm/Conflictname D: closed db index /var/lib/rpm/Requirename D: closed db index /var/lib/rpm/Name D: closed db index /var/lib/rpm/Packages D: closed db environment /var/lib/rpm D: opening db environment /var/lib/rpm cdb:0x401 D: opening db index /var/lib/rpm/Packages (none) mode=0x42 D: sanity checking 1 elements D: running pre-transaction scripts D: computing 5 file fingerprints D: opening db index /var/lib/rpm/Name (none) mode=0x42 D: opening db index /var/lib/rpm/Basenames (none) mode=0x42 D: opening db index /var/lib/rpm/Group (none) mode=0x42 D: opening db index /var/lib/rpm/Requirename (none) mode=0x42 D: opening db index /var/lib/rpm/Providename (none) mode=0x42 D: opening db index /var/lib/rpm/Conflictname (none) mode=0x42 D: opening db index /var/lib/rpm/Obsoletename (none) mode=0x42 D: opening db index /var/lib/rpm/Triggername (none) mode=0x42 D: opening db index /var/lib/rpm/Dirnames (none) mode=0x42 D: opening db index /var/lib/rpm/Installtid (none) mode=0x42 D: opening db index /var/lib/rpm/Sigmd5 (none) mode=0x42 D: opening db index /var/lib/rpm/Sha1header (none) mode=0x42 软件包准备中... D: computing file dispositions D: 0x0000fd00 4096 9228841 19356493 / D: ========== +++ tree-1.6.0-10.el7 x86_64-linux 0x2 D: read h# 380 头V3 RSA/SHA256 Signature, 密钥 ID f4a80eb5: OK D: erase: tree-1.6.0-10.el7 has 5 files tree-1.6.0-10.el7.x86_64 D: erase 100644 1 ( 0, 0) 4100 /usr/share/man/man1/tree.1.gz D: erase 100644 1 ( 0, 0) 4628 /usr/share/doc/tree-1.6.0/README D: erase 100644 1 ( 0, 0) 18009 /usr/share/doc/tree-1.6.0/LICENSE D: erase 040755 2 ( 0, 0) 6 /usr/share/doc/tree-1.6.0 D: erase 100755 1 ( 0, 0) 62768 /usr/bin/tree D: --- h# 380 tree-1.6.0-10.el7.x86_64 D: removing \u0026quot;tree\u0026quot; from Name index. D: removing 5 entries from Basenames index. D: removing \u0026quot;Applications/File\u0026quot; from Group index. D: removing 11 entries from Requirename index. D: removing 2 entries from Providename index. D: removing 4 entries from Dirnames index. D: removing 1 entries from Installtid index. D: removing 1 entries from Sigmd5 index. D: removing \u0026quot;a09f99f73ee3fe352489e734c63c32fa41b1be56\u0026quot; from Sha1header index. D: running post-transaction scripts D: closed db index /var/lib/rpm/Sha1header D: closed db index /var/lib/rpm/Sigmd5 D: closed db index /var/lib/rpm/Installtid D: closed db index /var/lib/rpm/Dirnames D: closed db index /var/lib/rpm/Triggername D: closed db index /var/lib/rpm/Obsoletename D: closed db index /var/lib/rpm/Conflictname D: closed db index /var/lib/rpm/Providename D: closed db index /var/lib/rpm/Requirename D: closed db index /var/lib/rpm/Group D: closed db index /var/lib/rpm/Basenames D: closed db index /var/lib/rpm/Name D: closed db index /var/lib/rpm/Packages D: closed db environment /var/lib/rpm [root@localhost ~]# echo $? 0 [root@localhost ~]# rpm -e tree 错误：未安装软件包 tree  11. 忽略依赖地删除RPM 包 使用 \u0026ndash;nodeps (Do not check dependencies) 命令项强制从系统中删除 RPM 包。需要注意的是，删除特定的软件包可能会破坏其他正在运行的应用程序。\n[root@localhost ~]# rpm -ev --nodeps tree  12. 查看一个文件属于哪个 RPM 包 假设有一个文件列表，并且想找出这些文件属于哪个 RPM 包的，那么可以使用 -qf (query file) 命令。\n[root@localhost ~]# rpm -qf /usr/bin/tree tree-1.6.0-10.el7.x86_64  13. 查看已安装的 RPM 包的信息 使用 -qi (query info) 命令查询想要知道的 rpm 包的信息。\n[root@localhost ~]# rpm -qi tree Name : tree Version : 1.6.0 Release : 10.el7 Architecture: x86_64 Install Date: 2020年06月02日 星期二 19时05分24秒 Group : Applications/File Size : 89505 License : GPLv2+ Signature : RSA/SHA256, 2014年07月04日 星期五 13时36分46秒, Key ID 24c6a8a7f4a80eb5 Source RPM : tree-1.6.0-10.el7.src.rpm Build Date : 2014年06月10日 星期二 03时28分53秒 Build Host : worker1.bsys.centos.org Relocations : (not relocatable) Packager : CentOS BuildSystem \u0026lt;http://bugs.centos.org\u0026gt; Vendor : CentOS URL : http://mama.indstate.edu/users/ice/tree/ Summary : File system tree viewer Description : The tree utility recursively displays the contents of directories in a tree-like format. Tree is basically a UNIX port of the DOS tree utility.  14. 在安装之前获取 RPM 包的信息 假设你从网上下载了一个 rpm 包，并且想要在安装之前知道这个 rpm 包的信息，那么可以使用 -qip (query info package) 这个命令来打印软件包的信息。\n[root@localhost ~]# rpm -qip python3-3.6.8-13.el7.x86_64.rpm Name : python3 Version : 3.6.8 Release : 13.el7 Architecture: x86_64 Install Date: (not installed) Group : Unspecified Size : 39904 License : Python Signature : RSA/SHA256, 2020年04月04日 星期六 05时06分11秒, Key ID 24c6a8a7f4a80eb5 Source RPM : python3-3.6.8-13.el7.src.rpm Build Date : 2020年04月02日 星期四 22时17分47秒 Build Host : x86-01.bsys.centos.org Relocations : (not relocatable) Packager : CentOS BuildSystem \u0026lt;http://bugs.centos.org\u0026gt; Vendor : CentOS URL : https://www.python.org/ Summary : Interpreter of the Python programming language Description : Python is an accessible, high-level, dynamically typed, interpreted programming language, designed with an emphasis on code readability. It includes an extensive standard library, and has a vast ecosystem of third-party libraries. The python3 package provides the \u0026quot;python3\u0026quot; executable: the reference interpreter for the Python language, version 3. The majority of its standard library is provided in the python3-libs package, which should be installed automatically along with python3. The remaining parts of the Python standard library are broken out into the python3-tkinter and python3-test packages, which may need to be installed separately. Documentation for Python is provided in the python3-docs package. Packages containing additional libraries for Python are generally named with the \u0026quot;python3-\u0026quot; prefix.  15. 查看 RPM 包安装了哪些目录 要获取已安装软件包的文件列表，使用选项 -qdf（query document file) 的命令。\n[root@localhost ~]# rpm -qdf /usr/bin/tree /usr/share/doc/tree-1.6.0/LICENSE /usr/share/doc/tree-1.6.0/README /usr/share/man/man1/tree.1.gz  16. 验证一个 RPM 包 验证软件包会将软件包已安装文件的信息与rpm数据库进行比较。使用 -Vp (verify package) 命令来验证一个软件包。\n[root@localhost ~]# rpm -Vp python3-3.6.8-13.el7.x86_64.rpm 未满足的依赖关系 python3-3.6.8-13.el7.x86_64： libpython3.6m.so.1.0()(64bit) 被 python3-3.6.8-13.el7.x86_64 需要 python3-libs(x86-64) = 3.6.8-13.el7 被 python3-3.6.8-13.el7.x86_64 需要 python3-pip 被 python3-3.6.8-13.el7.x86_64 需要 python3-setuptools 被 python3-3.6.8-13.el7.x86_64 需要 遗漏 /usr/bin/pydoc3 遗漏 /usr/bin/pydoc3.6 遗漏 /usr/bin/python3 遗漏 /usr/bin/python3.6 遗漏 /usr/bin/python3.6m 遗漏 /usr/bin/pyvenv 遗漏 /usr/bin/pyvenv-3.6 遗漏 /usr/share/doc/python3-3.6.8 遗漏 d /usr/share/doc/python3-3.6.8/README.rst 遗漏 /usr/share/licenses/python3-3.6.8 遗漏 l /usr/share/licenses/python3-3.6.8/LICENSE 遗漏 d /usr/share/man/man1/python3.1.gz 遗漏 d /usr/share/man/man1/python3.6.1.gz  17. 验证所有的 RPM 包 [root@tecmint]# rpm -Va S.5....T. c /etc/rc.d/rc.local .......T. c /etc/dnsmasq.conf .......T. /etc/ld.so.conf.d/kernel-2.6.32-279.5.2.el6.i686.conf S.5....T. c /etc/yum.conf S.5....T. c /etc/yum.repos.d/epel.repo  18. 导入 GPG key 要验证 RHEL / CentOS / Fedora 软件包，必须导入 GPG 密钥。为此，执行以下命令，它将导入CentOS 6 GPG密钥。\n[root@localhost ~]# rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7  19. 列出所有导入的 RPM GPG key [root@localhost ~]# rpm -qa gpg-pubkey* gpg-pubkey-85c6cd8a-4e060c35 gpg-pubkey-f4a80eb5-53a7ff4b  20. 重建损坏的RPM数据库 有时rpm数据库损坏并停止rpm和系统上其他应用程序的所有功能。因此，当时我们需要重建rpm数据库并在以下命令的帮助下将其还原。\n[root@tecmint]# cd /var/lib [root@tecmint]# rm __db* [root@tecmint]# rpm --rebuilddb [root@tecmint]# rpmdb_verify Packages  21. 查看 RPM 的脚本 rpm -qp --scripts \u0026lt;rpm file name\u0026gt;  export LD_LIBRARY_PATH=/apps/svr/python3/lib:$LD_LIBRARY_PATH；\n","date":"2020年06月02日","permalink":"https://ahamoment.cn/posts/linux/linux-20-rpm-command/","summary":"本文翻译自20 Practical Examples of RPM Commands in Linux RMP (Red Hat Package Manager) 是一款 Red Hat 系统的开源包管理工具，支持安装、更新、卸载、查询、验证和管理系统软件包。RPM以前称为 .rpm 文件","title":"20 常用的 RPM 命令"},{"contents":"一、准备 Itellj IDEA， jdk1.8 的源代码包(解压 jdk 目录下的 src.zip 包得到)\n二、项目结构 IDEA 创建一个普通的 java 项目 把解压得到的 jdk1.8 的源代码复制到 source 目录下： test 目录用来写测试用例, 这里用不到 Main 方法。\n三、IDEA 设置  Project Structure -\u0026gt; Project 设置项目的 SDK (jdk8u221)，language level 选择 8 - Lambdas, type annotations etc.  Project Structure -\u0026gt; Dependencies 选择 Modules，SDK 选择 1.8_221  设置平台的 SDK 源代码路径为自己项目的 source 目录  调整编译线程的堆大小，避免内存不足，编译无法通过，调整到 1G 以上。  IDEA 默认调试是不会进入到 jdk 的源代码的，在 Debugger 设置中允许进入到 jdk 的包 到这里，我们完成了 idea 的设置，可以开始写个测试程序编译运行。  四、编译调式 创建一个测试类进行调试：\nimport java.util.HashMap; import java.util.Map; public class Test { public static void main(String[] args) { Map\u0026lt;String, Double\u0026gt; hashMap = new HashMap\u0026lt;\u0026gt;(); hashMap.put(\u0026quot;k1\u0026quot;, 0.1); hashMap.put(\u0026quot;k2\u0026quot;, 0.2); hashMap.put(\u0026quot;k3\u0026quot;, 0.3); hashMap.put(\u0026quot;k4\u0026quot;, 0.4); } }  进入 debug 之后，就可以在源代码里写一些笔记了。 五、编译问题 经常碰到的几个问题：\n  缺少com.sun.tools包 缺少sun.awt.UNIXToolkit 和 sun.font.FontConfigManager这两个类   解决办法可以参考这篇博客：JDK1.8源码分析03之idea搭建源码阅读环境\n","date":"2020年05月11日","permalink":"https://ahamoment.cn/posts/java/java-source-code-learn/","summary":"一、准备 Itellj IDEA， jdk1.8 的源代码包(解压 jdk 目录下的 src.zip 包得到) 二、项目结构 IDEA 创建一个普通的 java 项目 把解压得到的 jdk1.8 的源代码复制到 source 目录下： test 目录用来","title":"JDK1.8 源代码阅读环境搭建"},{"contents":"大多数 UNIX 系统命令从你的终端接受输入并将所产生的输出发送回到您的终端。一个命令通常从一个叫标准输入的地方读取输入，默认情况下，这恰好是你的终端。同样，一个命令通常将其输出写入到标准输出，默认情况下，这也是你的终端。\n重定向命令列表如下：\n   命令 说明     command \u0026gt; file 将输出重定向到 file。   command \u0026lt; file 将输入重定向到 file。   command \u0026raquo; file 将输出以追加的方式重定向到 file。   n \u0026gt; file 将文件描述符为 n 的文件重定向到 file。   n \u0026raquo; file 将文件描述符为 n 的文件以追加的方式重定向到 file。   n \u0026gt;\u0026amp; m 将输出文件 m 和 n 合并。   n \u0026lt;\u0026amp; m 将输入文件 m 和 n 合并。   \u0026laquo; tag 将开始标记 tag 和结束标记 tag 之间的内容作为输入。     需要注意的是文件描述符 0 通常是标准输入（STDIN），1 是标准输出（STDOUT），2 是标准错误输出（STDERR）。\n 输出重定向 重定向一般通过在命令间插入特定的符号来实现。特别的，这些符号的语法如下所示:\ncommand1 \u0026gt; file1\n上面这个命令执行command1然后将输出的内容存入file1。\n注意任何file1内的已经存在的内容将被新内容替代。如果要将新内容添加在文件末尾，请使用\u0026raquo;操作符。\n实例 执行下面的 who 命令，它将命令的完整的输出重定向在用户文件中(users):\n$ who \u0026gt; users  执行后，并没有在终端输出信息，这是因为输出已被从默认的标准输出设备（终端）重定向到指定的文件。\n你可以使用 cat 命令查看文件内容：\n$ cat users _mbsetupuser console Oct 31 17:35 laolan console Oct 31 17:35 laolan ttys000 Dec 1 11:33  输出重定向会覆盖文件内容，请看下面的例子：\n$ echo \u0026quot;W3Cschool教程：www.w3cschool.cn\u0026quot; \u0026gt; users $ cat users W3Cschool教程：www.w3cschool.cn $  如果不希望文件内容被覆盖，可以使用 \u0026raquo; 追加到文件末尾，例如：\n$ echo \u0026quot;W3Cschool教程：www.w3cschool.cn\u0026quot; \u0026gt;\u0026gt; users $ cat users W3Cschool教程：www.w3cschool.cn W3Cschool教程：www.w3cschool.cn $  输入重定向 和输出重定向一样，Unix 命令也可以从文件获取输入，语法为：\ncommand1 \u0026lt; file1\n这样，本来需要从键盘获取输入的命令会转移到文件读取内容。\n注意：输出重定向是大于号(\u0026gt;)，输入重定向是小于号(\u0026lt;)。\n实例 接着以上实例，我们需要统计 users 文件的行数,执行以下命令：\n$ wc -l users 2 users  也可以将输入重定向到 users 文件：\n$ wc -l \u0026lt; users 2  注意：上面两个例子的结果不同：第一个例子，会输出文件名；第二个不会，因为它仅仅知道从标准输入读取内容。\ncommand1 \u0026lt; infile \u0026gt; outfile\n同时替换输入和输出，执行command1，从文件infile读取内容，然后将输出写入到outfile中。\n重定向深入讲解 一般情况下，每个 Unix/Linux 命令运行时都会打开三个文件：\n 标准输入文件(stdin)：stdin的文件描述符为0，Unix程序默认从stdin读取数据。 标准输出文件(stdout)：stdout 的文件描述符为1，Unix程序默认向stdout输出数据。 标准错误文件(stderr)：stderr的文件描述符为2，Unix程序会向stderr流中写入错误信息。  默认情况下，command \u0026gt; file 将 stdout 重定向到 file，command \u0026lt; file 将stdin 重定向到 file。\n如果希望 stderr 重定向到 file，可以这样写：\n$ command 2 \u0026gt; file\n如果希望 stderr 追加到 file 文件末尾，可以这样写：\n$ command 2 \u0026gt;\u0026gt; file\n2 表示标准错误文件(stderr)。\n如果希望将 stdout 和 stderr 合并后重定向到 file，可以这样写：\n$ command \u0026gt; file 2\u0026gt;\u0026amp;1\n或者\n$ command \u0026gt;\u0026gt; file 2\u0026gt;\u0026amp;1\n如果希望对 stdin 和 stdout 都重定向，可以这样写：\n$ command \u0026lt; file1 \u0026gt;file2\ncommand 命令将 stdin 重定向到 file1，将 stdout 重定向到 file2。\nHere Document Here Document 是 Shell 中的一种特殊的重定向方式，用来将输入重定向到一个交互式 Shell 脚本或程序。\n它的基本的形式如下：\ncommand \u0026lt;\u0026lt; delimiter document delimiter  它的作用是将两个 delimiter 之间的内容(document) 作为输入传递给 command。\n注意：\n结尾的delimiter 一定要顶格写，前面不能有任何字符，后面也不能有任何字符，包括空格和 tab 缩进。 开始的delimiter前后的空格会被忽略掉。\n实例 在命令行中通过 wc -l 命令计算 Here Document 的行数：\n$ wc -l \u0026lt;\u0026lt; EOF 欢迎来到 W3Cschool教程 www.w3cschool.cn EOF 3 # 输出结果为 3 行 $  我们也可以将 Here Document 用在脚本中，例如：\n#!/bin/bash # author:W3Cschool教程 # url:www.w3cschool.cn cat \u0026lt;\u0026lt; EOF 欢迎来到 W3Cschool教程 www.w3cschool.cn EOF  执行以上脚本，输出结果：\n欢迎来到 W3Cschool教程 www.w3cschool.cn  /dev/null 文件 如果希望执行某个命令，但又不希望在屏幕上显示输出结果，那么可以将输出重定向到 /dev/null：\n$ command \u0026gt; /dev/null\n/dev/null 是一个特殊的文件，写入到它的内容都会被丢弃；如果尝试从该文件读取内容，那么什么也读不到。但是 /dev/null 文件非常有用，将命令的输出重定向到它，会起到\u0026quot;禁止输出\u0026quot;的效果。\n如果希望屏蔽 stdout 和 stderr，可以这样写：\n$ command \u0026gt; /dev/null 2\u0026gt;\u0026amp;1\n 注意：0 是标准输入（STDIN），1 是标准输出（STDOUT），2 是标准错误输出（STDERR）。\n ","date":"2019年11月26日","permalink":"https://ahamoment.cn/posts/linux/linux-shell-input-output-redirect/","summary":"大多数 UNIX 系统命令从你的终端接受输入并将所产生的输出发送回到您的终端。一个命令通常从一个叫标准输入的地方读取输入，默认情况下，这恰好是你的终端","title":"Linux Shell 输入/输出重定向"},{"contents":"1. 前言 当我们发现服务器上的应用发生某些故障，并且没有足够的日志来定位问题的时候，就会觉得非常头疼，尤其是在生产环境中想要对应用进行调试并非易事。在本文中，我们使用Java平台提供的标准功能来配置正在运行的Web服务器和调试应用程序。\n2. 配置 在开始之前，我们有必要介绍一下本文的示例工程所用的工具和环境：\n 应用使用spring boot框架，部署在linux中，由于 spring boot 内置tomcat服务器，因此部署的时通过maven/gradle打包后，直接用 java -jar test.jar 命令启动应用。 调试工具用的是IntelliJ idea  2.1 Java 启动参数配置 Java Platform Debugging Architecture（JPDA）是一组可扩展的API，其中一部分是称为JDWP（Java Debug Wire Protocol）的特殊调试协议。\nJDWP是用于在应用程序和调试器进程之间进行通信的协议，可用于对正在运行的Java应用程序进行远程故障排除。\n要配置远程应用程序进行调试，您必须在Java应用的启动参数中为此协议指定参数。\njava -Xdebug -Xrunjdwp:transport=dt_socket,address=8000,server=y,suspend=y Test\n这些参数要做的事情就是启用远程调试和配置有效的选项：\n -Xdebug：参数启用debug调试特性 -Xrunjdwp：使用几个重要参数配置JDWP协议。  从 JDK5 开始，可以使用 -agentlib:jdwp 选项，而不是 -Xdebug 和 -Xrunjdwp。但如果连接到 JDK5 以前的 VM，只能选择 -Xdebug 和 -Xrunjdwp。\njava -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8001 Test\nJDK 9 之后， 由于Java 9 JDWP代理默认情况下仅侦听本地网络接口，因此将拒绝远程连接。\n JDK 9 Realease Notes core-svc/debugger JDWP socket connector accept only local connections by default The JDWP socket connector has been changed to bind to localhost only if no ip address or hostname is specified on the agent command line. A hostname of asterisk (*) may be used to achieve the old behavior which is to bind the JDWP socket connector to all available interfaces; this is not secure and not recommended.\n 因此对于 JDK 9 及更高的版本，启动debug的命令如下：\njava -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:8001 Test\n下面介绍一下jdwp提供的一些子选项参数：\n  transport：指定运行的被调试应用和调试者之间的通信协议。这里通常使用套接字传输。但是在 Windows 平台上也可以使用共享内存传输。\n  server：如果值为 y，目标应用程序监听将要连接的调试器应用程序。否则，它将连接到特定地址上的调试器应用程序。\n  suspend：如果值为 n 用来告知 JVM 立即执行，不要等待未来将要附着上/连上（attached）的调试者。如果设成 y, 则应用将暂停不运行，直到有调试者连接上\n suspend=y的一个比较适用的场景是，当debug一个会阻止应用成功启动的问题时， 通过suspend=y可以确保调试者连上来之后再启动应用，否则应用已经启动报错了再调试也没意义了。\n   address：远程被调试应用开通的端口，可定义其他端口。\n  服务端使用jdwp的调试参数成功启动后，我们可以看到java进程如下所示：\n[root@test]$ ps -ef | grep java root 1323 0 99 11:36 ? 00:00:30 java -Dspring.profiles.active=default -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=2222 -jar ./test.jar  2.2 IntelliJ idea 配置 首先保证 IDEA 里面已经打开了需要远程调试的代码，注意代码要与线上的代码一致，这里也可以用war/jar包来调。 然后点击 Run ➝ Edit Configurations ➝ **+ **按钮 ➝ Remote 这里只需要填好服务器的地址和debug端口后，点击\u0026quot;debug\u0026quot;按钮启动即可。 当我们看到打印出这行信息时，就可以对需要调试的代码打断点调试了。\n3. 远程 JVM 调试的工作原理 一切源于被称作 Agents 的东西。\n运行着各种编译过的 .class 文件的JVM， 有一种特性，可以允许外部的库（Java或C++写的libraries）在运行时注入到 JVM 中。这些外部的库就称作 Agents, 他们有能力修改运行中 .class 文件的内容。\n这些 Agents 拥有的这些 JVM 的功能权限， 是在 JVM 内运行的 Java Code 所无法获取的， 他们能用来做一些有趣的事情，比如修改运行中的源码， 性能分析等。 像 JRebel 工具就是用了这些功能达到魔术般的效果。\n传递一个 Agent Lib 给 JVM, 通过添加 agentlib:libname[=options] 格式的启动参数即可办到。像上面的远程调试我们用的就是 -agentlib:jdwp=transport=dt_socket,address=1043,server=y,suspend=n来引入 jdwp 这个 Agent 的。\njdwp 是一个 JVM 特定的 JDWP（Java Debug Wire Protocol） 可选实现，用来定义调试者与运行JVM之间的通讯，它的是通过 JVM 本地库的 jdwp.so 或者 jdwp.dll 支持实现的。\nJPDA 由两个接口（分别是 JVM Tool Interface 和 JDI）、一个协议（Java Debug Wire Protocol）和两个用于合并它们的软件组件（后端和前端）组成。  JVM TI-Java VM工具接口。JVM TI是J2SE 5.0中引入的新接口，它替代了JVMDI。它定义了VM提供的调试服务。 JDWP-Java调试线协议。定义调试对象和调试器进程之间的通信。 JDI-Java调试接口。定义高级Java语言界面，工具开发人员可以轻松地使用该界面来编写远程调试器应用程序。  简单来说， jdwp agent 会建立运行应用的 JVM 和调试者（本地或者远程）之间的桥梁。既然他是一个Agent Library, 它就有能力拦截运行的代码。\n在 JVM 架构里， debugging 功能在 JVM 本身的内部是找不到的，它是一种抽象到外部工具的方式（也称作调试者 debugger）。这些调试工具或者运行在 JVM 的本地 或者在远程。这是一种解耦，模块化的架构。\n概述此模块化体系结构的所有规范都包含在Java平台中，调试器体系结构，JPDA，您可以在此处阅读其详细介绍： Java Platform Debugger Architecture Overview.\n4. 总结 在本文中，我们简单介绍了如何配置Java服务器以进行远程调试，以及如何使用简单的控制台工具来调试应用程序。但是需要注意的是，调试模式会降低服务器的速度，因为它会禁用一些JVM优化。另外，调试模式可能会带来潜在的安全风险。您需要通过特定端口向调试器提供对服务器的访问权限，这对于用心不良的人来说是另一个潜在的安全漏洞。所以并不建议长期开着调试模式跑应用。\n参考文档 [1] Java远程调试（Remote Debugging）的那些事\n[2] How to Remotely Debug Application Running on Tomcat From Within Intellij IDEA\n[3] A Practical Guide to Java Remote Debugging\n[4] 使用 Eclipse 远程调试 Java 应用程序\n[5] JPDA\n[6] 深入 Java 调试体系\n[7] Java: local and remote JVM debugging — JDK 8, 9 and later\n","date":"2019年11月21日","permalink":"https://ahamoment.cn/posts/java/java-remote-debug/","summary":"1. 前言 当我们发现服务器上的应用发生某些故障，并且没有足够的日志来定位问题的时候，就会觉得非常头疼，尤其是在生产环境中想要对应用进行调试并非易","title":"IntellJ IDEA 远程调试 Java 程序"},{"contents":"1. 背景 公司使用的代码仓库是Gitlab，个人代码仓库又是Github。每次提交代码的时候，需要切换不同的提交作者和提交邮箱，非常容易出错。 这个脚本是根据repo url自动设置提交作者，避免每次手动配置。\n2. 方法 2.1 安装Git  Linux 上安装Git直接使用 sudo yum install git 或者 sudo apt-get install git 命令即可。 Windows 上安装到官网下载安装软件安装即可。安装地址  2.2 添加脚本 在/root/下添加.git-templates/hooks 目录，添加脚本post-checkout，脚本内容如下：\n#!/bin/bash #checkout hook to locally set user name and email based on user defined patterns #The patterns are matched against the clone url. # #Based on http://www.dvratil.cz/2015/12/git-trick-628-automatically-set-commit-author-based-on-repo-url/ function warn { echo -e \u0026quot;\\n$1 Email and author not initialized in local config!\u0026quot; } email=\u0026quot;$(git config --local user.email)\u0026quot; name=\u0026quot;$(git config --local user.name)\u0026quot; if [[ $1 != \u0026quot;0000000000000000000000000000000000000000\u0026quot; || -n $email || -n $name ]]; then exit 0 fi #get remote name: # only one: take it # more: take \u0026quot;origin\u0026quot;, or fail remote=\u0026quot;$([[ $(git remote | wc -l) -eq 1 ]] \u0026amp;\u0026amp; git remote || git remote | grep \u0026quot;^origin$\u0026quot;)\u0026quot; if [[ -z $remote ]]; then warn \u0026quot;Failed to detect remote.\u0026quot; exit 0 fi url=\u0026quot;$(git config --local remote.${remote}.url)\u0026quot; if [[ ! -f ~/.git-clone-init ]]; then cat \u0026lt;\u0026lt; INPUT \u0026gt; ~/.git-clone-init #!/bin/bash case \u0026quot;\\$url\u0026quot; in *@github.com:* ) email=\u0026quot;\u0026quot;; name=\u0026quot;\u0026quot;;; *//github.com/* ) email=\u0026quot;\u0026quot;; name=\u0026quot;\u0026quot;;; esac INPUT warn \u0026quot;\\nMissing file ~/.git-clone-init. Template created...\u0026quot; exit 0 fi . ~/.git-clone-init if [[ -z $name || -z $email ]]; then warn \u0026quot;Failed to detect identity using ~/.git-clone-init.\u0026quot; exit 0 fi git config --local user.email \u0026quot;$email\u0026quot; git config --local user.name \u0026quot;$name\u0026quot; echo -e \u0026quot;\\nIdentity set to $name \u0026lt;$email\u0026gt;\u0026quot;  2.3 添加配置文件 在/root/.gitconfig文件中（没有的话自行创建）添加如下内容，指定使用的模板：\n[user] [init] templatedir = /root/.git-templates [push] default = simple  在/root/.git-clone-init中针对不同的git仓库添加提交作者和邮箱：\n#!/bin/bash case \u0026quot;$url\u0026quot; in *@github.com:* ) email=\u0026quot;test@qq.com\u0026quot;; name=\u0026quot;xq\u0026quot;;; *//github.com/* ) email=\u0026quot;test@qq.com\u0026quot;; name=\u0026quot;xq\u0026quot;;; *@gitlab.com:* ) email=\u0026quot;test@google.com\u0026quot;; name=\u0026quot;xq\u0026quot;;; *//gitlab.com/* ) email=\u0026quot;test@google.com\u0026quot;; name=\u0026quot;xq\u0026quot;;; esac  2.4 验证  更改脚本post-checkout的权限并执行。 sudo chmod 777 post-checkout sh post-checkout 尝试从github 克隆一个仓库： [root@yxj-test git_storage]# git clone git@github.com:XueqiangChen/scripts.git Cloning into 'scripts'... Warning: Permanently added the RSA host key for IP address '52.74.223.119' to the list of known hosts. remote: Enumerating objects: 8, done. remote: Counting objects: 100% (8/8), done. remote: Compressing objects: 100% (6/6), done. remote: Total 8 (delta 0), reused 5 (delta 0), pack-reused 0 Receiving objects: 100% (8/8), done. Identity set to XueqiangChen \u0026lt;569503960@qq.com\u0026gt;  `` 可以看到最后一行的打印出来的信息，自动针对该仓库设置了提交作者和邮箱，下一次提交就会使用这个提交作者和邮箱。\n  欢迎访问GITHUB查看具体代码！！！\n 参考文章\n GIT TRICK #628: AUTOMATICALLY SET COMMIT AUTHOR BASED ON REPO URL 廖雪峰的GIT教程   ","date":"2019年11月16日","permalink":"https://ahamoment.cn/posts/tool/tool-git-set-config/","summary":"1. 背景 公司使用的代码仓库是Gitlab，个人代码仓库又是Github。每次提交代码的时候，需要切换不同的提交作者和提交邮箱，非常容易出错。 这","title":"自动配置Git仓库提交作者"}]